#!/usr/bin/env python3
"""
Validation script for artifact analysis reports.

This script can:
1. Parse markdown analysis files generated by analyze_accumulation_flow.py
2. Run curl commands directly to fetch and validate artifacts from live agents
3. Generate both markdown (.md) and JSON (.json) validation artifacts

Usage:
    # Parse markdown file
    python scripts/validate_artifacts.py scripts/supervisor_argocd_version_8000.md
    python scripts/validate_artifacts.py scripts/supervisor_argocd_version_8000.md --show-messages

    # Run curl and validate live (generates both .md and .json files)
    python scripts/validate_artifacts.py --query "show argocd version" --host localhost --port 8000
    python scripts/validate_artifacts.py --query "show argocd version" --host localhost --port 8000 --output /tmp/my_validation
    python scripts/validate_artifacts.py --query "use jarvis agent to get llm access" --host localhost --port 8000 --show-messages --output results/validation
"""

import argparse
import json
import re
import subprocess
import sys
import tempfile
import os
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime


def parse_markdown_analysis(file_path: str) -> Dict[str, Any]:
    """Parse markdown analysis file and extract structured data."""
    with open(file_path, 'r') as f:
        content = f.read()

    result = {
        'query': '',
        'generated': '',
        'summary': {},
        'artifacts': {},
        'accumulation_steps': [],
        'status_updates': [],
        'content_comparison': {},
        'issues': []
    }

    # Extract query
    query_match = re.search(r'\*\*Query\*\*: "([^"]+)"', content)
    if query_match:
        result['query'] = query_match.group(1)

    # Extract generated timestamp
    generated_match = re.search(r'\*\*Generated\*\*: ([^\n]+)', content)
    if generated_match:
        result['generated'] = generated_match.group(1)

    # Extract summary
    summary_match = re.search(r'## Summary\n\n(.*?)\n\n###', content, re.DOTALL)
    if summary_match:
        summary_text = summary_match.group(1)
        for line in summary_text.split('\n'):
            if '**' in line:
                # Extract key-value pairs
                match = re.search(r'- \*\*([^*]+)\*\*: ([^\n]+)', line)
                if match:
                    key = match.group(1).strip()
                    value = match.group(2).strip()
                    result['summary'][key] = value

    # Extract artifact breakdown
    artifact_breakdown_match = re.search(r'### Artifact Breakdown\n\n(.*?)\n\n##', content, re.DOTALL)
    if artifact_breakdown_match:
        breakdown_text = artifact_breakdown_match.group(1)
        for line in breakdown_text.split('\n'):
            if line.strip().startswith('- **'):
                match = re.search(r'- \*\*([^*]+)\*\*: (\d+) events?', line)
                if match:
                    artifact_name = match.group(1).strip()
                    count = int(match.group(2))
                    result['artifacts'][artifact_name] = {'count': count, 'messages': []}

    # Extract DataPart artifacts section
    datapart_match = re.search(r'## DataPart Artifacts.*?\n\n(.*?)\n\n##', content, re.DOTALL)
    if datapart_match:
        datapart_text = datapart_match.group(1)
        if 'No DataPart artifacts found' not in datapart_text:
            # Extract DataPart artifact details
            artifact_matches = re.finditer(
                r'### (\w+) \(ID: ([^)]+)\)\n- \*\*Has DataPart\*\*: ‚úÖ Yes\n- \*\*Text Length\*\*: (\d+) chars\n- \*\*DataPart JSON\*\*:\n```json\n(.*?)```',
                datapart_text,
                re.DOTALL
            )
            for match in artifact_matches:
                artifact_name = match.group(1)
                artifact_id = match.group(2)
                text_length = int(match.group(3))
                json_data = match.group(4).strip()
                try:
                    data = json.loads(json_data)
                    if artifact_name not in result['artifacts']:
                        result['artifacts'][artifact_name] = {'count': 0, 'messages': []}
                    result['artifacts'][artifact_name]['datapart'] = {
                        'id': artifact_id,
                        'text_length': text_length,
                        'data': data
                    }
                except json.JSONDecodeError:
                    result['issues'].append(f"Failed to parse DataPart JSON for {artifact_name}")

    # Extract accumulation flow table
    table_match = re.search(r'## Accumulation Flow\n\n\|.*?\n\|.*?\n(.*?)\n\n##', content, re.DOTALL)
    if table_match:
        table_content = table_match.group(1)
        for line in table_content.split('\n'):
            if line.strip().startswith('|') and not line.strip().startswith('|---'):
                parts = [p.strip() for p in line.split('|')[1:-1]]  # Skip empty first/last
                if len(parts) >= 6:
                    step = parts[0]
                    event_type = parts[1]
                    artifact_id = parts[2]
                    content_chunk = parts[5] if len(parts) > 5 else ''

                    # Extract artifact name from event_type
                    artifact_name_match = re.search(r'\((\w+)\)', event_type)
                    if artifact_name_match:
                        artifact_name = artifact_name_match.group(1)
                        if artifact_name in result['artifacts']:
                            result['artifacts'][artifact_name]['messages'].append({
                                'step': step,
                                'artifact_id': artifact_id,
                                'content': content_chunk[:200] + '...' if len(content_chunk) > 200 else content_chunk,
                                'full_content_length': len(content_chunk)
                            })

    # Extract artifact details table
    details_match = re.search(r'## Artifact Details\n\n\|.*?\n\|.*?\n(.*?)(?:\n\n|\Z)', content, re.DOTALL)
    if details_match:
        details_table = details_match.group(1)
        for line in details_table.split('\n'):
            if line.strip().startswith('|') and not line.strip().startswith('|---'):
                parts = [p.strip() for p in line.split('|')[1:-1]]
                if len(parts) >= 3:
                    artifact_name = parts[0]
                    count = parts[1]
                    text_length = parts[2]
                    has_datapart = parts[3] if len(parts) > 3 else '‚ùå'

                    if artifact_name in result['artifacts']:
                        result['artifacts'][artifact_name]['details'] = {
                            'count': count,
                            'text_length': text_length,
                            'has_datapart': has_datapart == '‚úÖ'
                        }

    # Extract content comparison
    comparison_match = re.search(r'## Content Comparison\n\n(.*?)(?:\n\n##|\Z)', content, re.DOTALL)
    if comparison_match:
        comparison_text = comparison_match.group(1)

        # Extract sub-agent content
        sub_agent_match = re.search(r'### Sub-Agent Full Content \((\d+) chars\)\n```\n(.*?)```', comparison_text, re.DOTALL)
        if sub_agent_match:
            result['content_comparison']['sub_agent'] = {
                'length': int(sub_agent_match.group(1)),
                'content': sub_agent_match.group(2).strip()
            }

        # Extract partial_result content
        partial_match = re.search(r'### Partial Result Content \((\d+) chars\)\n```\n(.*?)```', comparison_text, re.DOTALL)
        if partial_match:
            result['content_comparison']['partial_result'] = {
                'length': int(partial_match.group(1)),
                'content': partial_match.group(2).strip()
            }

        # Extract final_result content
        final_match = re.search(r'### Final Result Content \((\d+) chars\)\n```\n(.*?)```', comparison_text, re.DOTALL)
        if final_match:
            result['content_comparison']['final_result'] = {
                'length': int(final_match.group(1)),
                'content': final_match.group(2).strip()
            }

        # Check for duplicate warnings
        if 'DUPLICATE DETECTED' in comparison_text:
            duplicate_match = re.search(r'‚ö†Ô∏è \*\*DUPLICATE DETECTED\*\*: ([^\n]+)', comparison_text)
            if duplicate_match:
                result['issues'].append(f"Duplicate detected: {duplicate_match.group(1)}")

    # Extract status updates
    status_match = re.search(r'## Status Updates\n\n(.*?)(?:\n\n##|\Z)', content, re.DOTALL)
    if status_match:
        status_text = status_match.group(1)
        status_matches = re.finditer(
            r'### Status Update (\d+)\n- \*\*State\*\*: ([^\n]+)\n- \*\*Final\*\*: ([^\n]+)',
            status_text
        )
        for match in status_matches:
            result['status_updates'].append({
                'number': int(match.group(1)),
                'state': match.group(2).strip(),
                'final': match.group(3).strip()
            })

    return result


def validate_artifacts(analysis: Dict[str, Any], verbose: bool = False) -> List[str]:
    """Validate artifact flow and detect issues."""
    issues = []

    # Check for required artifacts
    required_artifacts = ['tool_notification_start', 'streaming_result']
    for artifact_name in required_artifacts:
        if artifact_name not in analysis['artifacts']:
            issues.append(f"‚ö†Ô∏è Missing required artifact: {artifact_name}")

    # Check for duplicate content
    if 'content_comparison' in analysis:
        sub_agent = analysis['content_comparison'].get('sub_agent', {})
        partial_result = analysis['content_comparison'].get('partial_result', {})

        if sub_agent.get('content') and partial_result.get('content'):
            sub_content = sub_agent['content']
            partial_content = partial_result['content']

            # Check for exact duplicates
            if sub_content == partial_content:
                issues.append("‚úÖ No duplication: sub-agent and partial_result match exactly")
            elif sub_content in partial_content:
                issues.append("‚ö†Ô∏è Partial duplication: sub-agent content is contained in partial_result")
            elif partial_content in sub_content:
                issues.append("‚ö†Ô∏è Partial duplication: partial_result content is contained in sub-agent")

    # Check for corruption (embedded JSON status payloads)
    for artifact_name, artifact_data in analysis['artifacts'].items():
        for message in artifact_data.get('messages', []):
            content = message.get('content', '')
            if '{"status":"completed"' in content or '{"status":"completed","message"' in content:
                issues.append(f"‚ö†Ô∏è Corruption detected in {artifact_name}: embedded JSON status payload")

    # Check DataPart consistency
    datapart_artifacts = [name for name, data in analysis['artifacts'].items()
                          if data.get('datapart') or data.get('details', {}).get('has_datapart')]
    if datapart_artifacts:
        issues.append(f"‚úÖ DataPart artifacts found: {', '.join(datapart_artifacts)}")

    return issues


def print_artifact_summary(analysis: Dict[str, Any], show_messages: bool = False):
    """Print formatted artifact summary."""
    print("=" * 80)
    print(f"Artifact Validation Report")
    print("=" * 80)
    print(f"\nQuery: {analysis.get('query', 'N/A')}")
    print(f"Generated: {analysis.get('generated', 'N/A')}")

    # Summary
    if analysis.get('summary'):
        print("\n" + "-" * 80)
        print("SUMMARY")
        print("-" * 80)
        for key, value in analysis['summary'].items():
            print(f"  {key}: {value}")

    # Artifact breakdown
    if analysis.get('artifacts'):
        print("\n" + "-" * 80)
        print("ARTIFACT BREAKDOWN")
        print("-" * 80)
        print(f"{'Artifact Name':<30} {'Count':<10} {'Text Length':<15} {'Has DataPart':<15} {'Messages':<10}")
        print("-" * 80)

        for artifact_name, artifact_data in sorted(analysis['artifacts'].items()):
            count = artifact_data.get('count', 0)
            details = artifact_data.get('details', {})
            # For live curl analysis, use total_text_length instead of details
            text_length = details.get('text_length') or artifact_data.get('total_text_length', 0)
            if isinstance(text_length, int):
                text_length = f"{text_length} chars"
            has_datapart = '‚úÖ Yes' if details.get('has_datapart') or artifact_data.get('datapart') or artifact_data.get('has_datapart') else '‚ùå No'
            message_count = len(artifact_data.get('messages', []))

            print(f"{artifact_name:<30} {count:<10} {text_length:<15} {has_datapart:<15} {message_count:<10}")

    # DataPart artifacts
    datapart_artifacts = {name: data for name, data in analysis['artifacts'].items()
                          if data.get('datapart')}
    if datapart_artifacts:
        print("\n" + "-" * 80)
        print("DATAPART ARTIFACTS")
        print("-" * 80)
        for artifact_name, artifact_data in datapart_artifacts.items():
            datapart = artifact_data['datapart']
            print(f"\n{artifact_name} (ID: {datapart['id'][:8]}...)")
            print(f"  Text Length: {datapart['text_length']} chars")
            print(f"  Data Keys: {list(datapart['data'].keys())}")
            if 'metadata' in datapart['data']:
                metadata = datapart['data']['metadata']
                if isinstance(metadata, dict) and 'input_fields' in metadata:
                    print(f"  Input Fields: {len(metadata['input_fields'])} fields")

    # Messages by artifact type - show as table
    if show_messages and analysis.get('artifacts'):
        print("\n" + "-" * 80)
        print("EVENTS TABLE")
        print("-" * 80)

        # Collect all events with their artifact names
        all_events = []
        for artifact_name, artifact_data in sorted(analysis['artifacts'].items()):
            messages = artifact_data.get('messages', [])
            for i, msg in enumerate(messages, 1):
                full_content = msg.get('full_content') or msg.get('content_preview', '')
                content_preview = full_content.replace('\n', ' ').replace('|', '\\|')[:150] + ('...' if len(full_content) > 150 else '')

                chunk_info = f" ({msg.get('chunk_count', 0)} chunks)" if msg.get('chunk_count') else ""

                all_events.append({
                    'artifact_name': artifact_name,
                    'event_num': i,
                    'artifact_id': (msg.get('artifact_id', '')[:8] + '...') if msg.get('artifact_id') else 'N/A',
                    'append': str(msg.get('append', False)),
                    'last_chunk': str(msg.get('last_chunk', False)),
                    'content_length': msg.get('full_content_length') or msg.get('content_length', 0),
                    'chunk_info': chunk_info,
                    'content_preview': content_preview,
                    'has_datapart': '‚úÖ' if (msg.get('has_datapart') or msg.get('artifact_data')) else '‚ùå'
                })

        # Print table header
        print(f"{'#':<4} {'Artifact Name':<25} {'ID':<12} {'Append':<8} {'Last':<8} {'Length':<10} {'Chunks':<10} {'DataPart':<10} {'Content Preview'}")
        print("-" * 80)

        # Print table rows
        for i, event in enumerate(all_events, 1):
            print(f"{i:<4} {event['artifact_name']:<25} {event['artifact_id']:<12} {event['append']:<8} {event['last_chunk']:<8} "
                  f"{event['content_length']:<10} {event['chunk_info'].strip():<10} {event['has_datapart']:<10} {event['content_preview']}")

        # Show detailed DataPart info if present
        datapart_events = [e for e in all_events if e['has_datapart'] == '‚úÖ']
        if datapart_events:
            print("\n" + "-" * 80)
            print("DATAPART DETAILS")
            print("-" * 80)
            for artifact_name, artifact_data in sorted(analysis['artifacts'].items()):
                if artifact_data.get('datapart'):
                    datapart = artifact_data['datapart']
                    print(f"\n{artifact_name} (ID: {datapart['id'][:8]}...):")
                    if isinstance(datapart['data'], dict):
                        print(f"  Data Keys: {', '.join(datapart['data'].keys())}")
                        if 'metadata' in datapart['data']:
                            metadata = datapart['data']['metadata']
                            if isinstance(metadata, dict) and 'input_fields' in metadata:
                                print(f"  Input Fields: {len(metadata['input_fields'])} fields")
                    print(f"  JSON: {json.dumps(datapart['data'], indent=2)[:500]}...")

    # Validation issues
    issues = validate_artifacts(analysis, verbose=True)
    if issues:
        print("\n" + "-" * 80)
        print("VALIDATION ISSUES")
        print("-" * 80)
        for issue in issues:
            print(f"  {issue}")

    # Content comparison
    if analysis.get('content_comparison'):
        print("\n" + "-" * 80)
        print("CONTENT COMPARISON")
        print("-" * 80)

        for content_type, content_data in analysis['content_comparison'].items():
            print(f"\n{content_type.replace('_', ' ').title()}:")
            print(f"  Length: {content_data['length']} chars")
            if content_data.get('content'):
                preview = content_data['content'][:200].replace('\n', ' ')
                print(f"  Preview: {preview}...")

    print("\n" + "=" * 80)


def run_curl_query(query: str, host: str = "localhost", port: int = 8000, timeout: int = 60) -> str:
    """Run curl command to send query to agent and return raw response."""
    url = f"http://{host}:{port}"
    payload = {
        "id": f"validate-{datetime.now().timestamp()}",
        "method": "message/stream",
        "params": {
            "message": {
                "role": "user",
                "parts": [{"kind": "text", "text": query}],
                "messageId": f"msg-validate-{datetime.now().timestamp()}"
            }
        }
    }

    cmd = [
        "curl",
        "-X", "POST",
        url,
        "-H", "Content-Type: application/json",
        "-H", "Accept: text/event-stream",
        "-d", json.dumps(payload),
        "--max-time", str(timeout),
        "--silent",
        "--show-error"
    ]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout + 10)
        # Curl may return non-zero exit code but still have partial data (e.g., connection closed)
        # We'll use whatever data we got, but warn if there was an error
        if result.returncode != 0 and result.stderr:
            # Check if stderr contains the "transfer closed" error which is often OK for streaming
            if "transfer closed with outstanding read data remaining" in result.stderr:
                print("‚ö†Ô∏è Warning: Connection closed early, but using partial response", file=sys.stderr)
            else:
                print(f"‚ö†Ô∏è Warning: curl returned error: {result.stderr}", file=sys.stderr)

        if not result.stdout.strip():
            raise ValueError("Empty response from agent")

        return result.stdout
    except subprocess.TimeoutExpired:
        raise TimeoutError(f"Request timed out after {timeout} seconds")
    except Exception as e:
        raise RuntimeError(f"Error running curl: {e}")


def parse_events_from_curl(content: str) -> List[Dict[str, Any]]:
    """Parse SSE events from curl response."""
    events = []
    for line in content.split('\n'):
        if line.startswith('data:'):
            try:
                event_data = json.loads(line[5:].strip())
                events.append(event_data)
            except json.JSONDecodeError:
                continue
    return events


def generate_markdown_report(query: str, events: List[Dict[str, Any]], analysis_data: Dict[str, Any]) -> str:
    """Generate markdown report from events (similar to analyze_accumulation_flow.py)."""
    accumulation_log = []
    sub_agent_accumulated = []
    supervisor_accumulated = []
    final_content = None
    partial_result_content = None
    artifact_updates = []

    # Track streaming_result by artifact_id for accumulation
    streaming_by_id = {}

    for i, event in enumerate(events):
        result = event.get('result', {})
        kind = result.get('kind', '')

        if kind == 'artifact-update':
            artifact = result.get('artifact', {})
            artifact_name = artifact.get('name', '')
            artifact_id = artifact.get('artifactId', '')
            parts = artifact.get('parts', [])
            append = result.get('append', False)
            last_chunk = result.get('lastChunk', False)

            artifact_text = ''
            artifact_data = None
            has_datapart = False

            for part in parts:
                if isinstance(part, dict):
                    root = part.get('root', {})
                    if isinstance(root, dict):
                        text = root.get('text', '') or part.get('text', '')
                        data = root.get('data') or part.get('data')
                    else:
                        text = part.get('text', '')
                        data = part.get('data')

                    if text:
                        artifact_text += text
                        if artifact_name == 'streaming_result':
                            sub_agent_accumulated.append(text)
                        elif artifact_name == 'partial_result':
                            partial_result_content = text if not partial_result_content else partial_result_content + text
                        elif artifact_name == 'final_result':
                            final_content = text if not final_content else final_content + text

                    if data:
                        has_datapart = True
                        artifact_data = data if artifact_data is None else artifact_data
                        json_str = json.dumps(data)
                        if artifact_name in ['complete_result', 'final_result', 'partial_result']:
                            sub_agent_accumulated.append(json_str)
                        artifact_text += f"\n[DataPart JSON]: {json_str}"

            if artifact_text or artifact_data:
                # For streaming_result, accumulate by artifact_id
                if artifact_name == 'streaming_result':
                    if artifact_id not in streaming_by_id:
                        streaming_by_id[artifact_id] = {
                            'chunks': [],
                            'total_text': '',
                            'first_append': append,
                            'last_chunk': False
                        }
                    streaming_by_id[artifact_id]['chunks'].append({
                        'text': artifact_text,
                        'append': append,
                        'last_chunk': last_chunk
                    })
                    streaming_by_id[artifact_id]['total_text'] += artifact_text
                    streaming_by_id[artifact_id]['last_chunk'] = last_chunk
                    # Don't add to accumulation_log yet - we'll add accumulated version later
                else:
                    accumulation_log.append({
                        'step': len(accumulation_log) + 1,
                        'event_type': f'artifact-update ({artifact_name})' + (' [DataPart]' if has_datapart else ''),
                        'artifact_id': artifact_id[:8] + '...' if len(artifact_id) > 8 else artifact_id,
                        'append': append,
                        'last_chunk': last_chunk,
                        'content_chunk': artifact_text,
                        'data_part': json.dumps(artifact_data) if artifact_data else None,
                        'sub_agent_accumulated': ''.join(sub_agent_accumulated),
                        'supervisor_accumulated': ''.join(supervisor_accumulated),
                        'total_sub_agent': len(''.join(sub_agent_accumulated)),
                        'total_supervisor': len(''.join(supervisor_accumulated))
                    })

                artifact_updates.append({
                    'name': artifact_name,
                    'id': artifact_id,
                    'append': append,
                    'last_chunk': last_chunk,
                    'text_length': len(artifact_text),
                    'has_datapart': has_datapart,
                    'data_part': artifact_data
                })

        elif kind == 'status-update':
            status = result.get('status', {})
            message = status.get('message', {}) if isinstance(status, dict) else {}
            parts = message.get('parts', []) if isinstance(message, dict) else []
            status_text = ''
            for part in parts:
                if isinstance(part, dict) and part.get('text'):
                    status_text += part.get('text', '')

    # Add accumulated streaming_result entries
    for artifact_id, stream_data in streaming_by_id.items():
        accumulation_log.append({
            'step': len(accumulation_log) + 1,
            'event_type': f'artifact-update (streaming_result) [{len(stream_data["chunks"])} chunks]',
            'artifact_id': artifact_id[:8] + '...' if len(artifact_id) > 8 else artifact_id,
            'append': stream_data['first_append'],
            'last_chunk': stream_data['last_chunk'],
            'content_chunk': stream_data['total_text'],
            'data_part': None,
            'sub_agent_accumulated': ''.join(sub_agent_accumulated),
            'supervisor_accumulated': ''.join(supervisor_accumulated),
            'total_sub_agent': len(''.join(sub_agent_accumulated)),
            'total_supervisor': len(''.join(supervisor_accumulated))
        })

    sub_agent_full = ''.join(sub_agent_accumulated)
    supervisor_full = ''.join(supervisor_accumulated)

    # Count artifacts by type
    artifact_counts = {}
    for artifact in artifact_updates:
        name = artifact['name']
        artifact_counts[name] = artifact_counts.get(name, 0) + 1

    # Generate markdown
    markdown = f"""# Content Accumulation Flow Analysis

**Query**: "{query}"
**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Summary

- **Total Sub-Agent Accumulated**: {len(sub_agent_full)} chars
- **Total Supervisor Accumulated**: {len(supervisor_full)} chars
- **Partial Result Length**: {len(partial_result_content) if partial_result_content else 0} chars
- **Final Result Length**: {len(final_content) if final_content else 0} chars
- **Total Streaming Events**: {len(accumulation_log)}
- **Status Updates**: {len(analysis_data.get('status_updates', []))}

### Artifact Breakdown

"""

    for artifact_name, count in artifact_counts.items():
        markdown += f"- **{artifact_name}**: {count} events\n"

    # Count DataPart artifacts
    datapart_artifacts = [a for a in artifact_updates if a.get('has_datapart')]
    if datapart_artifacts:
        markdown += f"\n- **DataPart Artifacts**: {len(datapart_artifacts)} artifacts with structured JSON data\n"

    markdown += "\n## Accumulation Flow\n\n"
    markdown += "| Step | Event Type | Artifact ID | Append | Last Chunk | Content Chunk | Sub-Agent Accumulated | Supervisor Accumulated | Sub-Agent Total | Supervisor Total |\n"
    markdown += "|------|------------|-------------|--------|------------|---------------|------------------------|------------------------|-----------------|------------------|\n"

    # Show all events (no truncation)
    for log in accumulation_log:
        markdown += "| {} | {} | {} | {} | {} | {} | {} | {} | {} | {} |\n".format(
            log['step'],
            log['event_type'],
            log.get('artifact_id', ''),
            log.get('append', False),
            log.get('last_chunk', False),
            log['content_chunk'].replace('|', '\\|').replace('\n', ' '),  # No truncation
            log['sub_agent_accumulated'].replace('|', '\\|').replace('\n', ' '),  # No truncation
            log['supervisor_accumulated'].replace('|', '\\|').replace('\n', ' '),  # No truncation
            log['total_sub_agent'],
            log['total_supervisor']
        )

    # Content comparison
    markdown += "\n## Content Comparison\n\n"

    if sub_agent_full:
        markdown += f"### Sub-Agent Full Content ({len(sub_agent_full)} chars)\n"
        markdown += f"```\n{sub_agent_full}\n```\n\n"

    if supervisor_full:
        markdown += f"### Supervisor Full Content ({len(supervisor_full)} chars)\n"
        markdown += f"```\n{supervisor_full}\n```\n\n"

    if partial_result_content:
        markdown += f"### Partial Result Content ({len(partial_result_content)} chars)\n"
        markdown += f"```\n{partial_result_content}\n```\n\n"

    if final_content:
        markdown += f"### Final Result Content ({len(final_content)} chars)\n"
        markdown += f"```\n{final_content}\n```\n\n"

    # DataPart artifacts
    if datapart_artifacts:
        markdown += "\n## DataPart Artifacts (Structured JSON)\n\n"
        for artifact in datapart_artifacts:
            markdown += f"""
### {artifact['name']} (ID: {artifact['id'][:8]}...)
- **Has DataPart**: ‚úÖ Yes
- **Text Length**: {artifact['text_length']} chars
- **DataPart JSON**:
```json
{json.dumps(artifact['data_part'], indent=2)}
```
"""
    else:
        markdown += "\n## DataPart Artifacts\n\n*No DataPart artifacts found - all artifacts are TextPart only.*\n\n"

    # Artifact details
    if artifact_updates:
        markdown += "\n## Artifact Details\n\n"
        markdown += "| Artifact Name | Count | Total Text Length | Has DataPart |\n"
        markdown += "|---------------|-------|-------------------|--------------|\n"

        artifact_stats = {}
        for artifact in artifact_updates:
            name = artifact['name']
            if name not in artifact_stats:
                artifact_stats[name] = {'count': 0, 'total_length': 0, 'has_datapart': False}
            artifact_stats[name]['count'] += 1
            artifact_stats[name]['total_length'] += artifact['text_length']
            if artifact.get('has_datapart'):
                artifact_stats[name]['has_datapart'] = True

        for name, stats in artifact_stats.items():
            datapart_indicator = "‚úÖ" if stats['has_datapart'] else "‚ùå"
            markdown += f"| {name} | {stats['count']} | {stats['total_length']} chars | {datapart_indicator} |\n"

    return markdown


def generate_json_validation(query: str, events: List[Dict[str, Any]], analysis: Dict[str, Any], analysis_data: Dict[str, Any]) -> Dict[str, Any]:
    """Generate JSON validation object with key components."""

    # Extract artifact details
    artifacts_json = {}
    for artifact_name, artifact_data in analysis.get('artifacts', {}).items():
        artifacts_json[artifact_name] = {
            'count': artifact_data.get('count', 0),
            'total_text_length': artifact_data.get('details', {}).get('text_length') or artifact_data.get('total_text_length', 0),
            'has_datapart': artifact_data.get('datapart') is not None or artifact_data.get('details', {}).get('has_datapart', False),
            'messages_count': len(artifact_data.get('messages', [])),
            'datapart': artifact_data.get('datapart')
        }

    # Extract event flow
    events_flow = []
    for i, event in enumerate(events):
        result = event.get('result', {})
        kind = result.get('kind', '')

        event_entry = {
            'index': i + 1,
            'kind': kind,
            'timestamp': datetime.now().isoformat()  # Could be extracted from event if available
        }

        if kind == 'artifact-update':
            artifact = result.get('artifact', {})
            event_entry['artifact'] = {
                'name': artifact.get('name', ''),
                'id': artifact.get('artifactId', ''),
                'append': result.get('append', False),
                'last_chunk': result.get('lastChunk', False),
                'has_parts': len(artifact.get('parts', [])) > 0
            }
        elif kind == 'status-update':
            status = result.get('status', {})
            event_entry['status'] = {
                'state': status.get('state', '') if isinstance(status, dict) else '',
                'final': result.get('final', False)
            }

        events_flow.append(event_entry)

    # Validation checks
    validation_issues = []
    validation_passed = True

    # Check for required artifacts
    required_artifacts = ['tool_notification_start', 'streaming_result']
    for artifact_name in required_artifacts:
        if artifact_name not in artifacts_json:
            validation_issues.append({
                'type': 'missing_required_artifact',
                'severity': 'error',
                'artifact': artifact_name,
                'message': f'Required artifact {artifact_name} not found'
            })
            validation_passed = False

    # Check for duplicate content
    content_comparison = analysis.get('content_comparison', {})
    if content_comparison.get('sub_agent') and content_comparison.get('partial_result'):
        sub_content = content_comparison['sub_agent'].get('content', '')
        partial_content = content_comparison['partial_result'].get('content', '')

        if sub_content and partial_content:
            if sub_content in partial_content or partial_content in sub_content:
                validation_issues.append({
                    'type': 'content_duplication',
                    'severity': 'warning',
                    'message': 'Sub-agent content is duplicated in partial_result',
                    'sub_agent_length': len(sub_content),
                    'partial_result_length': len(partial_content)
                })

    # Check for corruption (embedded JSON status payloads)
    for artifact_name, artifact_data in analysis.get('artifacts', {}).items():
        for message in artifact_data.get('messages', []):
            content = message.get('content', '') or message.get('content_preview', '')
            if '{"status":"completed"' in content or '{"status":"completed","message"' in content:
                validation_issues.append({
                    'type': 'corruption',
                    'severity': 'warning',
                    'artifact': artifact_name,
                    'message': f'Embedded JSON status payload detected in {artifact_name}',
                    'artifact_id': message.get('artifact_id', '')
                })

    # Check DataPart consistency
    datapart_artifacts = [name for name, data in artifacts_json.items() if data.get('has_datapart')]

    # Summary statistics
    total_artifacts = sum(a.get('count', 0) for a in artifacts_json.values())
    total_text_length = sum(int(str(a.get('total_text_length', 0)).replace(' chars', '').split()[0]) if isinstance(a.get('total_text_length'), str) else a.get('total_text_length', 0) for a in artifacts_json.values())

    return {
        'query': query,
        'generated': datetime.now().isoformat(),
        'summary': {
            'total_events': len(events),
            'total_artifacts': total_artifacts,
            'total_text_length': total_text_length,
            'artifact_types': list(artifacts_json.keys()),
            'has_datapart': len(datapart_artifacts) > 0,
            'datapart_artifacts': datapart_artifacts,
            'status_updates_count': len(analysis.get('status_updates', []))
        },
        'artifacts': artifacts_json,
        'events_flow': events_flow,  # Include all events (no truncation)
        'content_comparison': {
            'sub_agent_length': content_comparison.get('sub_agent', {}).get('length', 0),
            'supervisor_length': content_comparison.get('supervisor', {}).get('length', 0),
            'partial_result_length': content_comparison.get('partial_result', {}).get('length', 0),
            'final_result_length': content_comparison.get('final_result', {}).get('length', 0)
        },
        'validation': {
            'passed': validation_passed,
            'issues': validation_issues,
            'issues_count': len(validation_issues),
            'checks_performed': [
                'required_artifacts',
                'content_duplication',
                'corruption_detection',
                'datapart_consistency'
            ]
        }
    }


def analyze_artifacts_from_events(events: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze artifacts directly from parsed events (similar to analyze_accumulation_flow.py)."""
    artifacts = {}
    status_updates = []
    # Track streaming_result chunks by artifact_id for accumulation
    streaming_accumulator = {}  # artifact_id -> {'chunks': [], 'total_text': '', 'first_append': None, 'last_chunk': False}

    for event in events:
        result = event.get('result', {})
        kind = result.get('kind', '')

        if kind == 'artifact-update':
            artifact = result.get('artifact', {})
            artifact_name = artifact.get('name', '')
            artifact_id = artifact.get('artifactId', '')
            parts = artifact.get('parts', [])
            append = result.get('append', False)
            last_chunk = result.get('lastChunk', False)

            if artifact_name not in artifacts:
                artifacts[artifact_name] = {
                    'count': 0,
                    'messages': [],
                    'has_datapart': False,
                    'total_text_length': 0
                }

            # Extract content from parts
            artifact_text = ''
            artifact_data = None
            has_datapart = False

            for part in parts:
                if isinstance(part, dict):
                    # Handle different part formats
                    root = part.get('root', {})
                    if isinstance(root, dict):
                        text = root.get('text', '') or part.get('text', '')
                        data = root.get('data') or part.get('data')
                    else:
                        text = part.get('text', '')
                        data = part.get('data')

                    if text:
                        artifact_text += text

                    if data:
                        has_datapart = True
                        artifact_data = data if artifact_data is None else artifact_data

            # Special handling for streaming_result: accumulate chunks
            if artifact_name == 'streaming_result':
                if artifact_id not in streaming_accumulator:
                    streaming_accumulator[artifact_id] = {
                        'chunks': [],
                        'total_text': '',
                        'first_append': append,
                        'last_chunk': False,
                        'has_datapart': False,
                        'artifact_data': None
                    }

                streaming_accumulator[artifact_id]['chunks'].append({
                    'append': append,
                    'last_chunk': last_chunk,
                    'text': artifact_text,
                    'content_length': len(artifact_text)
                })
                streaming_accumulator[artifact_id]['total_text'] += artifact_text
                streaming_accumulator[artifact_id]['last_chunk'] = last_chunk

                if has_datapart:
                    streaming_accumulator[artifact_id]['has_datapart'] = True
                    streaming_accumulator[artifact_id]['artifact_data'] = artifact_data

                # Don't add individual messages for streaming_result yet - we'll add accumulated ones later
                artifacts[artifact_name]['count'] += 1
                artifacts[artifact_name]['total_text_length'] += len(artifact_text)
            else:
                # For non-streaming artifacts, add each event as a separate message
                artifacts[artifact_name]['count'] += 1

                if has_datapart:
                    artifacts[artifact_name]['has_datapart'] = True
                    if 'datapart' not in artifacts[artifact_name]:
                        artifacts[artifact_name]['datapart'] = {
                            'id': artifact_id,
                            'data': artifact_data
                        }

                artifacts[artifact_name]['total_text_length'] += len(artifact_text)
                artifacts[artifact_name]['messages'].append({
                    'artifact_id': artifact_id,
                    'append': append,
                    'last_chunk': last_chunk,
                    'content_length': len(artifact_text),
                    'content_preview': artifact_text[:200] + '...' if len(artifact_text) > 200 else artifact_text,
                    'has_datapart': has_datapart,
                    'full_content': artifact_text  # Store full content for display
                })

        elif kind == 'status-update':
            status = result.get('status', {})
            message = status.get('message', {}) if isinstance(status, dict) else {}
            parts = message.get('parts', []) if isinstance(message, dict) else []
            status_text = ''
            for part in parts:
                if isinstance(part, dict) and part.get('text'):
                    status_text += part.get('text', '')

            status_updates.append({
                'state': status.get('state', '') if isinstance(status, dict) else '',
                'final': result.get('final', False),
                'text': status_text,
                'text_length': len(status_text)
            })

    # Now add accumulated streaming_result messages (one per artifact_id)
    if 'streaming_result' in artifacts:
        for artifact_id, accumulator in streaming_accumulator.items():
            chunk_count = len(accumulator['chunks'])
            total_text = accumulator['total_text']

            artifacts['streaming_result']['messages'].append({
                'artifact_id': artifact_id,
                'append': accumulator['first_append'],
                'last_chunk': accumulator['last_chunk'],
                'content_length': len(total_text),
                'content_preview': total_text[:200] + '...' if len(total_text) > 200 else total_text,
                'has_datapart': accumulator['has_datapart'],
                'full_content': total_text,
                'chunk_count': chunk_count,  # Number of chunks accumulated
                'artifact_data': accumulator.get('artifact_data')
            })

            # Update datapart if present
            if accumulator['has_datapart'] and accumulator.get('artifact_data'):
                artifacts['streaming_result']['has_datapart'] = True
                if 'datapart' not in artifacts['streaming_result']:
                    artifacts['streaming_result']['datapart'] = {
                        'id': artifact_id,
                        'data': accumulator['artifact_data']
                    }

    return {
        'artifacts': artifacts,
        'status_updates': status_updates,
        'query': '',  # Will be set by caller
        'generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }


def main():
    parser = argparse.ArgumentParser(
        description='Validate artifact analysis reports or fetch artifacts directly from agent',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('file', nargs='?', help='Path to markdown analysis file (optional if using --query)')
    parser.add_argument('--query', '-q', help='Query to send to agent (runs curl instead of parsing file)')
    parser.add_argument('--host', default='localhost', help='Agent host (default: localhost)')
    parser.add_argument('--port', type=int, default=8000, help='Agent port (default: 8000)')
    parser.add_argument('--timeout', type=int, default=60, help='Request timeout in seconds (default: 60)')
    parser.add_argument('--output', '-o', help='Output base filename (without extension). Will generate .md and .json files. Default: /tmp/validate_artifacts_<timestamp>')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    parser.add_argument('--show-messages', '-m', action='store_true', help='Show message content for each artifact')

    args = parser.parse_args()

    # Determine mode: curl or file parsing
    temp_md_file = None
    if args.query:
        # Run curl, generate markdown report, then validate it
        print(f"üîç Validating query: \"{args.query}\"")
        print(f"üì° Connecting to {args.host}:{args.port}...")

        temp_md_file = None
        try:
            raw_response = run_curl_query(args.query, args.host, args.port, args.timeout)
            if not raw_response.strip():
                print("‚ö†Ô∏è Warning: Empty response from agent", file=sys.stderr)
                return 1

            print("üìä Parsing events...")
            events = parse_events_from_curl(raw_response)
            print(f"‚úÖ Parsed {len(events)} events")

            if not events:
                print("‚ö†Ô∏è Warning: No events found in response", file=sys.stderr)
                return 1

            print("üìù Generating markdown report...")
            # Generate markdown report similar to analyze_accumulation_flow.py
            analysis_data = analyze_artifacts_from_events(events)
            markdown_content = generate_markdown_report(args.query, events, analysis_data)

            # Determine output filenames
            if args.output:
                base_filename = args.output
                # Remove extension if provided
                if base_filename.endswith('.md') or base_filename.endswith('.json'):
                    base_filename = os.path.splitext(base_filename)[0]
            else:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                base_filename = f"/tmp/validate_artifacts_{timestamp}"

            md_file = f"{base_filename}.md"
            json_file = f"{base_filename}.json"

            # Write markdown file
            with open(md_file, 'w') as f:
                f.write(markdown_content)

            print(f"üíæ Markdown file: {md_file}")

            # Now parse and validate the markdown file
            print("üî¨ Validating markdown report...")
            analysis = parse_markdown_analysis(md_file)

            # Generate JSON validation object
            print("üìã Generating JSON validation object...")
            json_validation = generate_json_validation(args.query, events, analysis, analysis_data)

            # Write JSON file
            with open(json_file, 'w') as f:
                json.dump(json_validation, f, indent=2)

            print(f"üíæ JSON file: {json_file}")

            print_artifact_summary(analysis, show_messages=args.show_messages)

            # Print JSON validation summary
            print("\n" + "-" * 80)
            print("JSON VALIDATION SUMMARY")
            print("-" * 80)
            print(json.dumps({
                'validation_passed': json_validation['validation']['passed'],
                'total_artifacts': json_validation['summary']['total_artifacts'],
                'artifact_types': list(json_validation['artifacts'].keys()),
                'has_datapart': json_validation['summary']['has_datapart'],
                'issues_count': len(json_validation['validation']['issues'])
            }, indent=2))

            print(f"\n‚úÖ Generated artifacts:")
            print(f"   - Markdown: {md_file}")
            print(f"   - JSON: {json_file}")

            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Error running curl: {e}", file=sys.stderr)
            if e.stderr:
                print(f"   stderr: {e.stderr}", file=sys.stderr)
            return 1
        except Exception as e:
            print(f"‚ùå Error: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            return 1

    elif args.file:
        # Parse markdown file
        if not Path(args.file).exists():
            print(f"‚ùå Error: File not found: {args.file}")
            return 1

        try:
            analysis = parse_markdown_analysis(args.file)
            print_artifact_summary(analysis, show_messages=args.show_messages)
            return 0
        except Exception as e:
            print(f"‚ùå Error parsing file: {e}")
            import traceback
            traceback.print_exc()
            return 1
    else:
        parser.print_help()
        print("\n‚ùå Error: Either provide a file path or use --query to fetch artifacts directly")
        return 1


if __name__ == '__main__':
    exit(main())

