# ============================================================================
# CAIPE DOCKER COMPOSE DEV CONFIGURATION
# ============================================================================
# Development version of docker-compose.yaml with local builds and file mounts.
# For setup instructions and agent orchestration details, refer to:
#   https://cnoe-io.github.io/ai-platform-engineering/getting-started/quick-start
#
# This docker-compose file supports P2P and SLIM transport via profiles.
# Agent selection and configuration are controlled through environment variables
# in the .env fileâ€”see documentation for full list and descriptions.
# ============================================================================

services:

  ####################################################################################################
  #                                 CAIPE Supervisor Deep Agent                                      #
  ####################################################################################################
  caipe-supervisor:
    build:
      context: .
      dockerfile: build/Dockerfile
    container_name: caipe-supervisor
    volumes:
      - ${PROMPT_CONFIG_PATH:-./charts/ai-platform-engineering/data/prompt_config.deep_agent.yaml}:/app/prompt_config.yaml
      - ./ai_platform_engineering:/app/ai_platform_engineering
    env_file:
      - .env
    ports:
      - "8000:8000"
    environment:
      # A2A transport configuration (p2p or slim)
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      # Prompt configuration file path
      - PROMPT_CONFIG_PATH=${PROMPT_CONFIG_PATH:-./prompt_config.yaml}

      # Slim dataplane endpoint configuration
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}

      # Agent connectivity configuration
      - AGENT_CONNECTIVITY_ENABLE_BACKGROUND=${AGENT_CONNECTIVITY_ENABLE_BACKGROUND:-true}
      - SKIP_AGENT_CONNECTIVITY_CHECK=${SKIP_AGENT_CONNECTIVITY_CHECK:-false}
      - ENABLE_ARTIFACT_STREAMING=true

      # Agent hostname mappings for agent connectivity.
      # These environment variables allow the CAIPE agent to connect to sub-agents for multi-agent orchestration.
      # If you add new agents/services, include a corresponding *_AGENT_HOST assignment below.
      - ARGOCD_AGENT_HOST=agent-argocd
      - AWS_AGENT_HOST=agent-aws
      - BACKSTAGE_AGENT_HOST=agent-backstage
      - CONFLUENCE_AGENT_HOST=agent-confluence
      - GITHUB_AGENT_HOST=agent-github
      - JIRA_AGENT_HOST=agent-jira
      - PAGERDUTY_AGENT_HOST=agent-pagerduty
      - PETSTORE_AGENT_HOST=agent-petstore
      - RAG_AGENT_HOST=agent_rag
      - SLACK_AGENT_HOST=agent-slack
      - SPLUNK_AGENT_HOST=agent-splunk
      - WEATHER_AGENT_HOST=agent-weather
      - WEBEX_AGENT_HOST=agent-webex

      # Agent ports
      # Add an exception for the RAG agent port to 8099. All other agents use the default port 8000.
      - RAG_AGENT_PORT=${RAG_AGENT_PORT:-8099}

      # The following ENABLE_* environment variables control the activation of individual sub-agents for multi-agent orchestration.
      # In .env file, set each variable to 'true' to enable the corresponding agent, or 'false' to disable it.
      # Example: ENABLE_ARGOCD=true enables the ArgoCD agent, allowing the platform engineer to route tasks related to Kubernetes application management.
      # These flags allow you to fine-tune which domain-specific agents are available as tools to the main platform engineer agent at runtime.
      # Update these variables as needed based on your deployment requirements, security posture, or available agent containers.

      - ENABLE_ARGOCD=${ENABLE_ARGOCD:-false}
      - ENABLE_AWS=${ENABLE_AWS:-false}
      - ENABLE_BACKSTAGE=${ENABLE_BACKSTAGE:-false}
      - ENABLE_CONFLUENCE=${ENABLE_CONFLUENCE:-false}
      - ENABLE_GITHUB=${ENABLE_GITHUB:-false}
      - ENABLE_JIRA=${ENABLE_JIRA:-false}
      - ENABLE_PAGERDUTY=${ENABLE_PAGERDUTY:-false}
      - ENABLE_PETSTORE=${ENABLE_PETSTORE:-false}
      - ENABLE_RAG=${ENABLE_RAG:-false}
      - ENABLE_SLACK=${ENABLE_SLACK:-false}
      - ENABLE_SPLUNK=${ENABLE_SPLUNK:-false}
      - ENABLE_WEBEX=${ENABLE_WEBEX:-false}
      - ENABLE_WEBEX=${ENABLE_WEBEX:-false}
      - ENABLE_WEATHER=${ENABLE_WEATHER:-false}

      # Tracing configuration (will be read from .env if ENABLE_TRACING=true)
      - ENABLE_TRACING=${ENABLE_TRACING:-false}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY:-NOT_SET}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY:-NOT_SET}
      - LANGFUSE_HOST=${LANGFUSE_HOST:-http://langfuse-web:3000}
      - LANGFUSE_SESSION_ID=${LANGFUSE_SESSION_ID:-ai-platform-engineering}
      - LANGFUSE_USER_ID=${LANGFUSE_USER_ID:-platform-engineer}
    command: platform-engineer

  ####################################################################################################
  #                                      SLIM Transport                                              #
  ####################################################################################################
  slim-dataplane:
    image: ghcr.io/agntcy/slim:0.6.1
    container_name: slim-dataplane
    ports: ["46357:46357"]
    environment:
      - PASSWORD=${SLIM_GATEWAY_PASSWORD:-dummy_password}
      - CONFIG_PATH=/config.yaml
    volumes: ["./slim-config.yaml:/config.yaml"]
    command: ["/slim", "--config", "/config.yaml"]
    profiles:
      - slim

  slim-control-plane:
    image: ghcr.io/agntcy/slim/control-plane:0.0.1
    container_name: slim-control-plane
    ports: ["50051:50051", "50052:50052"]
    environment:
      - PASSWORD=${SLIM_GATEWAY_PASSWORD:-dummy_password}
      - CONFIG_PATH=/config.yaml
    volumes: ["./slim-config.yaml:/config.yaml"]
    command: ["/slim", "--config", "/config.yaml"]
    profiles:
      - slim

  ####################################################################################################
  #                                         Sub-Agents                                               #
  ####################################################################################################
  # AWS Agent
  agent-aws:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=aws
    container_name: agent-aws
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.aws_agent.yaml:/app/prompt_config.aws_agent.yaml
      # Mount only agent_aws subdirectory to preserve .venv from build
      - ./ai_platform_engineering/agents/aws/agent_aws:/app/ai_platform_engineering/agents/agent/agent_aws
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8012:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
      - AWS_AGENT_BACKEND=${AWS_AGENT_BACKEND:-langgraph}
      - ENABLE_TRACING=${ENABLE_TRACING:-false}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_HOST=${LANGFUSE_HOST:-http://langfuse-web:3000}
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - ENABLE_EKS_MCP=${ENABLE_EKS_MCP:-false}
      - ENABLE_COST_EXPLORER_MCP=${ENABLE_COST_EXPLORER_MCP:-false}
      - ENABLE_IAM_MCP=${ENABLE_IAM_MCP:-false}
      - IAM_MCP_READONLY=${IAM_MCP_READONLY:-true}
      - USE_AWS_CLI_AS_TOOL=${USE_AWS_CLI_AS_TOOL:-true}
      # Multi-account support - format: "name1:id1,name2:id2" or just "id1,id2"
      - DEFAULT_AWS_ACCOUNT_ID=${DEFAULT_AWS_ACCOUNT_ID:-outshift-common-dev:471112537430}
      - AWS_ACCOUNT_LIST=${AWS_ACCOUNT_LIST:-eticloud:626007623524,outshift-common-dev:471112537430,outshift-common-staging:637423531539,outshift-common-prod:058264538874,eti-ci:009736724745,cisco-research:509581005347,eticloud-demo:075967417574}
      - CROSS_ACCOUNT_ROLE_NAME=${CROSS_ACCOUNT_ROLE_NAME:-caipe-read-only}
      - STRANDS_LOG_LEVEL=${STRANDS_LOG_LEVEL:-INFO}
      - FASTMCP_LOG_LEVEL=${FASTMCP_LOG_LEVEL:-ERROR}
    profiles:
      - aws
      - all-agents

  # Petstore Agent
  agent-petstore:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=template
    container_name: agent-petstore
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.petstore_agent.yaml:/app/prompt_config.petstore_agent.yaml
      # Mount template agent directory (petstore uses template)
      - ./ai_platform_engineering/agents/template/agent_petstore:/app/ai_platform_engineering/agents/agent/agent_petstore
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8013:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_PORT=443
      - MCP_HOST=petstore.outshift.io
      - PETSTORE_API_KEY=${PETSTORE_API_KEY}
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
      - ENABLE_TRACING=${ENABLE_TRACING:-false}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_HOST=${LANGFUSE_HOST:-http://langfuse-web:3000}
    profiles:
      - petstore
      - all-agents

  # GitHub
  agent-github:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=github
    container_name: agent-github
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.github_agent.yaml:/app/prompt_config.github_agent.yaml
      - ./ai_platform_engineering/agents/github/agent_github:/app/ai_platform_engineering/agents/agent/agent_github
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8001:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
      - ENABLE_TRACING=${ENABLE_TRACING:-false}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_HOST=${LANGFUSE_HOST:-http://langfuse-web:3000}
    # Docker socket mount is only needed when MCP_MODE=stdio
    # For local Github Offical MCP server execution.
    # Uncomment this when MCP_MODE=stdio is supported
    # volumes: ["/var/run/docker.sock:/var/run/docker.sock"]
    profiles:
      - github
      - all-agents

  # Weather Agent
  agent-weather:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=weather
    container_name: agent-weather
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.weather_agent.yaml:/app/prompt_config.weather_agent.yaml
      - ./ai_platform_engineering/agents/weather/agent_weather:/app/ai_platform_engineering/agents/agent/agent_weather
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8002:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_HOST=weather.outshift.io
      - MCP_PORT=8000
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
    profiles:
      - weather
      - all-agents

  # Backstage Agent
  agent-backstage:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=backstage
    container_name: agent-backstage
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.backstage_agent.yaml:/app/prompt_config.backstage_agent.yaml
      - ./ai_platform_engineering/agents/backstage/agent_backstage:/app/ai_platform_engineering/agents/agent/agent_backstage
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8003:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_HOST=mcp-backstage
      - MCP_PORT=8000
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
    depends_on:
      mcp-backstage:
        condition: service_healthy
    profiles:
      - backstage
      - all-agents

  mcp-backstage:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.mcp
      args:
        - AGENT_NAME=backstage
    container_name: mcp-backstage
    env_file: [.env]
    ports: ["18001:8000"]
    volumes:
      - ./ai_platform_engineering/agents/backstage/mcp/mcp_backstage:/app/mcp_backstage
    environment:
      - MCP_MODE=http
      - MCP_HOST=0.0.0.0
      - MCP_PORT=8000
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(1); result=s.connect_ex(('localhost', 8000)); s.close(); exit(0 if result == 0 else 1)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    profiles:
      - backstage
      - all-agents

  # ArgoCD Agents
  agent-argocd:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=argocd
    container_name: agent-argocd
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.argocd_agent.yaml:/app/prompt_config.argocd_agent.yaml
      - ./ai_platform_engineering/agents/argocd/agent_argocd:/app/ai_platform_engineering/agents/agent/agent_argocd
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8004:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_HOST=mcp-argocd
      - MCP_PORT=8000
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
    depends_on:
      mcp-argocd:
        condition: service_healthy
    profiles:
      - argocd
      - all-agents

  mcp-argocd:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.mcp
      args:
        - AGENT_NAME=argocd
    container_name: mcp-argocd
    env_file: [.env]
    ports: ["18002:8000"]
    volumes:
      - ./ai_platform_engineering/agents/argocd/mcp/mcp_argocd:/app/mcp_argocd
    environment:
      - MCP_MODE=http
      - MCP_HOST=0.0.0.0
      - MCP_PORT=8000
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(1); result=s.connect_ex(('localhost', 8000)); s.close(); exit(0 if result == 0 else 1)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    profiles:
      - argocd
      - all-agents

  # Confluence Agents
  agent-confluence:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=confluence
    container_name: agent-confluence
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.confluence_agent.yaml:/app/prompt_config.confluence_agent.yaml
      - ./ai_platform_engineering/agents/confluence/agent_confluence:/app/ai_platform_engineering/agents/agent/agent_confluence
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8005:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_HOST=mcp-confluence
      - MCP_PORT=8000
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
    depends_on:
      mcp-confluence:
        condition: service_started
    profiles:
      - confluence
      - all-agents

  mcp-confluence:
    image: ghcr.io/sooperset/mcp-atlassian:latest
    container_name: mcp-confluence
    env_file: [.env]
    ports: ["18003:8000"]
    environment:
      - CONFLUENCE_URL=${CONFLUENCE_URL}
      - CONFLUENCE_USERNAME=${CONFLUENCE_USERNAME}
      - CONFLUENCE_API_TOKEN=${CONFLUENCE_API_TOKEN}
    profiles:
      - confluence
      - all-agents

  # Jira Agent
  agent-jira:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=jira
    container_name: agent-jira
    env_file: [.env]
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.jira_agent.yaml:/app/prompt_config.jira_agent.yaml
      - ./ai_platform_engineering/agents/jira/agent_jira:/app/ai_platform_engineering/agents/agent/agent_jira
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    ports: ["8006:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_HOST=mcp-jira
      - MCP_PORT=8000
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
    depends_on:
      mcp-jira:
        condition: service_healthy
    profiles:
      - jira
      - all-agents

  mcp-jira:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.mcp
      args:
        - AGENT_NAME=jira
    container_name: mcp-jira
    env_file: [.env]
    ports: ["18004:8000"]
    volumes:
      - ./ai_platform_engineering/agents/jira/mcp/mcp_jira:/app/mcp_jira
    environment:
      - MCP_MODE=http
      - MCP_HOST=0.0.0.0
      - MCP_PORT=8000
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(1); result=s.connect_ex(('localhost', 8000)); s.close(); exit(0 if result == 0 else 1)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    profiles:
      - jira
      - all-agents

  # Komodor Agent
  agent-komodor:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=komodor
    container_name: agent-komodor
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.komodor_agent.yaml:/app/prompt_config.komodor_agent.yaml
      - ./ai_platform_engineering/agents/komodor/agent_komodor:/app/ai_platform_engineering/agents/agent/agent_komodor
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8007:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_HOST=mcp-komodor
      - MCP_PORT=8000
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
    depends_on:
      mcp-komodor:
        condition: service_healthy
    profiles:
      - komodor
  mcp-komodor:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.mcp
      args:
        - AGENT_NAME=komodor
    container_name: mcp-komodor
    env_file: [.env]
    ports: ["18005:8000"]
    volumes:
      - ./ai_platform_engineering/agents/komodor/mcp/mcp_komodor:/app/mcp_komodor
    environment:
      - MCP_MODE=http
      - MCP_HOST=0.0.0.0
      - MCP_PORT=8000
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(1); result=s.connect_ex(('localhost', 8000)); s.close(); exit(0 if result == 0 else 1)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    profiles:
      - komodor
      - all-agents

  # PagerDuty Agent
  agent-pagerduty:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=pagerduty
    container_name: agent-pagerduty
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.pagerduty_agent.yaml:/app/prompt_config.pagerduty_agent.yaml
      - ./ai_platform_engineering/agents/pagerduty/agent_pagerduty:/app/ai_platform_engineering/agents/agent/agent_pagerduty
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8008:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_HOST=mcp-pagerduty
      - MCP_PORT=8000
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
    depends_on:
      mcp-pagerduty:
        condition: service_healthy
    profiles:
      - pagerduty
      - all-agents

  mcp-pagerduty:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.mcp
      args:
        - AGENT_NAME=pagerduty
    container_name: mcp-pagerduty
    env_file: [.env]
    ports: ["18006:8000"]
    volumes:
      - ./ai_platform_engineering/agents/pagerduty/mcp/mcp_pagerduty:/app/mcp_pagerduty
    environment:
      - MCP_MODE=http
      - MCP_HOST=0.0.0.0
      - MCP_PORT=8000
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(1); result=s.connect_ex(('localhost', 8000)); s.close(); exit(0 if result == 0 else 1)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    profiles:
      - pagerduty
      - all-agents

  # Slack Agent
  agent-slack:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=slack
    container_name: agent-slack
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.slack_agent.yaml:/app/prompt_config.slack_agent.yaml
      - ./ai_platform_engineering/agents/slack/agent_slack:/app/ai_platform_engineering/agents/agent/agent_slack
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8009:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_HOST=mcp-slack
      - MCP_PORT=8000
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
    depends_on:
      mcp-slack:
        condition: service_healthy
    profiles:
      - slack
      - all-agents

  mcp-slack:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.mcp
      args:
        - AGENT_NAME=slack
    container_name: mcp-slack
    env_file: [.env]
    ports: ["18007:8000"]
    volumes:
      - ./ai_platform_engineering/agents/slack/mcp/mcp_slack:/app/mcp_slack
    environment:
      - MCP_MODE=http
      - MCP_HOST=0.0.0.0
      - MCP_PORT=8000
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(1); result=s.connect_ex(('localhost', 8000)); s.close(); exit(0 if result == 0 else 1)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    profiles:
      - slack
      - all-agents

  # Splunk
  agent-splunk:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=splunk
    container_name: agent-splunk
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.splunk_agent.yaml:/app/prompt_config.splunk_agent.yaml
      - ./ai_platform_engineering/agents/splunk/agent_splunk:/app/ai_platform_engineering/agents/agent/agent_splunk
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8010:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_HOST=mcp-splunk
      - MCP_PORT=8000
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
    depends_on:
      mcp-splunk:
        condition: service_healthy
    profiles:
      - splunk
      - all-agents

  mcp-splunk:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.mcp
      args:
        - AGENT_NAME=splunk
    container_name: mcp-splunk
    env_file: [.env]
    ports: ["18008:8000"]
    volumes:
      - ./ai_platform_engineering/agents/splunk/mcp/mcp_splunk:/app/mcp_splunk
    environment:
      - MCP_MODE=http
      - MCP_HOST=0.0.0.0
      - MCP_PORT=8000
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(1); result=s.connect_ex(('localhost', 8000)); s.close(); exit(0 if result == 0 else 1)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    profiles:
      - splunk
      - all-agents

  # Webex
  agent-webex:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.a2a
      args:
        - AGENT_NAME=webex
    container_name: agent-webex
    volumes:
      - ./charts/ai-platform-engineering/data/prompt_config.webex_agent.yaml:/app/prompt_config.webex_agent.yaml
      - ./ai_platform_engineering/agents/webex/agent_webex:/app/ai_platform_engineering/agents/agent/agent_webex
      - ./ai_platform_engineering/utils:/app/ai_platform_engineering/utils
    env_file: [.env]
    ports: ["8011:8000"]
    environment:
      - A2A_TRANSPORT=${A2A_TRANSPORT:-p2p}
      - MCP_MODE=http
      - MCP_HOST=mcp-webex
      - MCP_PORT=8000
      - SLIM_ENDPOINT=${SLIM_ENDPOINT:-http://slim-dataplane:46357}
    depends_on:
      mcp-webex:
        condition: service_healthy
    profiles:
      - webex
      - all-agents

  mcp-webex:
    build:
      context: .
      dockerfile: build/agents/Dockerfile.mcp
      args:
        - AGENT_NAME=webex
    container_name: mcp-webex
    env_file: [.env]
    ports: ["18009:8000"]
    volumes:
      - ./ai_platform_engineering/agents/webex/mcp/mcp_webex:/app/mcp_webex
    environment:
      - MCP_MODE=http
      - MCP_HOST=0.0.0.0
      - MCP_PORT=8000
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(1); result=s.connect_ex(('localhost', 8000)); s.close(); exit(0 if result == 0 else 1)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    profiles:
      - webex
      - all-agents

  # Backstage Plugin (Agent Forge)
  backstage-agent-forge:
    image: ghcr.io/cnoe-io/backstage-plugin-agent-forge:latest
    container_name: backstage-agent-forge
    ports: ["13000:3000"]
    environment:
      - NODE_ENV=development
      - AGENT_FORGE_BASE_URL=${AGENT_FORGE_BASE_URL:-http://localhost:8000}
    profiles:
      - agentforge
      - all-agents

  ####################################################################################################
  #                                      RAG Services                                                #
  ####################################################################################################
  rag_server:
    image: ghcr.io/cnoe-io/caipe-rag-server:${IMAGE_TAG:-stable}
    container_name: rag_server
    ports: ["9446:9446"]
    environment:
      - LOG_LEVEL=DEBUG
      - REDIS_URL=redis://rag-redis:6379/0
      - NEO4J_ADDR=neo4j://neo4j:7687
      - NEO4J_ONTOLOGY_ADDR=neo4j://neo4j-ontology:7688
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=dummy_password
      - MILVUS_URI=http://milvus-standalone:19530
      - ONTOLOGY_AGENT_RESTAPI_ADDR=http://agent_ontology:8098
      - ENABLE_GRAPH_RAG=${ENABLE_GRAPH_RAG:-true}
      - CLEANUP_INTERVAL=86400
      - EMBEDDINGS_MODEL=${EMBEDDINGS_MODEL:-text-embedding-3-large}
    restart: unless-stopped
    env_file: [.env]
    depends_on: [rag-redis]
    profiles:
      - rag
      - graph_rag

  agent_rag:
    image: ghcr.io/cnoe-io/caipe-rag-agent-rag:${IMAGE_TAG:-stable}
    container_name: agent_rag
    ports: ["8099:8099"]
    env_file: [.env]
    environment:
      - REDIS_URL=redis://rag-redis:6379/0
      - NEO4J_ADDR=neo4j://neo4j:7687
      - NEO4J_ONTOLOGY_ADDR=neo4j://neo4j-ontology:7688
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=dummy_password
      - RAG_SERVER_URL=http://rag_server:9446
      - ENABLE_GRAPH_RAG=${ENABLE_GRAPH_RAG:-true}
    restart: unless-stopped
    profiles:
      - rag
      - graph_rag

  agent_ontology:
    image: ghcr.io/cnoe-io/caipe-rag-agent-ontology:${IMAGE_TAG:-stable}
    container_name: agent_ontology
    ports: ["8098:8098"]
    environment:
      - LOG_LEVEL=DEBUG
      - REDIS_URL=redis://rag-redis:6379/0
      - NEO4J_ADDR=neo4j://neo4j:7687
      - NEO4J_ONTOLOGY_ADDR=neo4j://neo4j-ontology:7688
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=dummy_password
      - SYNC_INTERVAL=86400
    env_file: [.env]
    restart: unless-stopped
    depends_on: [rag_server, neo4j, neo4j-ontology, rag-redis]
    profiles:
      - graph_rag

  rag_webui:
    image: ghcr.io/cnoe-io/caipe-rag-webui:${IMAGE_TAG:-stable}
    container_name: rag-webui
    environment:
      - RAG_SERVER_URL=http://rag_server:9446
      - NGINX_ENVSUBST_TEMPLATE_SUFFIX=.conf
    depends_on: [rag_server]
    ports: ["9447:80"]
    profiles:
      - rag
      - graph_rag

  # RAG Dependencies
  neo4j:
    image: neo4j:latest
    container_name: neo4j
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/neo4j/logs:/logs
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/neo4j/config:/config
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/neo4j/data:/data
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/neo4j/plugins:/plugins
    ports: ["7474:7474", "7687:7687"]
    restart: unless-stopped
    environment:
      - NEO4J_AUTH=neo4j/dummy_password
      - NEO4J_PLUGINS='["apoc"]'
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
    profiles:
      - graph_rag

  neo4j-ontology:
    image: neo4j:latest
    container_name: neo4j-ontology
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/neo4j-ontology/logs:/logs
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/neo4j-ontology/config:/config
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/neo4j-ontology/data:/data
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/neo4j-ontology/plugins:/plugins
    ports: ["7688:7687"]
    restart: unless-stopped
    environment:
      - NEO4J_AUTH=neo4j/dummy_password
      - NEO4J_PLUGINS='["apoc"]'
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
    profiles:
      - graph_rag

  rag-redis:
    image: redis
    container_name: rag-redis
    volumes:
      - rag_redis_data:/data
    command: ["/bin/sh", "-c", "redis-server --save 60 1 --appendonly yes"]
    ports: [":6379"]
    restart: unless-stopped
    profiles:
      - rag
      - graph_rag

  milvus-standalone:
    image: milvusdb/milvus:v2.6.0
    container_name: milvus-standalone
    command: ["milvus", "run", "standalone"]
    security_opt: [seccomp:unconfined]
    environment:
      - MINIO_REGION=us-east-1
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=milvus-minio:9000
      - LOG_LEVEL=error
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    ports: [":19530", ":9091"]
    depends_on: [etcd, milvus-minio]
    profiles:
      - rag
      - graph_rag

  etcd:
    image: quay.io/coreos/etcd:v3.5.18
    container_name: milvus-etcd
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
    command: etcd -advertise-client-urls=http://etcd:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3
    profiles:
      - rag
      - graph_rag

  milvus-minio:
    image: minio/minio:RELEASE.2024-05-28T17-19-04Z
    container_name: milvus-minio
    environment:
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
    ports: [":9001", ":9000"]
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    profiles:
      - rag
      - graph_rag

  # Langfuse Tracing Services
  langfuse-worker:
    image: langfuse/langfuse-worker:3
    container_name: langfuse-worker
    restart: always
    depends_on:
      langfuse-postgres:
        condition: service_healthy
      langfuse-minio:
        condition: service_healthy
      langfuse-redis:
        condition: service_healthy
      langfuse-clickhouse:
        condition: service_healthy
    ports: ["127.0.0.1:3030:3030"]
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@langfuse-postgres:5432/postgres
      - SALT=mysalt
      - ENCRYPTION_KEY=0000000000000000000000000000000000000000000000000000000000000000
      - CLICKHOUSE_MIGRATION_URL=clickhouse://langfuse-clickhouse:9000
      - CLICKHOUSE_URL=http://langfuse-clickhouse:8123
      - CLICKHOUSE_USER=clickhouse
      - CLICKHOUSE_PASSWORD=clickhouse
      - CLICKHOUSE_CLUSTER_ENABLED=false
      - LANGFUSE_S3_EVENT_UPLOAD_BUCKET=langfuse
      - LANGFUSE_S3_EVENT_UPLOAD_REGION=us-east-1
      - LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID=minio
      - LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY=miniosecret
      - LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT=http://langfuse-minio:9000
      - LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE=true
      - LANGFUSE_S3_EVENT_UPLOAD_PREFIX=events/
      - LANGFUSE_S3_MEDIA_UPLOAD_BUCKET=langfuse
      - LANGFUSE_S3_MEDIA_UPLOAD_REGION=us-east-1
      - LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID=minio
      - LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY=miniosecret
      - LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT=http://langfuse-minio:9000
      - LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE=true
      - LANGFUSE_S3_MEDIA_UPLOAD_PREFIX=media/
      - REDIS_HOST=langfuse-redis
      - REDIS_AUTH=myredissecret
    profiles:
      - tracing

  langfuse-web:
    image: langfuse/langfuse:3
    container_name: langfuse-web
    restart: always
    depends_on:
      langfuse-postgres:
        condition: service_healthy
      langfuse-minio:
        condition: service_healthy
      langfuse-redis:
        condition: service_healthy
      langfuse-clickhouse:
        condition: service_healthy
    ports: ["3000:3000"]
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@langfuse-postgres:5432/postgres
      - SALT=mysalt
      - ENCRYPTION_KEY=0000000000000000000000000000000000000000000000000000000000000000
      - CLICKHOUSE_MIGRATION_URL=clickhouse://langfuse-clickhouse:9000
      - CLICKHOUSE_URL=http://langfuse-clickhouse:8123
      - CLICKHOUSE_USER=clickhouse
      - HOSTNAME=0.0.0.0
      - CLICKHOUSE_PASSWORD=clickhouse
      - CLICKHOUSE_CLUSTER_ENABLED=false
      - LANGFUSE_S3_EVENT_UPLOAD_BUCKET=langfuse
      - LANGFUSE_S3_EVENT_UPLOAD_REGION=us-east-1
      - LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID=minio
      - LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY=miniosecret
      - LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT=http://langfuse-minio:9000
      - LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE=true
      - LANGFUSE_S3_EVENT_UPLOAD_PREFIX=events/
      - LANGFUSE_S3_MEDIA_UPLOAD_BUCKET=langfuse
      - LANGFUSE_S3_MEDIA_UPLOAD_REGION=us-east-1
      - LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID=minio
      - LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY=miniosecret
      - LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT=http://langfuse-minio:9000
      - LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE=true
      - LANGFUSE_S3_MEDIA_UPLOAD_PREFIX=media/
      - REDIS_HOST=langfuse-redis
      - REDIS_AUTH=myredissecret
      - NEXTAUTH_URL=http://localhost:3000
      - NEXTAUTH_SECRET=mysecret
    profiles:
      - tracing

  langfuse-clickhouse:
    image: clickhouse/clickhouse-server
    container_name: langfuse-clickhouse
    restart: always
    user: "101:101"
    environment:
      - CLICKHOUSE_DB=default
      - CLICKHOUSE_USER=clickhouse
      - CLICKHOUSE_PASSWORD=clickhouse
    volumes:
      - langfuse_clickhouse_data:/var/lib/clickhouse
      - langfuse_clickhouse_logs:/var/log/clickhouse-server
    ports: ["127.0.0.1:8123:8123", "127.0.0.1:9000:9000"]
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 1s
    profiles:
      - tracing

  langfuse-minio:
    image: minio/minio
    container_name: langfuse-minio
    restart: always
    entrypoint: sh
    command: -c 'mkdir -p /data/langfuse && minio server --address ":9000" --console-address ":9001" /data'
    environment:
      - MINIO_ROOT_USER=minio
      - MINIO_ROOT_PASSWORD=miniosecret
    ports: ["9090:9000", "127.0.0.1:9091:9001"]
    volumes:
      - langfuse_minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 1s
      timeout: 5s
      retries: 5
      start_period: 1s
    profiles:
      - tracing

  langfuse-redis:
    image: redis:7
    container_name: langfuse-redis
    restart: always
    volumes:
      - langfuse_redis_data:/data
    command: --requirepass ${REDIS_AUTH:-myredissecret} --appendonly yes
    ports: ["127.0.0.1:6379:6379"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 3s
      timeout: 10s
      retries: 10
    profiles:
      - tracing

  langfuse-postgres:
    image: postgres:15
    container_name: langfuse-postgres
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 3s
      timeout: 3s
      retries: 10
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
    ports: ["127.0.0.1:5432:5432"]
    volumes:
      - langfuse_postgres_data:/var/lib/postgresql/data
    profiles:
      - tracing

  # Evaluation Webhook Service
  evaluation-webhook:
    build:
      context: ./evals
      dockerfile: Dockerfile
    container_name: evaluation-webhook
    restart: unless-stopped
    depends_on:
      langfuse-web:
        condition: service_started
    ports: ["8024:8000"]
    env_file: [.env]
    environment:
      - PLATFORM_ENGINEER_URL=http://platform-engineering:8000
      - LANGFUSE_HOST=${LANGFUSE_HOST:-http://langfuse-web:3000}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    volumes:
      - ./evals/datasets:/app/datasets
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    profiles:
      - evaluation

volumes:
  langfuse_postgres_data:
    driver: local
  langfuse_clickhouse_data:
    driver: local
  langfuse_clickhouse_logs:
    driver: local
  langfuse_minio_data:
    driver: local
  langfuse_redis_data:
    driver: local
  rag_redis_data:
    driver: local
  milvus_etcd:
    driver: local
  milvus_minio:
    driver: local
  milvus_data:
    driver: local
