---
# Source: rag-stack/charts/agent-ontology/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: agent-ontology
  labels:
    helm.sh/chart: agent-ontology-0.0.1
    app.kubernetes.io/name: agent-ontology
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: rag-stack/charts/agent-rag/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: agent-rag
  labels:
    helm.sh/chart: agent-rag-0.0.1
    app.kubernetes.io/name: agent-rag
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: rag-stack/charts/milvus/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "test-release-minio"
  namespace: "default"
  labels:
    app: minio
    chart: minio-8.0.17
    release: "test-release"
---
# Source: rag-stack/charts/rag-redis/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rag-redis
  labels:
    helm.sh/chart: rag-redis-0.0.1
    app.kubernetes.io/name: rag-redis
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: rag-stack/charts/agent-ontology/templates/llm-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: llm-secret
  labels:
    helm.sh/chart: agent-ontology-0.0.1
    app.kubernetes.io/name: agent-ontology
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
---
# Source: rag-stack/charts/agent-rag/templates/llm-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: llm-secret
  labels:
    helm.sh/chart: agent-rag-0.0.1
    app.kubernetes.io/name: agent-rag
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
---
# Source: rag-stack/charts/milvus/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: test-release-minio
  labels:
    app: minio
    chart: minio-8.0.17
    release: test-release
    heritage: Helm
type: Opaque
data:
  accesskey: "bWluaW9hZG1pbg=="
  secretkey: "bWluaW9hZG1pbg=="
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-auth.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "rag-neo4j-ontology-auth"
  namespace: "default"
  labels:
    app: "rag-neo4j-ontology"    
type: Opaque
data:
  NEO4J_AUTH: "bmVvNGovZHVtbXlfcGFzc3dvcmQ="
---
# Source: rag-stack/charts/neo4j/templates/neo4j-auth.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "rag-neo4j-auth"
  namespace: "default"
  labels:
    app: "rag-neo4j"    
type: Opaque
data:
  NEO4J_AUTH: "bmVvNGovZHVtbXlfcGFzc3dvcmQ="
---
# Source: rag-stack/charts/milvus/charts/minio/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-release-minio
  labels:
    app: minio
    chart: minio-8.0.17
    release: test-release
    heritage: Helm
data:
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for Minio service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/accesskey) ; SECRET=$(cat /config/secretkey) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to Minio server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} config host add myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of `mc ls`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(${MC} ls myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }
    
    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
      VERSIONING=$4
    
      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for `true`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo "Purging bucket '$BUCKET'."
          set +e ; # don't exit if this fails
          ${MC} rm -r --force myminio/$BUCKET
          set -e ; # reset `e` as active
        else
          echo "Bucket '$BUCKET' does not exist, skipping purge."
        fi
      fi
    
      # Create the bucket if it does not exist
      if ! checkBucketExists $BUCKET ; then
        echo "Creating bucket '$BUCKET'"
        ${MC} mb myminio/$BUCKET
      else
        echo "Bucket '$BUCKET' already exists."
      fi
    
    
      # set versioning for bucket
      if [ ! -z $VERSIONING ] ; then
        if [ $VERSIONING = true ] ; then
            echo "Enabling versioning for '$BUCKET'"
            ${MC} version enable myminio/$BUCKET
        elif [ $VERSIONING = false ] ; then
            echo "Suspending versioning for '$BUCKET'"
            ${MC} version suspend myminio/$BUCKET
        fi
      else
          echo "Bucket '$BUCKET' versioning unchanged."
      fi
    
      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
      ${MC} policy set $POLICY myminio/$BUCKET
    }
    
    # Try connecting to Minio instance
    scheme=http
    connectToMinio $scheme
---
# Source: rag-stack/charts/milvus/templates/configmap.yaml
# If customConfigMap is not set, this ConfigMap will be redendered.
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-release-milvus
  namespace: default
data:
  default.yaml: |+
    # Copyright (C) 2019-2021 Zilliz. All rights reserved.
    #
    # Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance
    # with the License. You may obtain a copy of the License at
    #
    # http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software distributed under the License
    # is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
    # or implied. See the License for the specific language governing permissions and limitations under the License.
    
    etcd:
      endpoints:
      - test-release-etcd-0.test-release-etcd-headless.default.svc.cluster.local:2379
      - test-release-etcd-1.test-release-etcd-headless.default.svc.cluster.local:2379
      - test-release-etcd-2.test-release-etcd-headless.default.svc.cluster.local:2379
    
    metastore:
      type: etcd
    
    minio:
      address: test-release-minio
      port: 9000
      accessKeyID: minioadmin
      secretAccessKey: minioadmin
      useSSL: false
      bucketName: milvus-bucket
      rootPath: file
      useIAM: false
      useVirtualHost: false
    
    mq:
      type: woodpecker
    
    messageQueue: woodpecker
    
    rootCoord:
      address: test-release-milvus-rootcoord
      port: 53100
      enableActiveStandby: false  # Enable rootcoord active-standby
    
    proxy:
      port: 19530
      internalPort: 19529
    
    queryCoord:
      address: test-release-milvus-querycoord
      port: 19531
    
      enableActiveStandby: false  # Enable querycoord active-standby
    
    queryNode:
      port: 21123
      enableDisk: true # Enable querynode load disk index, and search on disk index
    
    indexCoord:
      address: test-release-milvus-indexcoord
      port: 31000
      enableActiveStandby: false  # Enable indexcoord active-standby
    
    indexNode:
      port: 21121
      enableDisk: true # Enable index node build disk vector index
    
    dataCoord:
      address: test-release-milvus-datacoord
      port: 13333
      enableActiveStandby: false  # Enable datacoord active-standby
    
    dataNode:
      port: 21124
    
    log:
      level: info
      file:
        rootPath: ""
        maxSize: 300
        maxAge: 10
        maxBackups: 20
      format: text
  user.yaml: |-
    #    For example enable rest http for milvus proxy
    #    proxy:
    #      http:
    #        enabled: true
    #      maxUserNum: 100
    #      maxRoleNum: 10
    ##  Enable tlsMode and set the tls cert and key
    #  tls:
    #    serverPemPath: /etc/milvus/certs/tls.crt
    #    serverKeyPath: /etc/milvus/certs/tls.key
    #   common:
    #     security:
    #       tlsMode: 1
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-config.yaml
# Neo4j config values that are required for neo4j to work correctly in Kubernetes, these are not overridden by user-provided values
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-ontology-k8s-config"
  namespace: "default"
  labels:
    app: "rag-neo4j-ontology"    
data:
  server.default_listen_address: "0.0.0.0"
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-config.yaml
# User-provided Neo4j config values
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-ontology-user-config"
  namespace: "default"
  labels:
    app: "rag-neo4j-ontology"    
data:
  dbms.security.procedures.allowlist: apoc.*
  dbms.security.procedures.unrestricted: apoc.*
  dbms.security.tls_reload_enabled: "true"
  internal.dbms.sharded_property_database.enabled: "false"
  server.config.strict_validation.enabled: "false"
  server.directories.plugins: /var/lib/neo4j/labs
  server.jvm.additional: |-  
    -XX:+UseG1GC
    -XX:-OmitStackTraceInFastThrow
    -XX:+AlwaysPreTouch
    -XX:+UnlockExperimentalVMOptions
    -XX:+TrustFinalNonStaticFields
    -XX:+DisableExplicitGC
    -Djdk.nio.maxCachedBufferSize=1024
    -Dio.netty.tryReflectionSetAccessible=true
    -Djdk.tls.ephemeralDHKeySize=2048
    -Djdk.tls.rejectClientInitiatedRenegotiation=true
    -XX:FlightRecorderOptions=stackdepth=256
    -XX:+UnlockDiagnosticVMOptions
    -XX:+DebugNonSafepoints
    --add-opens=java.base/java.nio=ALL-UNNAMED
    --add-opens=java.base/java.io=ALL-UNNAMED
    --add-opens=java.base/sun.nio.ch=ALL-UNNAMED
    -Dlog4j2.disable.jmx=true
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-config.yaml
# Default Neo4j config values, these are overridden by user-provided values in rag-neo4j-ontology-user-config
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-ontology-default-config"
  namespace: "default"
  labels:
    app: "rag-neo4j-ontology"    
data:

  # Neo4j defaults
  db.tx_log.rotation.retention_policy: 1 days
  server.windows_service_name: neo4j
  
  server.logs.config: /config/server-logs.xml/server-logs.xml
  server.logs.user.config: /config/user-logs.xml/user-logs.xml

  # Helm defaults

  # Bolt keep alive
  # this helps to ensure that LoadBalancers do not close bolt connections that are in use but appear idle
  server.bolt.connection_keep_alive: "30s"
  server.bolt.connection_keep_alive_for_requests: "ALL"
  server.bolt.connection_keep_alive_streaming_scheduling_interval: "30s"

  # If we set default advertised address it over-rides the bolt address used to populate the browser in a really annoying way
  # dbms.default_advertised_address: "$(bash -c 'echo ${SERVICE_DOMAIN}')"


  # Other
  internal.dbms.ssl.system.ignore_dot_files: "true"

  # set the below configs in case of cluster or analytics
  # Logging
  server.directories.logs: "/logs"
  # Import
  server.directories.import: "/import"

  # Use more reliable defaults SSL / TLS settings for K8s
  dbms.ssl.policy.bolt.client_auth: "NONE"
  dbms.ssl.policy.https.client_auth: "NONE"
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-config.yaml
# User-provided Neo4j Apoc config values
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-ontology-apoc-config"
  namespace: "default"
  labels:
    app: "rag-neo4j-ontology"    
data:
  apoc.conf: |-
      apoc.import.file.enabled=true
      apoc.trigger.enabled=true
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-config.yaml
# server-logs.xml as configmap
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-ontology-server-logs-config"
  namespace: "default"
  labels:
    app: "rag-neo4j-ontology"    
data:
  server-logs.xml: |-
    <?xml version="1.0" encoding="UTF-8"?>
    <!--
    
        Copyright (c) "Neo4j"
        Neo4j Sweden AB [http://neo4j.com]
        This file is a commercial add-on to Neo4j Enterprise Edition.
    
    -->
    <!--
        This is a log4j 2 configuration file.
    
        It is highly recommended to keep the original "debug.log" as is, to make sure enough data is captured in case
        of errors in a format that neo4j developers can work with.
    
        All configuration values can be queried with the lookup prefix "config:". You can for example, resolve
        the path to your neo4j home directory with ${config:dbms.directories.neo4j_home}.
    
        Please consult https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions and
        available configuration options.
    -->
    <Configuration status="ERROR" monitorInterval="30" packages="org.neo4j.logging.log4j">
        <Appenders>
            <!-- Default debug.log, please keep -->
            <RollingRandomAccessFile name="DebugLog" fileName="${config:server.directories.logs}/debug.log"
                                     filePattern="$${config:server.directories.logs}/debug.log.%02i">
                <Neo4jDebugLogLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p [%c{1.}] %m%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="20 MB"/>
                </Policies>
                <DefaultRolloverStrategy fileIndex="min" max="7"/>
            </RollingRandomAccessFile>
    
            <RollingRandomAccessFile name="HttpLog" fileName="${config:server.directories.logs}/http.log"
                                     filePattern="$${config:server.directories.logs}/http.log.%02i">
                <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="20 MB"/>
                </Policies>
                <DefaultRolloverStrategy fileIndex="min" max="5"/>
            </RollingRandomAccessFile>
    
            <RollingRandomAccessFile name="QueryLog" fileName="${config:server.directories.logs}/query.log"
                                     filePattern="$${config:server.directories.logs}/query.log.%02i">
                <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="20 MB"/>
                </Policies>
                <DefaultRolloverStrategy fileIndex="min" max="7"/>
            </RollingRandomAccessFile>
    
            <RollingRandomAccessFile name="SecurityLog" fileName="${config:server.directories.logs}/security.log"
                                     filePattern="$${config:server.directories.logs}/security.log.%02i">
                <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="20 MB"/>
                </Policies>
                <DefaultRolloverStrategy fileIndex="min" max="7"/>
            </RollingRandomAccessFile>
        </Appenders>
    
        <Loggers>
            <!-- Log levels. One of DEBUG, INFO, WARN, ERROR or OFF -->
    
            <!-- The debug log is used as the root logger to catch everything -->
            <Root level="INFO">
                <AppenderRef ref="DebugLog"/> <!-- Keep this -->
            </Root>
    
            <!-- The query log, must be named "QueryLogger" -->
            <Logger name="QueryLogger" level="INFO" additivity="false">
                <AppenderRef ref="QueryLog"/>
            </Logger>
    
            <!-- The http request log, must be named "HttpLogger" -->
            <Logger name="HttpLogger" level="INFO" additivity="false">
                <AppenderRef ref="HttpLog"/>
            </Logger>
    
            <!-- The security log, must be named "SecurityLogger" -->
            <Logger name="SecurityLogger" level="INFO" additivity="false">
                <AppenderRef ref="SecurityLog"/>
            </Logger>
        </Loggers>
    </Configuration>
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-config.yaml
# user-logs.xml as configmap
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-ontology-user-logs-config"
  namespace: "default"
  labels:
    app: "rag-neo4j-ontology"    
data:
  user-logs.xml: |-
    <?xml version="1.0" encoding="UTF-8"?>
    <!--
    
        Copyright (c) "Neo4j"
        Neo4j Sweden AB [http://neo4j.com]
        This file is a commercial add-on to Neo4j Enterprise Edition.
    
    -->
    <!--
        This is a log4j 2 configuration file that provides maximum flexibility.
    
        All configuration values can be queried with the lookup prefix "config:". You can for example, resolve
        the path to your neo4j home directory with ${config:dbms.directories.neo4j_home}.
    
        Please consult https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions and
        available configuration options.
    -->
    <Configuration status="ERROR" monitorInterval="30" packages="org.neo4j.logging.log4j">
    
        <Appenders>
            <RollingRandomAccessFile name="Neo4jLog" fileName="${config:server.directories.logs}/neo4j.log"
                                     filePattern="$${config:server.directories.logs}/neo4j.log.%02i">
                <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="20 MB"/>
                </Policies>
                <DefaultRolloverStrategy fileIndex="min" max="7"/>
            </RollingRandomAccessFile>
    
            <!-- Only used by "neo4j console", will be ignored otherwise -->
            <Console name="ConsoleAppender" target="SYSTEM_OUT">
                <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
            </Console>
        </Appenders>
    
        <Loggers>
            <!-- Log level for the neo4j log. One of DEBUG, INFO, WARN, ERROR or OFF -->
            <Root level="INFO">
                <AppenderRef ref="Neo4jLog"/>
                <AppenderRef ref="ConsoleAppender"/>
            </Root>
        </Loggers>
    
    </Configuration>
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-env.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-ontology-env"
  namespace: "default"
  labels:
    app: "rag-neo4j-ontology"    
data:
  # It should not be necessary for neo4j users/administrators to modify this configMap
  # Neo4j configuration is set in the rag-neo4j-ontology-user-config ConfigMap
  NEO4J_AUTH_PATH: "/config/neo4j-auth/NEO4J_AUTH"
  NEO4J_EDITION: "COMMUNITY_K8S"
  NEO4J_CONF: "/config/"
  K8S_NEO4J_NAME: "rag-neo4j-ontology"
  EXTENDED_CONF: "yes"
---
# Source: rag-stack/charts/neo4j/templates/neo4j-config.yaml
# Neo4j config values that are required for neo4j to work correctly in Kubernetes, these are not overridden by user-provided values
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-k8s-config"
  namespace: "default"
  labels:
    app: "rag-neo4j"    
data:
  server.default_listen_address: "0.0.0.0"
---
# Source: rag-stack/charts/neo4j/templates/neo4j-config.yaml
# User-provided Neo4j config values
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-user-config"
  namespace: "default"
  labels:
    app: "rag-neo4j"    
data:
  dbms.security.procedures.allowlist: apoc.*
  dbms.security.procedures.unrestricted: apoc.*
  dbms.security.tls_reload_enabled: "true"
  internal.dbms.sharded_property_database.enabled: "false"
  server.config.strict_validation.enabled: "false"
  server.directories.plugins: /var/lib/neo4j/labs
  server.jvm.additional: |-  
    -XX:+UseG1GC
    -XX:-OmitStackTraceInFastThrow
    -XX:+AlwaysPreTouch
    -XX:+UnlockExperimentalVMOptions
    -XX:+TrustFinalNonStaticFields
    -XX:+DisableExplicitGC
    -Djdk.nio.maxCachedBufferSize=1024
    -Dio.netty.tryReflectionSetAccessible=true
    -Djdk.tls.ephemeralDHKeySize=2048
    -Djdk.tls.rejectClientInitiatedRenegotiation=true
    -XX:FlightRecorderOptions=stackdepth=256
    -XX:+UnlockDiagnosticVMOptions
    -XX:+DebugNonSafepoints
    --add-opens=java.base/java.nio=ALL-UNNAMED
    --add-opens=java.base/java.io=ALL-UNNAMED
    --add-opens=java.base/sun.nio.ch=ALL-UNNAMED
    -Dlog4j2.disable.jmx=true
---
# Source: rag-stack/charts/neo4j/templates/neo4j-config.yaml
# Default Neo4j config values, these are overridden by user-provided values in rag-neo4j-user-config
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-default-config"
  namespace: "default"
  labels:
    app: "rag-neo4j"    
data:

  # Neo4j defaults
  db.tx_log.rotation.retention_policy: 1 days
  server.windows_service_name: neo4j
  
  server.logs.config: /config/server-logs.xml/server-logs.xml
  server.logs.user.config: /config/user-logs.xml/user-logs.xml

  # Helm defaults

  # Bolt keep alive
  # this helps to ensure that LoadBalancers do not close bolt connections that are in use but appear idle
  server.bolt.connection_keep_alive: "30s"
  server.bolt.connection_keep_alive_for_requests: "ALL"
  server.bolt.connection_keep_alive_streaming_scheduling_interval: "30s"

  # If we set default advertised address it over-rides the bolt address used to populate the browser in a really annoying way
  # dbms.default_advertised_address: "$(bash -c 'echo ${SERVICE_DOMAIN}')"


  # Other
  internal.dbms.ssl.system.ignore_dot_files: "true"

  # set the below configs in case of cluster or analytics
  # Logging
  server.directories.logs: "/logs"
  # Import
  server.directories.import: "/import"

  # Use more reliable defaults SSL / TLS settings for K8s
  dbms.ssl.policy.bolt.client_auth: "NONE"
  dbms.ssl.policy.https.client_auth: "NONE"
---
# Source: rag-stack/charts/neo4j/templates/neo4j-config.yaml
# User-provided Neo4j Apoc config values
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-apoc-config"
  namespace: "default"
  labels:
    app: "rag-neo4j"    
data:
  apoc.conf: |-
      apoc.import.file.enabled=true
      apoc.trigger.enabled=true
---
# Source: rag-stack/charts/neo4j/templates/neo4j-config.yaml
# server-logs.xml as configmap
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-server-logs-config"
  namespace: "default"
  labels:
    app: "rag-neo4j"    
data:
  server-logs.xml: |-
    <?xml version="1.0" encoding="UTF-8"?>
    <!--
    
        Copyright (c) "Neo4j"
        Neo4j Sweden AB [http://neo4j.com]
        This file is a commercial add-on to Neo4j Enterprise Edition.
    
    -->
    <!--
        This is a log4j 2 configuration file.
    
        It is highly recommended to keep the original "debug.log" as is, to make sure enough data is captured in case
        of errors in a format that neo4j developers can work with.
    
        All configuration values can be queried with the lookup prefix "config:". You can for example, resolve
        the path to your neo4j home directory with ${config:dbms.directories.neo4j_home}.
    
        Please consult https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions and
        available configuration options.
    -->
    <Configuration status="ERROR" monitorInterval="30" packages="org.neo4j.logging.log4j">
        <Appenders>
            <!-- Default debug.log, please keep -->
            <RollingRandomAccessFile name="DebugLog" fileName="${config:server.directories.logs}/debug.log"
                                     filePattern="$${config:server.directories.logs}/debug.log.%02i">
                <Neo4jDebugLogLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p [%c{1.}] %m%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="20 MB"/>
                </Policies>
                <DefaultRolloverStrategy fileIndex="min" max="7"/>
            </RollingRandomAccessFile>
    
            <RollingRandomAccessFile name="HttpLog" fileName="${config:server.directories.logs}/http.log"
                                     filePattern="$${config:server.directories.logs}/http.log.%02i">
                <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="20 MB"/>
                </Policies>
                <DefaultRolloverStrategy fileIndex="min" max="5"/>
            </RollingRandomAccessFile>
    
            <RollingRandomAccessFile name="QueryLog" fileName="${config:server.directories.logs}/query.log"
                                     filePattern="$${config:server.directories.logs}/query.log.%02i">
                <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="20 MB"/>
                </Policies>
                <DefaultRolloverStrategy fileIndex="min" max="7"/>
            </RollingRandomAccessFile>
    
            <RollingRandomAccessFile name="SecurityLog" fileName="${config:server.directories.logs}/security.log"
                                     filePattern="$${config:server.directories.logs}/security.log.%02i">
                <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="20 MB"/>
                </Policies>
                <DefaultRolloverStrategy fileIndex="min" max="7"/>
            </RollingRandomAccessFile>
        </Appenders>
    
        <Loggers>
            <!-- Log levels. One of DEBUG, INFO, WARN, ERROR or OFF -->
    
            <!-- The debug log is used as the root logger to catch everything -->
            <Root level="INFO">
                <AppenderRef ref="DebugLog"/> <!-- Keep this -->
            </Root>
    
            <!-- The query log, must be named "QueryLogger" -->
            <Logger name="QueryLogger" level="INFO" additivity="false">
                <AppenderRef ref="QueryLog"/>
            </Logger>
    
            <!-- The http request log, must be named "HttpLogger" -->
            <Logger name="HttpLogger" level="INFO" additivity="false">
                <AppenderRef ref="HttpLog"/>
            </Logger>
    
            <!-- The security log, must be named "SecurityLogger" -->
            <Logger name="SecurityLogger" level="INFO" additivity="false">
                <AppenderRef ref="SecurityLog"/>
            </Logger>
        </Loggers>
    </Configuration>
---
# Source: rag-stack/charts/neo4j/templates/neo4j-config.yaml
# user-logs.xml as configmap
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-user-logs-config"
  namespace: "default"
  labels:
    app: "rag-neo4j"    
data:
  user-logs.xml: |-
    <?xml version="1.0" encoding="UTF-8"?>
    <!--
    
        Copyright (c) "Neo4j"
        Neo4j Sweden AB [http://neo4j.com]
        This file is a commercial add-on to Neo4j Enterprise Edition.
    
    -->
    <!--
        This is a log4j 2 configuration file that provides maximum flexibility.
    
        All configuration values can be queried with the lookup prefix "config:". You can for example, resolve
        the path to your neo4j home directory with ${config:dbms.directories.neo4j_home}.
    
        Please consult https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions and
        available configuration options.
    -->
    <Configuration status="ERROR" monitorInterval="30" packages="org.neo4j.logging.log4j">
    
        <Appenders>
            <RollingRandomAccessFile name="Neo4jLog" fileName="${config:server.directories.logs}/neo4j.log"
                                     filePattern="$${config:server.directories.logs}/neo4j.log.%02i">
                <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="20 MB"/>
                </Policies>
                <DefaultRolloverStrategy fileIndex="min" max="7"/>
            </RollingRandomAccessFile>
    
            <!-- Only used by "neo4j console", will be ignored otherwise -->
            <Console name="ConsoleAppender" target="SYSTEM_OUT">
                <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
            </Console>
        </Appenders>
    
        <Loggers>
            <!-- Log level for the neo4j log. One of DEBUG, INFO, WARN, ERROR or OFF -->
            <Root level="INFO">
                <AppenderRef ref="Neo4jLog"/>
                <AppenderRef ref="ConsoleAppender"/>
            </Root>
        </Loggers>
    
    </Configuration>
---
# Source: rag-stack/charts/neo4j/templates/neo4j-env.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "rag-neo4j-env"
  namespace: "default"
  labels:
    app: "rag-neo4j"    
data:
  # It should not be necessary for neo4j users/administrators to modify this configMap
  # Neo4j configuration is set in the rag-neo4j-user-config ConfigMap
  NEO4J_AUTH_PATH: "/config/neo4j-auth/NEO4J_AUTH"
  NEO4J_EDITION: "COMMUNITY_K8S"
  NEO4J_CONF: "/config/"
  K8S_NEO4J_NAME: "rag-neo4j"
  EXTENDED_CONF: "yes"
---
# Source: rag-stack/charts/agent-ontology/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: agent-ontology
  labels:
    helm.sh/chart: agent-ontology-0.0.1
    app.kubernetes.io/name: agent-ontology
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8098
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: agent-ontology
    app.kubernetes.io/instance: test-release
---
# Source: rag-stack/charts/agent-rag/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: agent-rag
  labels:
    helm.sh/chart: agent-rag-0.0.1
    app.kubernetes.io/name: agent-rag
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8099
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: agent-rag
    app.kubernetes.io/instance: test-release
---
# Source: rag-stack/charts/milvus/charts/etcd/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-release-etcd-headless
  namespace: default
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-6.3.3
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: "client"
      port: 2379
      targetPort: client
    - name: "peer"
      port: 2380
      targetPort: peer
  selector:
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: test-release
---
# Source: rag-stack/charts/milvus/charts/etcd/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-release-etcd
  namespace: default
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-6.3.3
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  ports:
    - name: "client"
      port: 2379
      targetPort: client
      nodePort: null
    - name: "peer"
      port: 2380
      targetPort: peer
      nodePort: null
  selector:
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: test-release
---
# Source: rag-stack/charts/milvus/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-release-minio
  labels:
    app: minio
    chart: minio-8.0.17
    release: test-release
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio
    release: test-release
---
# Source: rag-stack/charts/milvus/charts/minio/templates/statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-release-minio-svc
  labels:
    app: minio
    chart: minio-8.0.17
    release: "test-release"
    heritage: "Helm"
spec:
  publishNotReadyAddresses: true
  clusterIP: None
  ports:
    - name: http
      port: 9000
      protocol: TCP
  selector:
    app: minio
    release: test-release
---
# Source: rag-stack/charts/milvus/templates/datanode-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-release-milvus-datanode
  namespace: default
  labels:
    helm.sh/chart: milvus-5.0.2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "2.6.1"
    app.kubernetes.io/managed-by: Helm
    component: "datanode"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: metrics
      protocol: TCP
      port: 9091
      targetPort: metrics
  selector:
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    component: "datanode"
---
# Source: rag-stack/charts/milvus/templates/mixcoord-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-release-milvus-mixcoord
  namespace: default
  labels:
    helm.sh/chart: milvus-5.0.2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "2.6.1"
    app.kubernetes.io/managed-by: Helm
    component: "mixcoord"
spec:
  type: ClusterIP
  ports:
    - name: metrics
      protocol: TCP
      port: 9091
      targetPort: metrics
  selector:
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    component: "mixcoord"
---
# Source: rag-stack/charts/milvus/templates/querynode-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-release-milvus-querynode
  namespace: default
  labels:
    helm.sh/chart: milvus-5.0.2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "2.6.1"
    app.kubernetes.io/managed-by: Helm
    component: "querynode"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: metrics
      protocol: TCP
      port: 9091
      targetPort: metrics
  selector:
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    component: "querynode"
---
# Source: rag-stack/charts/milvus/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-release-milvus
  namespace: default
  labels:
    helm.sh/chart: milvus-5.0.2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "2.6.1"
    app.kubernetes.io/managed-by: Helm
    component: "proxy"
spec:
  type: ClusterIP
  ports:
    - name: milvus
      port: 19530
      protocol: TCP
      targetPort: milvus
    - name: metrics
      protocol: TCP
      port: 9091
      targetPort: metrics
  selector:
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    component: "proxy"
---
# Source: rag-stack/charts/milvus/templates/streamingnode-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-release-milvus-streamingnode
  namespace: default
  labels:
    helm.sh/chart: milvus-5.0.2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "2.6.1"
    app.kubernetes.io/managed-by: Helm
    component: "streamingnode"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: metrics
      protocol: TCP
      port: 9091
      targetPort: metrics
  selector:
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    component: "streamingnode"
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-svc.yaml
# ClusterIP service for bolt / http connections
apiVersion: v1
kind: Service
metadata:
  name: "rag-neo4j-ontology"
  namespace: "default"
  labels:
    helm.neo4j.com/neo4j.name: "rag-neo4j-ontology"
    app: "rag-neo4j-ontology"
    helm.neo4j.com/instance: "rag-neo4j-ontology"
    helm.neo4j.com/service: "default"    
spec:
  publishNotReadyAddresses: false
  type: ClusterIP
  selector:
    app: "rag-neo4j-ontology"
    helm.neo4j.com/instance: "rag-neo4j-ontology"
  ports:
    - protocol: TCP
      port: 7687
      targetPort: 7687
      name: tcp-bolt
    - protocol: TCP
      port: 7474
      targetPort: 7474
      name: tcp-http
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-svc.yaml
# ClusterIP service for admin connections to Neo4j inside Kubernetes.
apiVersion: v1
kind: Service
metadata:
  name: "rag-neo4j-ontology-admin"
  namespace: "default"
  labels:
    helm.neo4j.com/neo4j.name: "rag-neo4j-ontology"
    app: "rag-neo4j-ontology"
    helm.neo4j.com/instance: "rag-neo4j-ontology"
    helm.neo4j.com/service: "admin"    
spec:
  publishNotReadyAddresses: true
  type: "ClusterIP"
  selector:
    app: "rag-neo4j-ontology"
    helm.neo4j.com/instance: "rag-neo4j-ontology"
  ports:
    - protocol: TCP
      port: 7687
      targetPort: 7687
      name: tcp-bolt
    - protocol: TCP
      port: 7474
      targetPort: 7474
      name: tcp-http
---
# Source: rag-stack/charts/neo4j/templates/neo4j-svc.yaml
# ClusterIP service for bolt / http connections
apiVersion: v1
kind: Service
metadata:
  name: "rag-neo4j"
  namespace: "default"
  labels:
    helm.neo4j.com/neo4j.name: "rag-neo4j"
    app: "rag-neo4j"
    helm.neo4j.com/instance: "rag-neo4j"
    helm.neo4j.com/service: "default"    
spec:
  publishNotReadyAddresses: false
  type: ClusterIP
  selector:
    app: "rag-neo4j"
    helm.neo4j.com/instance: "rag-neo4j"
  ports:
    - protocol: TCP
      port: 7687
      targetPort: 7687
      name: tcp-bolt
    - protocol: TCP
      port: 7474
      targetPort: 7474
      name: tcp-http
---
# Source: rag-stack/charts/neo4j/templates/neo4j-svc.yaml
# ClusterIP service for admin connections to Neo4j inside Kubernetes.
apiVersion: v1
kind: Service
metadata:
  name: "rag-neo4j-admin"
  namespace: "default"
  labels:
    helm.neo4j.com/neo4j.name: "rag-neo4j"
    app: "rag-neo4j"
    helm.neo4j.com/instance: "rag-neo4j"
    helm.neo4j.com/service: "admin"    
spec:
  publishNotReadyAddresses: true
  type: "ClusterIP"
  selector:
    app: "rag-neo4j"
    helm.neo4j.com/instance: "rag-neo4j"
  ports:
    - protocol: TCP
      port: 7687
      targetPort: 7687
      name: tcp-bolt
    - protocol: TCP
      port: 7474
      targetPort: 7474
      name: tcp-http
---
# Source: rag-stack/charts/rag-redis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: rag-redis
  labels:
    helm.sh/chart: rag-redis-0.0.1
    app.kubernetes.io/name: rag-redis
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 6379
      targetPort: redis
      protocol: TCP
      name: redis
  selector:
    app.kubernetes.io/name: rag-redis
    app.kubernetes.io/instance: test-release
---
# Source: rag-stack/charts/rag-server/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: rag-server
  labels:
    helm.sh/chart: rag-server-0.0.1
    app.kubernetes.io/name: rag-server
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9446
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: rag-server
    app.kubernetes.io/instance: test-release
---
# Source: rag-stack/charts/rag-webui/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-release-rag-webui
  labels:
    helm.sh/chart: rag-webui-0.0.1
    app.kubernetes.io/name: rag-webui
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: rag-webui
    app.kubernetes.io/instance: test-release
---
# Source: rag-stack/charts/agent-ontology/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: agent-ontology
  labels:
    helm.sh/chart: agent-ontology-0.0.1
    app.kubernetes.io/name: agent-ontology
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: agent-ontology
      app.kubernetes.io/instance: test-release
  template:
    metadata:
      labels:
        helm.sh/chart: agent-ontology-0.0.1
        app.kubernetes.io/name: agent-ontology
        app.kubernetes.io/instance: test-release
        app.kubernetes.io/version: "0.0.1"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: agent-ontology
      containers:
        - name: agent-ontology
          image: "ghcr.io/cnoe-io/caipe-rag-agent-ontology:latest"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 8098
              protocol: TCP
          envFrom:
            - secretRef:
                name: llm-secret
          env:
            - name: LOG_LEVEL
              value: "DEBUG"
            - name: REDIS_URL
              value: "redis://rag-redis:6379/0"
            - name: NEO4J_ADDR
              value: "neo4j://rag-neo4j:7687"
            - name: NEO4J_ONTOLOGY_ADDR
              value: "neo4j://rag-neo4j-ontology:7688"
            - name: NEO4J_USERNAME
              value: "neo4j"
            - name: NEO4J_PASSWORD
              value: "dummy_password"
            - name: SYNC_INTERVAL
              value: "86400"
          livenessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 10
            tcpSocket:
              port: http
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 10
            tcpSocket:
              port: http
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              ephemeral-storage: 1Gi
              memory: 2Gi
            requests:
              cpu: 100m
              ephemeral-storage: 256Mi
              memory: 128Mi
          volumeMounts:
      volumes:
---
# Source: rag-stack/charts/agent-rag/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: agent-rag
  labels:
    helm.sh/chart: agent-rag-0.0.1
    app.kubernetes.io/name: agent-rag
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: agent-rag
      app.kubernetes.io/instance: test-release
  template:
    metadata:
      labels:
        helm.sh/chart: agent-rag-0.0.1
        app.kubernetes.io/name: agent-rag
        app.kubernetes.io/instance: test-release
        app.kubernetes.io/version: "0.0.1"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: agent-rag
      containers:
        - name: agent-rag
          image: "ghcr.io/cnoe-io/caipe-rag-agent-rag:latest"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 8099
              protocol: TCP
          envFrom:
            - secretRef:
                name: llm-secret
          env:
            - name: LOG_LEVEL
              value: "DEBUG"
            - name: REDIS_URL
              value: "redis://rag-redis:6379/0"
            - name: NEO4J_ADDR
              value: "neo4j://rag-neo4j:7687"
            - name: NEO4J_ONTOLOGY_ADDR
              value: "neo4j://rag-neo4j-ontology:7688"
            - name: NEO4J_USERNAME
              value: "neo4j"
            - name: NEO4J_PASSWORD
              value: "dummy_password"
            - name: ENABLE_GRAPH_RAG
              value: "true"
          livenessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 10
            tcpSocket:
              port: http
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 10
            tcpSocket:
              port: http
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              ephemeral-storage: 1Gi
              memory: 2Gi
            requests:
              cpu: 100m
              ephemeral-storage: 256Mi
              memory: 128Mi
          volumeMounts:
      volumes:
---
# Source: rag-stack/charts/milvus/templates/datanode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-release-milvus-datanode
  namespace: default
  labels:
    helm.sh/chart: milvus-5.0.2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "2.6.1"
    app.kubernetes.io/managed-by: Helm
    component: "datanode"

    
  annotations:
    

spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: milvus
      app.kubernetes.io/instance: test-release
      component: "datanode"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: milvus
        app.kubernetes.io/instance: test-release
        component: "datanode"
        
      annotations:
        checksum/config: adf89b3889e7f4c7fbcb8c1b7e96b5d906691e82c2176c13eedb1540d9f103fd
        
    spec:
      serviceAccountName: default
      initContainers:
      containers:
      - name: datanode
        image: "milvusdb/milvus:v2.6.1"
        imagePullPolicy: IfNotPresent
        
        
        args: [ "milvus", "run", "datanode" ]
        env:
        ports:
          - name: datanode
            containerPort: 21124
            protocol: TCP
          - name: metrics
            containerPort: 9091
            protocol: TCP
        livenessProbe:
          tcpSocket:
            port: metrics
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: metrics
          initialDelaySeconds: 90
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - name: milvus-config
          mountPath: /milvus/configs/default.yaml
          subPath: default.yaml
          readOnly: true
        - name: milvus-config
          mountPath: /milvus/configs/user.yaml
          subPath: user.yaml
          readOnly: true
        - mountPath: /milvus/tools
          name: tools
      volumes:
      - name: milvus-config
        configMap:
          name: test-release-milvus
      - name: tools
        emptyDir: {}
---
# Source: rag-stack/charts/milvus/templates/mixcoord-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-release-milvus-mixcoord
  namespace: default
  labels:
    helm.sh/chart: milvus-5.0.2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "2.6.1"
    app.kubernetes.io/managed-by: Helm
    component: "mixcoord"
    
  annotations:
    

spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: milvus
      app.kubernetes.io/instance: test-release
      component: "mixcoord"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: milvus
        app.kubernetes.io/instance: test-release
        
        component: "mixcoord"
      annotations:
        checksum/config: adf89b3889e7f4c7fbcb8c1b7e96b5d906691e82c2176c13eedb1540d9f103fd
        
    spec:
      serviceAccountName: default
      initContainers:
      containers:
      - name: mixcoord
        image: "milvusdb/milvus:v2.6.1"
        imagePullPolicy: IfNotPresent
        
        
        args: [ "milvus", "run", "mixture", "-rootcoord", "-querycoord", "-datacoord", "-indexcoord" ]
        env:
        ports:
          - name: metrics
            containerPort: 9091
            protocol: TCP
        livenessProbe:
          tcpSocket:
            port: metrics
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: metrics
          initialDelaySeconds: 90
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        resources:
          {}
        volumeMounts:
        - name: milvus-config
          mountPath: /milvus/configs/default.yaml
          subPath: default.yaml
          readOnly: true
        - name: milvus-config
          mountPath: /milvus/configs/user.yaml
          subPath: user.yaml
          readOnly: true
        - mountPath: /milvus/tools
          name: tools

      volumes:
      - name: milvus-config
        configMap:
          name: test-release-milvus
      - name: tools
        emptyDir: {}
---
# Source: rag-stack/charts/milvus/templates/proxy-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-release-milvus-proxy
  namespace: default
  labels:
    helm.sh/chart: milvus-5.0.2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "2.6.1"
    app.kubernetes.io/managed-by: Helm
    component: "proxy"

    
  annotations:
    

spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: milvus
      app.kubernetes.io/instance: test-release
      component: "proxy"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: milvus
        app.kubernetes.io/instance: test-release
        component: "proxy"
        
      annotations:
        checksum/config: adf89b3889e7f4c7fbcb8c1b7e96b5d906691e82c2176c13eedb1540d9f103fd
        
    spec:
      serviceAccountName: default
      initContainers:
      containers:
      - name: proxy
        image: "milvusdb/milvus:v2.6.1"
        imagePullPolicy: IfNotPresent
        
        
        args: [ "milvus", "run", "proxy" ]
        env:
        ports:
          - name: milvus
            containerPort: 19530
            protocol: TCP
          - name: metrics
            containerPort: 9091
            protocol: TCP
        livenessProbe:
          tcpSocket:
            port: metrics
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: metrics
          initialDelaySeconds: 90
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        resources:
          {}
        volumeMounts:
        - name: milvus-config
          mountPath: /milvus/configs/default.yaml
          subPath: default.yaml
          readOnly: true
        - name: milvus-config
          mountPath: /milvus/configs/user.yaml
          subPath: user.yaml
          readOnly: true
        - mountPath: /milvus/tools
          name: tools

      volumes:
      - name: milvus-config
        configMap:
          name: test-release-milvus
      - name: tools
        emptyDir: {}
---
# Source: rag-stack/charts/milvus/templates/querynode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-release-milvus-querynode
  namespace: default
  labels:
    helm.sh/chart: milvus-5.0.2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "2.6.1"
    app.kubernetes.io/managed-by: Helm
    component: "querynode"
    
  annotations:
    

spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: milvus
      app.kubernetes.io/instance: test-release
      component: "querynode"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: milvus
        app.kubernetes.io/instance: test-release
        component: "querynode"
        
      annotations:
        checksum/config: adf89b3889e7f4c7fbcb8c1b7e96b5d906691e82c2176c13eedb1540d9f103fd
        
    spec:
      serviceAccountName: default
      initContainers:
      containers:
      - name: querynode
        image: "milvusdb/milvus:v2.6.1"
        imagePullPolicy: IfNotPresent
        
        
        args: [ "milvus", "run", "querynode" ]
        env:
        ports:
          - name: querynode
            containerPort: 21123
            protocol: TCP
          - name: metrics
            containerPort: 9091
            protocol: TCP
        livenessProbe:
          tcpSocket:
            port: metrics
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: metrics
          initialDelaySeconds: 90
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - name: milvus-config
          mountPath: /milvus/configs/default.yaml
          subPath: default.yaml
          readOnly: true
        - name: milvus-config
          mountPath: /milvus/configs/user.yaml
          subPath: user.yaml
          readOnly: true
        - mountPath: /milvus/tools
          name: tools
        - mountPath: /var/lib/milvus/data
          name: disk

      volumes:
      - name: milvus-config
        configMap:
          name: test-release-milvus
      - name: tools
        emptyDir: {}
      - name: disk
        emptyDir: {}
---
# Source: rag-stack/charts/milvus/templates/streamingnode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-release-milvus-streamingnode
  namespace: default
  labels:
    helm.sh/chart: milvus-5.0.2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "2.6.1"
    app.kubernetes.io/managed-by: Helm
    component: "streamingnode"

    
  annotations:
    

spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: milvus
      app.kubernetes.io/instance: test-release
      component: "streamingnode"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: milvus
        app.kubernetes.io/instance: test-release
        component: "streamingnode"
        
      annotations:
        checksum/config: adf89b3889e7f4c7fbcb8c1b7e96b5d906691e82c2176c13eedb1540d9f103fd
        
    spec:
      serviceAccountName: default
      initContainers:
      containers:
      - name: streamingnode
        image: "milvusdb/milvus:v2.6.1"
        imagePullPolicy: IfNotPresent
        
        
        args: [ "milvus", "run", "streamingnode" ]
        env:
        ports:
          - name: streamingnode
            containerPort: 22222
            protocol: TCP
          - name: metrics
            containerPort: 9091
            protocol: TCP
        livenessProbe:
          tcpSocket:
            port: metrics
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: metrics
          initialDelaySeconds: 90
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        resources:
          {}
        volumeMounts:
        - name: milvus-config
          mountPath: /milvus/configs/default.yaml
          subPath: default.yaml
          readOnly: true
        - name: milvus-config
          mountPath: /milvus/configs/user.yaml
          subPath: user.yaml
          readOnly: true
        - mountPath: /milvus/tools
          name: tools
        - mountPath: /var/lib/milvus
          name: woodpecker
      volumes:
      - name: milvus-config
        configMap:
          name: test-release-milvus
      - name: tools
        emptyDir: {}
      - name: woodpecker
        emptyDir: {}
---
# Source: rag-stack/charts/rag-redis/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-redis
  labels:
    helm.sh/chart: rag-redis-0.0.1
    app.kubernetes.io/name: rag-redis
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: rag-redis
      app.kubernetes.io/instance: test-release
  template:
    metadata:
      labels:
        helm.sh/chart: rag-redis-0.0.1
        app.kubernetes.io/name: rag-redis
        app.kubernetes.io/instance: test-release
        app.kubernetes.io/version: "0.0.1"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: rag-redis
      containers:
        - name: rag-redis
          image: "redis:7.2-alpine"
          imagePullPolicy: IfNotPresent
          command:
            - redis-server
            - --appendonly
            - "yes"
            - --save
            - "60 1"
            - --maxmemory
            - "256mb"
            - --maxmemory-policy
            - "allkeys-lru"
          ports:
            - name: redis
              containerPort: 6379
              protocol: TCP
          livenessProbe:
            exec:
              command:
              - redis-cli
              - ping
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          readinessProbe:
            exec:
              command:
              - redis-cli
              - ping
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
---
# Source: rag-stack/charts/rag-server/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-server
  labels:
    helm.sh/chart: rag-server-0.0.1
    app.kubernetes.io/name: rag-server
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: rag-server
      app.kubernetes.io/instance: test-release
  template:
    metadata:
      labels:
        helm.sh/chart: rag-server-0.0.1
        app.kubernetes.io/name: rag-server
        app.kubernetes.io/instance: test-release
        app.kubernetes.io/version: "0.0.1"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: default
      automountServiceAccountToken: false
      containers:
        - name: rag-server
          image: "ghcr.io/cnoe-io/caipe-rag-server:latest"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 9446
              protocol: TCP
          envFrom:
            - secretRef:
                name: llm-secret
          env:
            - name: LOG_LEVEL
              value: "DEBUG"
            - name: REDIS_URL
              value: "redis://rag-redis:6379/0"
            - name: NEO4J_ADDR
              value: "neo4j://rag-neo4j:7687"
            - name: NEO4J_ONTOLOGY_ADDR
              value: "neo4j://rag-neo4j-ontology:7688"
            - name: NEO4J_USERNAME
              value: "neo4j"
            - name: NEO4J_PASSWORD
              value: "dummy_password"
            - name: MILVUS_URI
              value: "http://test-release-milvus:19530"
            - name: ONTOLOGY_AGENT_RESTAPI_ADDR
              value: "http://agent-ontology:8098"
            - name: ENABLE_GRAPH_RAG
              value: ""
            - name: CLEANUP_INTERVAL
              value: "86400"
          resources:
            limits:
              cpu: 500m
              ephemeral-storage: 1Gi
              memory: 512Mi
            requests:
              cpu: 100m
              ephemeral-storage: 256Mi
              memory: 128Mi
---
# Source: rag-stack/charts/rag-webui/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-release-rag-webui
  labels:
    helm.sh/chart: rag-webui-0.0.1
    app.kubernetes.io/name: rag-webui
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: rag-webui
      app.kubernetes.io/instance: test-release
  template:
    metadata:
      labels:
        helm.sh/chart: rag-webui-0.0.1
        app.kubernetes.io/name: rag-webui
        app.kubernetes.io/instance: test-release
        app.kubernetes.io/version: "0.0.1"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: default
      automountServiceAccountToken: false
      containers:
        - name: rag-webui
          image: "ghcr.io/cnoe-io/caipe-rag-webui:latest"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
---
# Source: rag-stack/charts/milvus/charts/etcd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: test-release-etcd
  namespace: default
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-6.3.3
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: etcd
      app.kubernetes.io/instance: test-release
  serviceName: test-release-etcd-headless
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: etcd
        helm.sh/chart: etcd-6.3.3
        app.kubernetes.io/instance: test-release
        app.kubernetes.io/managed-by: Helm
      annotations:
    spec:
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: etcd
                    app.kubernetes.io/instance: test-release
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: "default"
      containers:
        - name: etcd
          image: docker.io/milvusdb/etcd:3.5.18-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ETCDCTL_API
              value: "3"
            - name: ETCD_ON_K8S
              value: "yes"
            - name: ETCD_START_FROM_SNAPSHOT
              value: "no"
            - name: ETCD_DISASTER_RECOVERY
              value: "no"
            - name: ETCD_NAME
              value: "$(MY_POD_NAME)"
            - name: ETCD_DATA_DIR
              value: "/bitnami/etcd/data"
            - name: ETCD_LOG_LEVEL
              value: "info"
            - name: ALLOW_NONE_AUTHENTICATION
              value: "yes"
            - name: ETCD_ADVERTISE_CLIENT_URLS
              value: "http://$(MY_POD_NAME).test-release-etcd-headless.default.svc.cluster.local:2379"
            - name: ETCD_LISTEN_CLIENT_URLS
              value: "http://0.0.0.0:2379"
            - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
              value: "http://$(MY_POD_NAME).test-release-etcd-headless.default.svc.cluster.local:2380"
            - name: ETCD_LISTEN_PEER_URLS
              value: "http://0.0.0.0:2380"
            - name: ETCD_AUTO_COMPACTION_MODE
              value: "revision"
            - name: ETCD_AUTO_COMPACTION_RETENTION
              value: "1000"
            - name: ETCD_INITIAL_CLUSTER_TOKEN
              value: "etcd-cluster-k8s"
            - name: ETCD_INITIAL_CLUSTER_STATE
              value: "new"
            - name: ETCD_INITIAL_CLUSTER
              value: "test-release-etcd-0=http://test-release-etcd-0.test-release-etcd-headless.default.svc.cluster.local:2380,test-release-etcd-1=http://test-release-etcd-1.test-release-etcd-headless.default.svc.cluster.local:2380,test-release-etcd-2=http://test-release-etcd-2.test-release-etcd-headless.default.svc.cluster.local:2380"
            - name: ETCD_CLUSTER_DOMAIN
              value: "test-release-etcd-headless.default.svc.cluster.local"
            - name: ETCD_QUOTA_BACKEND_BYTES
              value: "4294967296"
            - name: ETCD_HEARTBEAT_INTERVAL
              value: "500"
            - name: ETCD_ELECTION_TIMEOUT
              value: "2500"
          envFrom:
          ports:
            - name: client
              containerPort: 2379
              protocol: TCP
            - name: peer
              containerPort: 2380
              protocol: TCP
          livenessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 20
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 5
          lifecycle:
            preStop:
              exec:
                command:
                  - /opt/bitnami/scripts/etcd/prestop.sh
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/etcd
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "10Gi"
---
# Source: rag-stack/charts/milvus/charts/minio/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: test-release-minio
  labels:
    app: minio
    chart: minio-8.0.17
    release: test-release
    heritage: Helm
spec:
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: "Parallel"
  serviceName: test-release-minio-svc
  replicas: 4
  selector:
    matchLabels:
      app: minio
      release: test-release
  template:
    metadata:
      name: test-release-minio
      labels:
        app: minio
        release: test-release
      annotations:
        checksum/secrets: 85dcdfd7c6e154b23411a81670081a877c30815874cc05621f4a41fa9e6d8a4c
        checksum/config: 63ab91e4d8ffbdcdfdb709c72e04b38e0341e8f5850146809d2dd3e7d78dc053
    spec:
      serviceAccountName: "test-release-minio"
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      containers:
        - name: minio
          image: minio/minio:RELEASE.2024-12-18T13-15-44Z
          imagePullPolicy: IfNotPresent

          command: [ "/bin/sh",
            "-ce",
            "/usr/bin/docker-entrypoint.sh minio -S /etc/minio/certs/ server  http://test-release-minio-{0...3}.test-release-minio-svc.default.svc.cluster.local/export" ]
          volumeMounts:
            - name: export
              mountPath: /export            
          ports:
            - name: http
              containerPort: 9000
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            tcpSocket:
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
          startupProbe:
            tcpSocket:
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 60
          env:
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: test-release-minio
                  key: accesskey
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: test-release-minio
                  key: secretkey
          resources:
            requests:
              memory: 2Gi      
      volumes:
        - name: minio-user
          secret:
            secretName: test-release-minio        
  volumeClaimTemplates:
    - metadata:
        name: export
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 500Gi
---
# Source: rag-stack/charts/neo4j-ontology/templates/neo4j-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    helm.neo4j.com/neo4j.name: "rag-neo4j-ontology"
    helm.neo4j.com/clustering: "false"
    app: "rag-neo4j-ontology"
    helm.neo4j.com/instance: rag-neo4j-ontology    
  name: rag-neo4j-ontology
  namespace: "default"
spec:
  serviceName: "rag-neo4j-ontology"
  podManagementPolicy: "Parallel" # This setting means that the StatefulSet controller doesn't block applying changes until the existing Pod is READY.
  replicas: 1
  selector:
    matchLabels:
      app: "rag-neo4j-ontology"
      helm.neo4j.com/instance: "rag-neo4j-ontology"
  template:
    metadata:
      labels:
        app: "rag-neo4j-ontology"
        helm.neo4j.com/neo4j.name: "rag-neo4j-ontology"
        helm.neo4j.com/clustering: "false"
        helm.neo4j.com/pod_category: "neo4j-instance" # used for anti affinity rules
        helm.neo4j.com/neo4j.loadbalancer: "include"
        helm.neo4j.com/instance: "rag-neo4j-ontology"        
      annotations:
        "checksum/rag-neo4j-ontology-config": fa6d326af9da7ff6a59bc3918a758cc1aba8ec4cf09ae41122e0fc724c927ddf
        "checksum/rag-neo4j-ontology-env": 37e52e81a72dd7da6d6d39ba99b3a27f5942c10f6d96e88bb1373320969e1a00        
    spec:      
      affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchLabels:
                    app: "rag-neo4j-ontology"
                    helm.neo4j.com/pod_category: "neo4j-instance"
                topologyKey: kubernetes.io/hostname
      dnsPolicy: ClusterFirst
      securityContext: 
        fsGroup: 7474
        fsGroupChangePolicy: Always
        runAsGroup: 7474
        runAsNonRoot: true
        runAsUser: 7474            
      
      terminationGracePeriodSeconds: 3600            
      containers:
        - name: "neo4j"
          image: "neo4j:2025.07.1"
          imagePullPolicy: "IfNotPresent"
          envFrom:
            - configMapRef:
                name: "rag-neo4j-ontology-env"
          env:
            - name: HELM_NEO4J_VERSION
              value: "2025.07.1"
            - name: HELM_CHART_VERSION
              value: "2025.7.1"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: SERVICE_NEO4J_ADMIN
              value: "rag-neo4j-ontology-admin.default.svc.cluster.local"
            - name: SERVICE_NEO4J_INTERNALS
              value: "rag-neo4j-ontology-internals.default.svc.cluster.local"
            - name: SERVICE_NEO4J
              value: "rag-neo4j-ontology.default.svc.cluster.local"
          ports:
            - containerPort: 7474
              name: http
            - containerPort: 7687
              name: bolt
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext: 
              capabilities:
                drop:
                - ALL
              runAsGroup: 7474
              runAsNonRoot: true
              runAsUser: 7474
          volumeMounts:
            - mountPath: "/config/neo4j.conf"
              name: neo4j-conf
            - mountPath: "/config/server-logs.xml"
              name: neo4j-server-logs
            - mountPath: "/config/user-logs.xml"
              name: neo4j-user-logs
            - mountPath: "/config/neo4j-auth"
              name: neo4j-auth
                        
            - mountPath: "/config/"
              name: "apoc-conf"                                    
            - mountPath: "/backups"
              name: "data"
              subPathExpr: "backups"
            - mountPath: "/data"
              name: "data"
              subPathExpr: "data"
            - mountPath: "/import"
              name: "data"
              subPathExpr: "import"
            - mountPath: "/licenses"
              name: "data"
              subPathExpr: "licenses"
            - mountPath: "/logs"
              name: "data"
              subPathExpr: "logs/$(POD_NAME)"
            - mountPath: "/metrics"
              name: "data"
              subPathExpr: "metrics/$(POD_NAME)"            
          # Allow user to override the readinessProbe settings for advanced use cases
          # See: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#probe-v1-core
          readinessProbe:
            # Allow overriding the probe action
            tcpSocket:
              port: 7687
            # If no probe action is set, default to using tcpSocket probe to check port 7687 to check if bolt connections can be made
            failureThreshold: 20
            timeoutSeconds: 10
            periodSeconds: 5
          # Allow user to override the livenessProbe settings for advanced use cases
          # See: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#probe-v1-core
          livenessProbe:
            # Allow overriding the probe action
            tcpSocket:
              port: 7687
            # If no probe action is set, default to using tcpSocket probe to check port 7687 to check if bolt connections can be made
            failureThreshold: 40
            timeoutSeconds: 10
            periodSeconds: 5
          # Allow user to override the startupProbe settings for advanced use cases
          # See: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#probe-v1-core
          startupProbe:
            # Allow overriding the probe action
            tcpSocket:
              port: 7687
            failureThreshold: 1000
            periodSeconds: 5
      volumes:
        - name: neo4j-conf
          projected:
            defaultMode: 0440
            sources:
              - configMap:
                  name: "rag-neo4j-ontology-default-config"
              - configMap:
                  name: "rag-neo4j-ontology-user-config"
              - configMap:
                  name: "rag-neo4j-ontology-k8s-config"
        - name: neo4j-server-logs
          configMap:
            name: "rag-neo4j-ontology-server-logs-config"
        - name: neo4j-user-logs
          configMap:
            name: "rag-neo4j-ontology-user-logs-config"
        - name: "neo4j-auth"
          secret:
            secretName: "rag-neo4j-ontology-auth"                
        - name: apoc-conf
          projected:
            defaultMode: 0440
            sources:
              - configMap:
                  name: "rag-neo4j-ontology-apoc-config"                
  volumeClaimTemplates: 
    - metadata:
        name: "data"
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi
        storageClassName: gp2
---
# Source: rag-stack/charts/neo4j/templates/neo4j-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    helm.neo4j.com/neo4j.name: "rag-neo4j"
    helm.neo4j.com/clustering: "false"
    app: "rag-neo4j"
    helm.neo4j.com/instance: rag-neo4j    
  name: rag-neo4j
  namespace: "default"
spec:
  serviceName: "rag-neo4j"
  podManagementPolicy: "Parallel" # This setting means that the StatefulSet controller doesn't block applying changes until the existing Pod is READY.
  replicas: 1
  selector:
    matchLabels:
      app: "rag-neo4j"
      helm.neo4j.com/instance: "rag-neo4j"
  template:
    metadata:
      labels:
        app: "rag-neo4j"
        helm.neo4j.com/neo4j.name: "rag-neo4j"
        helm.neo4j.com/clustering: "false"
        helm.neo4j.com/pod_category: "neo4j-instance" # used for anti affinity rules
        helm.neo4j.com/neo4j.loadbalancer: "include"
        helm.neo4j.com/instance: "rag-neo4j"        
      annotations:
        "checksum/rag-neo4j-config": eb782b019b5f87b3256bc95e1beb9e324a3b96b9cb9028cc6d395b864e5282bf
        "checksum/rag-neo4j-env": 08371b3b60da6c14792c3d5f09d0237fef53afa8ef184e9021a674f69a29d764        
    spec:      
      affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchLabels:
                    app: "rag-neo4j"
                    helm.neo4j.com/pod_category: "neo4j-instance"
                topologyKey: kubernetes.io/hostname
      dnsPolicy: ClusterFirst
      securityContext: 
        fsGroup: 7474
        fsGroupChangePolicy: Always
        runAsGroup: 7474
        runAsNonRoot: true
        runAsUser: 7474            
      
      terminationGracePeriodSeconds: 3600            
      containers:
        - name: "neo4j"
          image: "neo4j:2025.07.1"
          imagePullPolicy: "IfNotPresent"
          envFrom:
            - configMapRef:
                name: "rag-neo4j-env"
          env:
            - name: HELM_NEO4J_VERSION
              value: "2025.07.1"
            - name: HELM_CHART_VERSION
              value: "2025.7.1"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: SERVICE_NEO4J_ADMIN
              value: "rag-neo4j-admin.default.svc.cluster.local"
            - name: SERVICE_NEO4J_INTERNALS
              value: "rag-neo4j-internals.default.svc.cluster.local"
            - name: SERVICE_NEO4J
              value: "rag-neo4j.default.svc.cluster.local"
          ports:
            - containerPort: 7474
              name: http
            - containerPort: 7687
              name: bolt
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext: 
              capabilities:
                drop:
                - ALL
              runAsGroup: 7474
              runAsNonRoot: true
              runAsUser: 7474
          volumeMounts:
            - mountPath: "/config/neo4j.conf"
              name: neo4j-conf
            - mountPath: "/config/server-logs.xml"
              name: neo4j-server-logs
            - mountPath: "/config/user-logs.xml"
              name: neo4j-user-logs
            - mountPath: "/config/neo4j-auth"
              name: neo4j-auth
                        
            - mountPath: "/config/"
              name: "apoc-conf"                                    
            - mountPath: "/backups"
              name: "data"
              subPathExpr: "backups"
            - mountPath: "/data"
              name: "data"
              subPathExpr: "data"
            - mountPath: "/import"
              name: "data"
              subPathExpr: "import"
            - mountPath: "/licenses"
              name: "data"
              subPathExpr: "licenses"
            - mountPath: "/logs"
              name: "data"
              subPathExpr: "logs/$(POD_NAME)"
            - mountPath: "/metrics"
              name: "data"
              subPathExpr: "metrics/$(POD_NAME)"            
          # Allow user to override the readinessProbe settings for advanced use cases
          # See: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#probe-v1-core
          readinessProbe:
            # Allow overriding the probe action
            tcpSocket:
              port: 7687
            # If no probe action is set, default to using tcpSocket probe to check port 7687 to check if bolt connections can be made
            failureThreshold: 20
            timeoutSeconds: 10
            periodSeconds: 5
          # Allow user to override the livenessProbe settings for advanced use cases
          # See: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#probe-v1-core
          livenessProbe:
            # Allow overriding the probe action
            tcpSocket:
              port: 7687
            # If no probe action is set, default to using tcpSocket probe to check port 7687 to check if bolt connections can be made
            failureThreshold: 40
            timeoutSeconds: 10
            periodSeconds: 5
          # Allow user to override the startupProbe settings for advanced use cases
          # See: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#probe-v1-core
          startupProbe:
            # Allow overriding the probe action
            tcpSocket:
              port: 7687
            failureThreshold: 1000
            periodSeconds: 5
      volumes:
        - name: neo4j-conf
          projected:
            defaultMode: 0440
            sources:
              - configMap:
                  name: "rag-neo4j-default-config"
              - configMap:
                  name: "rag-neo4j-user-config"
              - configMap:
                  name: "rag-neo4j-k8s-config"
        - name: neo4j-server-logs
          configMap:
            name: "rag-neo4j-server-logs-config"
        - name: neo4j-user-logs
          configMap:
            name: "rag-neo4j-user-logs-config"
        - name: "neo4j-auth"
          secret:
            secretName: "rag-neo4j-auth"                
        - name: apoc-conf
          projected:
            defaultMode: 0440
            sources:
              - configMap:
                  name: "rag-neo4j-apoc-config"                
  volumeClaimTemplates: 
    - metadata:
        name: "data"
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi
        storageClassName: gp2
---
# Source: rag-stack/charts/rag-webui/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-release-rag-webui
  labels:
    helm.sh/chart: rag-webui-0.0.1
    app.kubernetes.io/name: rag-webui
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
    - host: "rag-webui.local"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: test-release-rag-webui
                port:
                  number: 80
---
# Source: rag-stack/charts/agent-ontology/templates/tests/test-agent-comprehensive.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "agent-ontology-test-comprehensive"
  labels:
    helm.sh/chart: agent-ontology-0.0.1
    app.kubernetes.io/name: agent-ontology
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: test
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "2"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  restartPolicy: Never
  containers:
  - name: test-agent-comprehensive
    image: curlimages/curl:latest
    command: ['sh', '-c']
    args:
    - |
      set -e
      echo "Testing Agent comprehensive functionality..."

      # Test main agent service
      echo "Testing main agent service..."
      curl -f http://agent-ontology:8098/health || echo "Health endpoint not available, trying basic connectivity"
      curl -f http://agent-ontology:8098 || echo "Basic connectivity test"

      # Test environment variables
      echo "Checking environment configuration..."

      echo " Agent comprehensive tests completed"
---
# Source: rag-stack/charts/agent-ontology/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "agent-ontology-test-connection"
  labels:
    helm.sh/chart: agent-ontology-0.0.1
    app.kubernetes.io/name: agent-ontology
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['agent-ontology:8098']
  restartPolicy: Never
---
# Source: rag-stack/charts/agent-rag/templates/tests/test-agent-comprehensive.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "agent-rag-test-comprehensive"
  labels:
    helm.sh/chart: agent-rag-0.0.1
    app.kubernetes.io/name: agent-rag
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: test
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "2"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  restartPolicy: Never
  containers:
  - name: test-agent-comprehensive
    image: curlimages/curl:latest
    command: ['sh', '-c']
    args:
    - |
      set -e
      echo "Testing Agent comprehensive functionality..."

      # Test main agent service
      echo "Testing main agent service..."
      curl -f http://agent-rag:8099/health || echo "Health endpoint not available, trying basic connectivity"
      curl -f http://agent-rag:8099 || echo "Basic connectivity test"

      # Test environment variables
      echo "Checking environment configuration..."

      echo " Agent comprehensive tests completed"
---
# Source: rag-stack/charts/agent-rag/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "agent-rag-test-connection"
  labels:
    helm.sh/chart: agent-rag-0.0.1
    app.kubernetes.io/name: agent-rag
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['agent-rag:8099']
  restartPolicy: Never
---
# Source: rag-stack/charts/rag-redis/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "rag-redis-test-connection"
  labels:
    helm.sh/chart: rag-redis-0.0.1
    app.kubernetes.io/name: rag-redis
    app.kubernetes.io/instance: test-release
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['rag-redis:6379']
  restartPolicy: Never
