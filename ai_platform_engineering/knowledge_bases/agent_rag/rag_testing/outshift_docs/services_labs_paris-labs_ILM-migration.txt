ILM Migration
¶
Overview
¶
The aim of this documentation is to give all details and capture all requirements with regards to the upcoming migraiton of French office, known as ILM.
Office details
¶
The current office is based in Issy-Les-Moulineaux (ILM) and we do have 4 different rooms hosting the lab with 13 racks in total as per the
inventory
The next office will be
20 rue Washington in Paris
and we should only be able to have 2 racks on site.
Worth mentioning that for now, we do not have lot of visibility with regards to dates (out of old office, new one available/ready...) so it might impact projects while moving the lab, which can takes time and if new office is not ready while we're moving out the old one, the transition period include a no lab availability period.
PIRL racks / workload
¶
Here is the details of the PIRL racks / workload which includes ET&I, STO, Meraki and VPP team:
Team
Project
Current Lab Utilisation (in U)
Desired in next Office (in U)
Workload
POC
Specific
Meraki
-
3
0
Model training using GPU
David Gaumont
GPU infra with enough storage/RAM while is required for trainings on datasets
VPP
-
14
build /run env
Jerome Tollet / Aloys Augustin
Need regular physical access to NIC
STO
-
3
test env
Jean Bernard Beuque
Reseach
6lab.cisco.com
3
2 VMs
Eric Vyncke
2ESXi / static website in DMZ
CIL
Interlab
3
6
Meraki / compute
Justin Cohen
DMZ
CIL
Immersive lab
Thierry Gruzska
ETI
IRIS
22
40
dev / test env
Giovanna Carofiglio
test env with local access / network Wifi, could move to the share VMWare infra. 10 ESXi /  125 VMs
ETI
Great Bear
78
52
OpenStack (CICD) / VMWare
Fabien Andrieux
Next: 36U to 8U for OpenStack and 16U VMWare - GPU. OpenStack is currently a nice to have and could take less space on new hardware.
ETI
SRE
8
16
dev env used by ETI / VPP / CIL
Jeremie / Issa
VMWare which include Tanzu
ETI
deepfusion
2
Franck Bachet
GPU
ETI
SRE Lab Infra
12
6
Jeremie / Issa
Lab management
:--------
:--------------
:------------------
:---------
:--------------
:------------------
:---------
TOTAL
-
146
120
-
-
-
:--------
:--------------
:------------------
:---------
:--------------
:------------------
:---------
Next steps
¶
VMWare env
¶
Current workload usage: dev env in VMWare for VPP, CIL and ETI, model training on Tanzu.
Maintenance / compliance / upgrade: SRE team
As we're currently having lot of projects relying on compute / model training: Meraki, Great Bear, etc...., we can merge those on the same infra. But we will need to consider what is required on the edge as we can move to other compute env (P3, AWS for example), the ones that does not have any edge requirements.
Project changes
¶
Great Bear
¶
Current workload usage: CICD on OpenStack, model training on VMWare/GPU.
Maintenance / compliance / upgrade: Fabien is managing OpenStack.
As discussed with Fabien, the current OpenStack is kind of nice to have as we're used to required an OpenStack environment. But given the current context, this is currently mainly used as CICD platform. So we could move the current workload within our ETI Build Infra but no real dependency so far on OpenStack.
30
th
June 2022 meetings notes
¶
Discussion with Trev S. and Fabien, globally, the migration is not really a blocker as there is not much hard dependencies to the PIRL lab, this is more convienent to have something local.
There is currently build infra using Jenkins in order to launch Yocto build which required lot of CPU and time to build for several targets. This could be move to P3, AI to Jeremie to launch 36vCPU and 72vCPU instance for testing.
There is also mobel training running on GPU/VMWare (9U) in the PIRL lab and this can move to another cluster while the migration happens. After, would be good to discuss where to run the model training, either on AWS or could be cheaper on newer hardware with UCS M6.
IRIS
¶
Current workload usage: dev/test on VMWare
Maintenance / compliance / upgrade: Luca/Mauro
Intro
¶
As discussed with Giovanna, the current team is using their own VMWare env for dev/test purpose. Setup is 8 (icn-ucs) + 3 (encs) + 3 (nexus) + 2 (esxi) + 6 (pirl-ndn, shared 5 out of 8) + 1 (mac mini) = 22. Following the move, they did ask for 1 rack but given that the env will be smaller, they can reduce the footpring in order to only keep what is really necessary and by this, we mean the test part which required local devices to be able to reach the infra. Current test need to run locally for latency constraints. Mauro is currently investigating P3.
24
th
June 2022 meetings notes
¶
Dev env: requires 1 (or 2) VM per team member and is an linux env, better if within Cisco Network
Tests env: more complex as this is integration focused on performance testing, with all the datas and checking regressions  generating reports. Manage by Vagrant. More than 10 VMs / tests, each tests runs with differents parameters. Own test env including traffic generators, Robot framework, Selenium, Zoom media bridge/client...
performance tests are using local jenkins agents linked to EngIT jenkins instance
tests results/logs are stored in a NAS within the lab
With regards to Zoom media bridge, ilke WebEx, there is an EN license to be able to install it on prem
stress tests done within the office with bigger VMs
tests env are using VPP which then rely on VMWare pass through / bare metal to switch
automation done using Vagrant with ESXi plugin targeting VMWare env
for multiple architecture support, there is dedicated pipeline where this is using mac-mini or quemu
Next steps: find which env could move to P3, VMWare IT, VMWare SJC9 etc... and/or what we need and the impact of moving to another env.
29
th
June 2022 meetings notes
¶
Luca is currently investigation the GLS options in order to migrate what they have in the PIRL lab within a new DC: lof ot discussion with UK / Netherlands or IL.
Links provided:
https://cisco.sharepoint.com/sites/EngineeringLabMove/SitePages/Engineering-Lab-Space-Request.aspx
https://cisco.sharepoint.com/sites/EngineeringLabMove
https://wwwin.cisco.com/c/cec/organizations/Engineering/services-tools/labs.html#lab_locations
https://wwwin.cisco.com/c/cec/organizations/Engineering/services-tools/clvs.html
http://wwwin-labsupport.cisco.com/tools/resources.shtml
29
th
July 2022 meetings notes
¶
update on the move plan, what need to be in the lab, in DC or in IT/Cloud env
reviewing the current rack allocation for IRIS especially the NDN part which is used for performance test with hardware specific and currently in a
shared chassis
IRIS might required max 25U in the ULTeam lab
Next steps: Luca will ask a quote for GLS in order to see if some hardware can be shipped to a DC, will need the inventory access (Jeremie to send details)
Meraki
¶
Current workload usage: model training on VMWare/GPU
Maintenance / compliance / upgrade: David
Meraki team run regular trainings on big datasets, which required GPUs / RAM and storage. There is no blockers on moving to a shared GPU infra such as VMWare Tanzu.
29
th
June 2022 meetings notes
¶
Not having the lab for the dev team is currently a blocker as they need to train model on a regular basis, David will have a look at the AWS solution which is something already in place for dev team in Meraki.
VPP
¶
Current workload usage: dev/test on NIC in UCS
Maintenance / compliance / upgrade: Aloys
The current VPP team is using UCS to do their dev/test on NIC which requires regular physical access to the lab. After discussion with the team, some dev part could move out of the lab but not much as their tests part required access to the server.
Q&A
¶
moving dev env outside the lab (P3/AWS) need to be consider as action item 1 to reduce the footprint/costs
moving to newer hardware will be a win in term of space & performance
tbc
2022-07-29