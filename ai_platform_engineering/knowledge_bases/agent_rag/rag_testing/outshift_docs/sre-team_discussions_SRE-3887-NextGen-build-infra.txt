SRE-3887 -- Next generation of Build infra
¶
Objectives (as on Jira issue)
¶
Context
: As per our last discussion, we need to optimise our build infra, both from a cost point of view but also from a build POV, having more dynamic nodes would be really flexible.
At the moment, we're having a static infra using TF (sre-tf-infra repo) and Ansible playbook to setup nodes (sre-build-infra-ansible repo). The EC2 plugin investigation show that this is not a viable options. So given we're moving to GHEC also, which is another story and required also definition/usage of GitHub Actions, we need to define the next step with regards to the build infra which could be relying on Auto scaling groups or something else.
DoD
: come up with a proposal taking into account the current build infra, new requirements and env so that this will be reviewed as a team during a dedicate session.
Research
¶
As per the
previous investigation
, we saw that the
EC2 Jenkins plugin
approach was not viable nor good for our needs.
In this document, I will describe different options to see what the best one for having the next generation of the build infra which is more efficient and better in term of performance and costs.
This will include research on:
using AWS scaling groups
Jenkins on Kubernetes
Using AWS scaling groups
¶
In this approach, instead of using static build infra as define in our
sre tf infra repo
, we will use AWS scaling groups in order to scale up/down the Jenkins infra based on load.
With that in mind, we will need to manage the Jenkins node registration/deletion as a separate process which can not be done within the AWS VMs which are not connected to the Cisco Network and so can not reach our
dedicated Jenkins instance in Cisco network
.
We will then rely on the common mechanism implemented for
GitHub Action through web proxy
but then this will be applied for Jenkins node management.
What we will need then is:
Base Jenkins AMI from Cisco hardened AMI
defining policy for scaling our build infra based on instance monitoring and other events (manual ask for new instances)
Amazon EventBridge targets
so that once we scale down/up the infra, this will send event to the AWS gateway for registering/deleting jenkins nodes in our dedicated instance
The architecure will look like:
The only issue so far will be the scaling, which will need to be done from a Jenkins executor POV and not a load, which will require more efforts than just using Cloudwatch monitoring.
Jenkins on Kubernetes
¶
This approach will be based on
Kubernetes plugin for Jenkins
which will be a way to have full dynamic infrastructure using a k8s cluster.
As you can see in this documentation, it mentioned:
Agents are launched as inbound agents, so it is expected that the container connects automatically to the Jenkins controller
So this will be quite a limitation for us as the agent bootstrap in a GKE/EKS cluster will not be able to access our
dedicated Jenkins instance in Cisco network
. To solve this issue, we will setup a new
SCI connector
in order to connect our AWS eti-ci VPC to Cisco network.
Also, as per the research, there is a good article on
Scaling Jenkins on Kubernetes
.
There is also this article on
Docker in Docker with Jenkins Pod on Kubernetes
The architecture will be:
This is worth investigating this solution as this could be really a good fit with deeper integrations within k8s and fully dynamic build infra.
Our pipeline will looks like:
pipeline {
agent {
kubernetes {
yaml '''
apiVersion: v1
kind: Pod
spec:
containers:
- name: sre-pipeline-docker
image: containers.cisco.com/eti-sre/sre-pipeline-docker:2022.10.15-3e4c823-82
command:
- cat
tty: true
volumeMounts:
- mountPath: /var/run/docker.sock
name: docker-sock
- name: securecn
image: gcr.io/gcp-etigcp-nprd-12855/pipelines/chuck_norris:v24
command:
- cat
tty: true
volumeMounts:
- mountPath: /var/run/docker.sock
name: docker-sock
volumes:
- name: docker-sock
hostPath:
path: /var/run/docker.sock
'''
}
}
stages {
stage('Clone') {
steps {
container('sre-pipeline-docker') {
git branch: 'main', url: 'https://wwwin-github.cisco.com/eti/sre-pipeline-library.git'
}
}
}
..
Conclusion
¶
From the research:
I think the AWS scaling group approach could be easily done and require less efforts but not optimal with regards to scaling integration. But this could also be a scaling solution in AWS for venture use cases where this will not be possible to use other build env.
Jenkins on Kubernetes looks like more a right approach which will require more work but better in term of scale/optimisation.
At the end also considering there is not a one size fit all, here is a recap of different venture use cases and which solution might fit:
Venture use cases
details
target solution
solution details
Building simple app on wwwin-github
repo in wwwin-github using the SRE pipeline
S
no need to have full dynamic infra and currently everything within Cisco network
Build simple app from github.com
if you have a simple app repo in public github.com
GHA
no need for dedicated build infra, just used
custom build from github.com
if you have an app repo in public github.com which needs custom workflow
k8sJ
no need for dedicated build infra, just used
Build with specific pipeline tooling
for example, having build using kind
S
This should be using our standard P3 infra like this is currentlu done by AppNet
Running performance tests
running performance test which will required lot of nodes for a dedicated time
AWSSG / k8sJ
if this is pure containers without much requirement, Jenkins on k8s could fit but if more specific requirements on tools/node configuration, might be better to use AWS scaling groups
Considering:
S: Standard build infra in P3
GHA: GitHub.com Action
GHEA: GitHub Enterprise (wwwin-github) Action
AWSSG: Aws Scaling Groups
k8sJ: Jenkins on k8s
Next steps: we will need to implement a PoC on k8s running in P3 to make sure that this is relevant and what we need. If yes, then we will move this approach to a production phase we cluster in AWS etc-ci and new functions in sre-pipeline library etc...
2023-08-25