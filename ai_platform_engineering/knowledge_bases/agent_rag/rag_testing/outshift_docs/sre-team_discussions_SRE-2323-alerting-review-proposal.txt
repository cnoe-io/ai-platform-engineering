Internal Documentation
¶
Overview
Current State
Limitations
Review
PagerDuty service directory
Production Prometheus Alertmanager
critical
warning
New proposal
References
Overview
¶
This document describes the current alerting mechanism with a review and proposal to apply new changes based on what we were able to see during the past few months.
Current State
¶
The current alerts comes from our monitoring stack and it generates either notifications through:
Pagerduty
which is SMS/Email/notifications based on your configuration: see your notification rules on your profile
Webex teams as a notification in our room:
On-call (PagerDuty)
Pipeline PagerDuty Alerts
PagerDuty Low Priority Alerts
So basically, we've got currently 2 levels of alerts:
high priority: alerts which needs immediate response which generate a page to the ETI SRE primary before reaching the ETI SRE secondary and a Webex notification in the on-call room. See the
schedule
low priority: alerts which does not need immediate response and just generate a WebEx team notification in the Pipeline PagerDuty Alerts room.
Limitations
¶
What we've seen so far is that the current implementation could be improve by either taking care of working hours and be more specific on some low priority alerts. As for example, having a jenkins node down is a low priority alert but this could have a big impact as if the node went down during the execution of a job linked to a venture release, this breaks everything. But if it went down without any running job, we've got enough nodes and so this will not impact any venture.
Review
¶
Before going into a new proposal for some alerts, let's review the current configuration and try to come up with an improved approach.
PagerDuty service directory
¶
You will find list of services configured in PagerDuty
here
which contained:
CloudWatch Alerts
Production Prometheus Alertmanager
SRE Manged K8S Clusters
Production Prometheus Alertmanager
¶
Most of the configuration is currently configured in the
monitoring Ansible playbook
critical
¶
BlackboxSslCertificateExpired - Blackbox SSL certificate expired
50JenkinsInstanceDownDedicatedNodes - Jenkins dedicated instance has been down for more than 5 minutes
50JenkinsInstanceDownSRESlaveNodes - Jenkins SRE dedicated instance has been down for more than 5 minutes
JenkinsMaster - instance eti-sre-monitoring has been down for more than 5 minutes
HostOutOfDiskSpace - Disk is almost full (< 3% left)
JenkinsKeyRotation - SRE Token Rotation Failure
JenkinsMasterDiskusage - Jenkins master usage is more than 600GB
warning
¶
BlackboxSslCertificateWillExpireSoon - Blackbox SSL certificate will expire soon (SSL certificate expires in 20 days)
InstanceDown - Instance has been down for more than 5 minutes
RebootRequired - instance which requires a reboot
NodeFilesystemSpaceFillingUp - Filesystem is predicted to run out of space within the next 24 hours
NodeFilesystemSpaceFillingUp - Filesystem is predicted to run out of space within the next 4 hours
NodeFilesystemAlmostOutOfSpace - Filesystem has less than 5% space left
NodeFilesystemAlmostOutOfSpace - Filesystem has less than 3% space left
NodeFilesystemFilesFillingUp - Filesystem is predicted to run out of inodes within the next 24 hours
NodeFilesystemFilesFillingUp - Filesystem is predicted to run out of inodes within the next 4 hours
NodeFilesystemAlmostOutOfFiles - Filesystem has less than 5% inodes left
NodeFilesystemAlmostOutOfFiles - Filesystem has less than 3% inodes left
NodeNetworkReceiveErrs - Network interface is reporting many receive errors
NodeNetworkTransmitErrs - Network interface is reporting many transmit errors
NodeHighNumberConntrackEntriesUsed - Number of conntrack are getting close to the limit
NodeClockSkewDetected - Clock skew detected
NodeClockNotSynchronising - Clock not synchronising
HostHighCpuLoad - CPU load is > 90%
HostOutOfMemory - Node memory is filling up (< 10% left)
JenkinsTooManyJobsQueued - jenkins_queue_size_value > 10
JenkinsTooManyJobsStuckInQueue - jenkins_queue_stuck_value
JenkinsWaitingTooMuchOnJobStart - jenkins_job_waiting_duration > 10
JenkinsTooLowJobSuccessRate - jenkins_runs_success_total / jenkins_runs_total_total < 0.20
JenkinsHealthScoreToLow - jenkins_health_check_score < 1
JenkinsTooManyOfflineNodes - jenkins_node_offline_value > 2
New proposal
¶
Here is a new proposal for:
prometheus alert monitoring
alerts to be reviewed:
InstanceDown is currently not an easy one as there is no impact on having a jenkins node down as we've got a big build infra but if the node is down during a release process and was execution a job, this just killed the process.
HostOutOfMemory is raised prior instance down and this should be higher severity as the node is going to be down and as this is running a job, for sure there is an impact to venture. We should have a page and also a discussion with venture to see why this is consuming too much resources on the node
alerts to be deleted:
50JenkinsInstanceDownSRESlaveNodes which is currently configured for instance name sre-slave-* which is not align with the new instance naming
duplicating NodeFilesystem with same expression but on different metrics, could be merged
Refine escalation policy in order to support working hours and dynamic notification based on alert severity
one escalation policy for jenkins where we do have working hours included and more granularity with regards to severity
References
¶
Prometheus Integration Guide Documentation
PagerDuty Event/Alert Severity Levels
2023-08-25