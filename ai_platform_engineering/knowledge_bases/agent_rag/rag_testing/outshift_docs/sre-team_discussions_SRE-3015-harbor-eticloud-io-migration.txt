SRE-3015: Migration plan for harbor.eticloud.io to new K8s cluster
¶
Background
¶
harbor.eticloud.io is currently hosted on eks-production-1
eks-production-1 is being
replaced
with eks-prod-1 due to
CVE-2021-25741
harbor.eticloud.io also uses in-cluster postgres which is not backed-up
Move harbor data to Aurora RDS in AWS
Harbor Access during migration
¶
Database and EBS volume migration involves taking snapshots and restoring them in the target cluster, hence SRE team will take a short maintenance window to perform harbor migration.
Harbor will be in readonly mode during this upgrade
How is the data validated post migration
¶
SRE team created a detailed
validation tests
Tests will be run pre and post migration to validate the data integrity
Test Type
Details
Jenkins Link
Source Code
Basic Access test
* docker login with robot
* docker rm and pull image
Jenkins
*
Jenkinsfile Source
Detailed test
* Compare pre-save config.yaml data with harbor.eticloud.io
Jenkins
*
Jenkinsfile Source
*
Python Source
Migration data validation test
* Compare harbor.eticloud.io data with new harbor-new.eticloud.io
Jenkins
*
Jenkinsfile Source
*
Python Source
Robot users login test
* Docker login with robot account to harbor.eticloud.io
Jenkins
*
Jenkinsfile Source
*
Python Source
Detailed Migration Steps
¶
Step 1: Backup harbor.eticloud.io in-cluster database using pgAdmin
¶
Pre-requisites:
-
Install PgAdmin
- Access to Keeper
Setup
EKS cluster access to eks-production-1
aws eks update-kubeconfig --name eks-production-1 --region us-east-2 --alias eks-production-1
Setup harbor postgres database port-forwarding
kubectl port-forward service/harbor-harbor-database 5432:5432 -n harbor
Connect to database from PgAdmin
Step 2a: Snapshot registry ebs volume
¶
Registry
aws ec2 create-snapshot --volume-id $(kubectl get pv $(kubectl get pvc harbor-harbor-registry -n harbor --output="jsonpath={.spec.volumeName}") --output="jsonpath={.spec.awsElasticBlockStore.volumeID}" | awk -F/ '{print $NF}') --description "harbor.eticloud.io registry June 30th 2022"
Chartmusuem
aws ec2 create-snapshot --region us-east-2 --volume-id $(kubectl get pv $(kubectl get pvc harbor-harbor-chartmuseum -n harbor --output="jsonpath={.spec.volumeName}") --output="jsonpath={.spec.awsElasticBlockStore.volumeID}" | awk -F/ '{print $NF}') --description "harbor.eticloud.io chartmuseum June 30th 2022"
Jobservice
aws ec2 create-snapshot --region us-east-2 --volume-id $(kubectl get pv $(kubectl get pvc harbor-harbor-jobservice -n harbor --output="jsonpath={.spec.volumeName}") --output="jsonpath={.spec.awsElasticBlockStore.volumeID}" | awk -F/ '{print $NF}') --description "harbor.eticloud.io jobservice June 30th 2022"
Trivy
aws ec2 create-snapshot --region us-east-2 --volume-id $(kubectl get pv $(kubectl get pvc data-harbor-harbor-trivy-0 -n harbor --output="jsonpath={.spec.volumeName}") --output="jsonpath={.spec.awsElasticBlockStore.volumeID}" | awk -F/ '{print $NF}') --description "harbor.eticloud.io trivy June 30th 2022"
Note:
Remove --dry-run before running the above command
Step 2b: Copy Snapshot from us-east-2 to us-west-2
¶
Step 3: Restore harbor databases to new Aurora db using pgAdmin
¶
Setup RDS access
to rds-prod-1
Create RDS snapshot
Create new db
registry
notaryserver
notarysigner
Create 'postgres' role
CREATE
USER
postgres
Step 4: Create volume from registry volume snapshot
¶
aws ec2 create-volume --snapshot-id snap-062ab80c154f15ff5 --availability-zone us-east-2a --dry-run
Note:
Remove --dry-run before running the above command
Step 5: Install harbor in new cluster using ArgoCD
¶
Setup EKS Access
aws eks update-kubeconfig --name eks-prod-2 --region us-west-2 --alias eks-prod-2
Create harbor namespace
kubectl create ns harbor
Clone harbor-instances
git clone https://wwwin-github.cisco.com/eti/harbor-instances
cd harbor.eticloud.io
Edit pv.yaml
apiVersion
:
v1
kind
:
PersistentVolume
metadata
:
finalizers
:
-
kubernetes.io/pv-protection
labels
:
failure-domain.beta.kubernetes.io/region
:
us-east-2
failure-domain.beta.kubernetes.io/zone
:
us-east-2a
name
:
harbor-restore-pv
spec
:
accessModes
:
-
ReadWriteOnce
awsElasticBlockStore
:
fsType
:
ext4
volumeID
:
aws://us-east-2a/vol-094dfdf750de7e643
capacity
:
storage
:
1Ti
mountOptions
:
-
debug
nodeAffinity
:
required
:
nodeSelectorTerms
:
-
matchExpressions
:
-
key
:
failure-domain.beta.kubernetes.io/zone
operator
:
In
values
:
-
us-east-2a
-
key
:
failure-domain.beta.kubernetes.io/region
operator
:
In
values
:
-
us-east-2
persistentVolumeReclaimPolicy
:
Retain
storageClassName
:
eti-ebs-gp2
volumeMode
:
Filesystem
pvc.yaml
apiVersion
:
v1
kind
:
PersistentVolumeClaim
metadata
:
annotations
:
volume.beta.kubernetes.io/storage-provisioner
:
kubernetes.io/aws-ebs
finalizers
:
-
kubernetes.io/pvc-protection
labels
:
app
:
harbor
chart
:
harbor
component
:
registry
heritage
:
Helm
release
:
harbor
name
:
harbor-harbor-registry
namespace
:
harbor
spec
:
accessModes
:
-
ReadWriteOnce
resources
:
requests
:
storage
:
1Ti
storageClassName
:
eti-ebs-gp2
volumeMode
:
Filesystem
volumeName
:
harbor-restore-pv
Helm manual installation
Vault password location
helm upgrade --install harbor harbor/harbor --namespace harbor --version 1.6.0 \
--set persistence.persistentVolumeClaim.registry.existingClaim=harbor-harbor-registry \
--set persistence.persistentVolumeClaim.chartmuseum.existingClaim=harbor-eticoud-io-chartmuseum \
--set persistence.persistentVolumeClaim.jobservice.existingClaim=harbor-eticoud-io-jobservice \
--set persistence.persistentVolumeClaim.trivy.existingClaim=harbor-eticoud-io-trivy \
--set database.type=external \
--set database.external.host=<RDS URL> \
--set database.external.username=<RDS USER> --set database.external.password=<RDS PASSWORD> \
--set externalURL=https://harbor-new.eticloud.io \
--set expose.ingress.hosts.notary=notary.harbor-new.eticloud.io \
--set expose.ingress.hosts.core=harbor-new.eticloud.io \
--set notary.enabled=false \
--set metrics.enabled=true \
Edit Ingress
remove core.harbor* from
harbor-harbor-ingress
in harbor namespace
kubectl edit ingress harbor-harbor-ingress -n harbor
add cert manage annotation
annotations:
cert-manager.io/cluster-issuer: letsencrypt
Step 6:
¶
Verify
https://harbor-new.eticloud.io
Alternatively:
Update /etc/hosts to verify new harbor
Update /etc/hosts on mac to verify new harbor
Step 7: Execute harbor tests again new harbor
¶
Verify new harbor with
integration tests
Step 8: Update Route53
¶
Update 'harbor.eticloud.io' A Record Alias to new ELB
2023-08-29