2022-10-13; registry.eticloud.io outage
¶
Summary
¶
registry.eticloud.io was unavailable due to a Redis failure.
Table of Contents
¶
Summary
When
Timeline
Impact
System(s) impacted?
Teams impacted?
Analysis
Takeaways
When
¶
On 2022-10-13, the automated integration tests for registry.eticloud.io reported a number of failures.
Timeline
¶
Events leading up to the incident:
2022-10-05
- The redis-operator component (responsible for managing in-cluster Redis instances) was upgraded to 1.2.1 in all environments.
2022-10-13 3:10 PM CEST
- Jeremie and Mark from the SRE team started working on launching a new Harbor instance for Great Bear.
Incident timeline:
2022-10-13 4:50 PM CEST
- Mark identified a critical issue with the redis-operator that prevented launching new Redis clusters.
2022-10-13 5:00 PM CEST
- Mark managed to find a fix for the problem and decided to deploy the fix as an emergency update.
2022-10-13 5:08 PM CEST
- Mark announced the update in the
ET&I Platform Status
Webex space.
2022-10-13 5:10 PM CEST
- Mark rolled out the fix.
2022-10-13 5:14 PM CEST
- Jeremie and Mark determined that even though the fix resolved the original problem (not being able to launch new Redis clusters), there was another issue with Redis configuration.
2022-10-13 5:15 PM CEST
- Integration tests started to break.
2022-10-13 5:29 PM CEST
- Sri noticed the breaking tests and notified on-call.
2022-10-13 5:40 PM CEST
- On-call acknowledged the incident.
2022-10-13 5:43 PM CEST
- Mark confirmed that the second issue (invalid Redis configuration) is related to the incident and is the cause for registry.eticloud.io being unavailable.
2022-10-13 5:44 PM CEST
- Mark decided to roll back the redis-operator to an earlier version (1.1.1) in the affected environments.
2022-10-13 5:45 PM CEST
- Mark started rolling back the redis-operator in the affected environments.
2022-10-13 5:46 PM CEST
- Mark announced the roll back in the
ET&I Platform Status
Webex space.
2022-10-13 5:52 PM CEST
- Redis recovered under registry.eticloud.io
2022-10-13 5:55 PM CEST
- Sri confirmed that registry.eticloud.io is back up and running again.
2022-10-13 6:18 PM CEST
- Mark rolled back the redis-operator in the remaining environments.
2022-10-13 6:31 PM CEST
- Mark announced the end of the incident in the
ET&I Platform Status
Webex space.
Impact
¶
System(s) impacted?
¶
registry.eticloud.io
Teams impacted?
¶
Calisti
Analysis
¶
The original event (namely: upgrading the redis-operator to 1.2.1) that led to the incident happened roughly a week before the incident.
Prior to upgrading, the new version was tested in an existing dev environment. The following test procedure was followed:
Launch a Redis instance
Connect to Redis, authenticate, write then read
Upgrade the redis-operator
Repeat the test
Launch a new Redis instance
Repeat the test
The test didn't show any issues, so the upgrade was rolled out to every environment without incident.
It's important to note that no version was specified during the tests which is why the tests didn't catch any of the issues.
About a week after the upgrade an unrelated activity to the incident (installing a new Harbor instance) uncovered various issues with the upgrade:
Launching new instances was impossible under certain conditions
The operator generated configuration that did not work with Redis 5
Harbor Redis is currently pinned to version 5 because it's unable to specify a username for authentication at this time and therefore cannot authenticate with Redis 6.
The solution to the first problem was easy enough: the
ClusterRole
used by the redis-operator
ServiceAccount
needed additional permissions.
Normally, this should be a relatively low-risk change. In this case however rolling out the change caused the operator to regenerate configuration for existing Redis clusters.
The most probable explanation is that the issue that prevented launching new instances (which didn't occur during the tests) also affected configuration updates.
By fixing that issue, the operator was able to update the configuration for existing Redis 5 clusters.
This led to the Redis instance running under registry.eticloud.io failing which ultimately led to the service being unavailable.
Takeaways
¶
The testing process proved to be insufficient: we need to document a detailed test process verifying upgrades using every production configuration we run.
We also need better functional tests after upgrading.
On-call was notified by a team member who noticed the failing integration tests: we need paging when that happens. (
SRE-3913
)
We didn't realize, but the redis-operator most probably wasn't completely functional before the permission fix was deployed.
We need a way to detect that based on metrics/logs.
A seemingly low-risk fix that otherwise wouldn't, led to an outage: although this wasn't an easy situation as the fix for another "incident" led to an even worse one,
we need a process for rolling these fixes out. The process should be liberal enough to allow resolving pending incidents, but strict enough to prevent the resolution causing harm in other services.
There is an argument that we shouldn't use in-cluster Redis for production in the first place, but a managed service (eg. ElastiCache).
In this case, Redis is used as a cache, therefore the information stored within can be discarded anytime.
Cache access should be as fast as possible and there is nothing faster than a cache running right next to the service.
We will investigate whether the use of ElastiCache (or similar solution) is warranted in these scenarios,
however bad configuration is entirely possible in managed services as well, so we are not necessarily protected for these kind of incidents by using a managed service.
2022-10-14