SRE-2586: EKS Cluster Migration
¶
JIRA: SRE-2586
SRE-2586: EKS Cluster Migration
Context
Definition of Done
Execution Plan
SRE-2600: Current and Post-Migration EKS Clusters
Context
¶
ETI Platform currently manages EKS clusters to which multiple ETI teams, including Ventures and SRE, deploy their applications.
This automated report
tracks all known applications currently deployed to these EKS clusters.
Most of these EKS clusters are on Kubernetes version 1.18, which will be
deprecated in AWS on March 31, 2022
. Moreover there is a
security vulnerability
that impacts this version of Kubernetes that will be resolved by upgrading to
version 1.22.2
. These clusters currently do not leverage Amazon EKS's
managed node groups
, which automates lifecycle management of EKS clusters. As a result, upgrading each cluster
safely
requires operationally risky and burdensome process. Furthermore, these existing clusters were created with configuration generated by code adopted from another team, which has proven to be a challenge to maintain (e.g., dependency on
terraform-aws-modules/eks
, which is not backwards-compatible with the
version
our existing terraform configuration depends on).
To address the above issues, we chose to write our own Terraform configuration that is better tailored to our EKS cluster requirements (use latest version of
terraform-aws-modules/eks
and enable Managed Node Groups to ensure easier EKS cluster upgrade paths going forward).
As a result, all applications currently deployed to impacted EKS clusters must be migrated over to replacement EKS clusters that will be created by this new and improved Terraform configuration.
Definition of Done
¶
Terraform code that automates EKS Cluster lifecycle management and "Day 2 Ops" (vertical and horizonal scaling of nodes, Managed Node Groups to support EKS version and node AMI upgrades)
Day 2 Ops how-to docs published to eti-platform-docs
All workloads in existing EKS clusters on Kubernetes version 1.18 have been migrated to new replacement EKS clusters on latest supported Kubernetes version created with new automation described above
EKS cluster creation, provisioning, workload migration steps documented for future
All legacy 1.18 EKS clusters have been decommissioned and deleted
Execution Plan
¶
Write new Terraform configuration for EKS cluster creation that eliminates the tech debt in the exisitng terraform code
SRE-1799
- Refactor EKS creation to use tf modules
SRE-2154
- Support Day 2 Ops requirements
SRE-1801
- Verified EKS cluster creation 2.0
[#TODO (Sally)]
: Check status/validity of open issues in
SRE-2259
and
SRE-1805
- these (with the exception of SRE-1805, which is a duplicate) are now linked to
SRE-2586
, which tracks the overall migration effort
Identify all existing EKS clusters on soon-to-be-deprecated version 1.18 and their respective workloads
Update
ETI Shared K8s Clusters doc
to reflect current and desired post-migration states (e.g.,
eks-development-1
to be replaced by
eks-dev-1
)
TODO: verify if
eks-dev-2
(EU) can be upgraded via automation or replaced
TODO: document naming convention
eks-<environment>-<number[1-9]>
(used for both EKS and its respective VPC)
For the least critical EKS cluster identified above (
eks-sre-1
,
eks-sre-3
):
Create replacement EKS cluster with latest automation created in step (1)
Deploy all pre-requisite
baseapps
. Capture/improve documentation as needed.
Deploy One-Eye (ideally via ArgoCD)
Validation
deploy canary-hello-world app
Define migration plan for application workloads in this cluster
E.g.: Update target cluster for each application in
sre-cluster-config
for non-active instance (e.g. cluster
b
vs
a
)
or
create another
b
non-active instance in new cluster, validate application health, then cutover DNS
Important
: Most DNS records have been created manually, so the DNS cutover plan needs to be reviewed carefully and existing records cleaned up where possible
Note
: We can also investigate leveraging
External-DNS
to automate DNS record creation in the new clusters
Note
: Fledge must be migrated to ArgoCD from jenkins
Execute and validate the above for a non-critical application (e.g., sre-go-helloworld)
Document the above in a start-to-finish doc
Repeat step 3 for all remaining EKS clusters identified in step 2. Consume the start-to-finish doc created in step 3 (ideally by a fresh pair of eyes so we can improve the docs)
SRE-2600: Current and Post-Migration EKS Clusters
¶
â See
this spreadsheet
for the most up-to-date status (the table below is a snapshot of that spread sheet).
Pre-Migration
K8s Version
Post-Migration
K8s Version
Account
Status
Notes
eks-development-1
1.18
eks-dev-1
eticloud
Not Started
eks-dev-2
1.20
upgrade-in-place?
eticloud
Not Started
eks-production-1
1.18
eks-prod-1
1.21
eticloud
In Progress
Cluster created (to unblock Harbor) and base apps installed
eks-production-2
1.19
eks-prod-2
eticloud
Not Started
eks-observability-1
1.18
eks-obs-1
eticloud
Not Started
eks-observability-2
1.18
eks-obs-2
eticloud-scratch
Not Started
Should eks-obs-2 be in eticloud?
eks-prod-3
1.21
eks-prod-4
eticloud
Not Started
cluster used by MAQY
eks-sre-1
1.19
eks-sre-4
eticloud-scratch
Not Started
eks-sre-3
1.20
eks-sre-5
eticloud-scratch
Not Started
appnet-controller-1
1.18
migration needed?
eticloud-scratch
appnet-member-1
1.18
migration needed?
eticloud-scratch
appnet-member-2
1.18
migration needed?
eticloud-scratch
cil-appnet-1
1.18
migration needed?
eticloud-scratch
cil-appnet-2
1.18
migration needed?
eticloud-scratch
eks-dev-3
1.21
migration needed?
eticloud-scratch
eks-pras-1
1.21
migration needed?
eticloud-scratch
eks-szigeti-1
1.18
migration needed?
eticloud-scratch
eks-szigeti-2
1.18
migration needed?
eticloud-scratch
eks-szigeti-3
1.18
migration needed?
eticloud-scratch
eks-szigeti-5
1.19
migration needed?
eticloud-scratch
eks-szigeti-6
1.19
migration needed?
eticloud-scratch
eks-tftest-1
1.21
migration needed?
eticloud-scratch
erez-cluster-2
1.18
migration needed?
eticloud-scratch
2023-09-03