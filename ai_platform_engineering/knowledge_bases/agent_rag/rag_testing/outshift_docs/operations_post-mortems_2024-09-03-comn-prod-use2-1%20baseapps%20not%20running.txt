INC240903 2024-09-03-comn-prod-use2-1 baseapps not running
¶
When
¶
2024-09-03 late afternoon - 2024-09-04 morning
Summary
¶
The
comn-prod-use2-1
cluster lost connectivity to the baseapps and was not able to serve any requests. The team was notified and started investigating the issue. The issue was found to be related to the manual update of the default node-group launch template that was done by the team. The update was done to resolve the ST&O request to update our AMI images to the latest versions because of the security vulnerabilities that were found in the previous versions. The update was done manually and issues started to arise when the new nodes with the new launch template were being created. The team was able to resolve the issue after finding out that it was caused by the Kyverno base application that was blocking the new nodes from joining the cluster. The team was able to resolve the issue and the applications were up and running as expected.
Timeline
¶
2024-09-03
¶
16:04 IST: A team member from the SRE team was notified by PagerDuty that the
comn-prod-use2-1
cluster was not able to serve any requests and that the baseapps were not running properly. After checking the common Argocd instance, it was found that the applications were not able to sync and that the cluster was not able to connect.
17:00 IST: Team member notified the team and started investigating the issue. The team found that the issue was related to the manual update of the default node-group launch template that was done by the team.
19:00 IST: The team opened an incident via the Incident Command Bot to track all the issues and findings:
https://cisco-eti.atlassian.net/browse/INC-57
19:09 IST: After realizing that the issue was related to the Kyverno base application that was blocking the new nodes from joining the cluster, the team started working on resolving the issue.
19:30 IST: The team found that all the pods were still in pending state and that the new nodes were not able to join the cluster because of the Kyverno policies that were blocking them.
2024-09-04
¶
02:12 IST: The team found the root cause with the Kyverno policy blocking all daemonsets in the cluster including resources in the kube-system namespace (which was supposed to be excluded according to the policies in place). This was therefore blocking nodes from joining the node-group and the applications from running.
07:00 IST: The team was able to resolve the issue by removing all Kyverno resources and admission webhooks that were blocking the nodes, updating the launch template by running the platform Terraform and redeploying the applications after all the nodes in the node-group were up and running.
10:00 IST - 19:40 IST: The team replicated the fix/procedure on all clusters that needed to update their launch templates, which potentially could have run into the same issues:
comn-prod-usw2-1, comn-staging-use2-1, comn-staging-usw2-1, comn-dev-use2-1, comn-dev-usw2-1
.
Impact
¶
System(s) impacted?
¶
Production Metrics Push Gateway (Jenkins, Jenkins Agent Metrics, Panoptica Agent Metrics)
Jarvis AI Bot (Prod internal application POC)
Teams impacted?
¶
None
Analysis
¶
Lack of proper procedure to update launch template AMI images and lack of guidance from ST&O/Cloud 9, left us to fend for ourselves on mitigating the request. The manual update of the launch template caused the Kyverno policies to inadvertently start blocking the new nodes from joining the cluster. This was an unexpected behavior and was not caught during any tests we have done in the past. The team was able to resolve the issue by removing the Kyverno policies and admission webhooks that were blocking the nodes from joining the cluster. The team was able to update the launch template and redeploy the applications after all the nodes were up and running.
Takeaways
¶
Create a well defined procedure for updating the launch templates and AMI images for the node-groups in the clusters.
Have multiple members (at least 2) work on such critical updates to make sure that all the steps are followed and that the changes are tested before they are applied to the production clusters.
Notifying any effected teams currently using the cluster about the potential downtime and issues that might arise during the update process. This will help in making sure that the team is aware of any potential issues that might arise and that they can plan accordingly.
Investigate the Kyverno/Kyverno policies and admission webhooks that are in place and make sure that they are not blocking any resources from joining the cluster. This will help in making sure that the applications are able to run as expected even after a launch template update.
Note
UPDATE:
We have found that there is no way to "exclude" resources in such a manner via Kyverno. The issue is that the Kyverno controller was already not functional (since there are no running nodes for it to use), and the admission webhooks were running stand-alone. So even if you add exclusions in the controller config, the admission webhooks will still block the resources. The way that Kyverno is designed, the admission webhooks work in a manner such that each API call in the cluster goes through the webhook first (regardless of namespace). The only way to resolve this is to remove the Kyverno resources and admission webhooks that are blocking the resources from joining the cluster. This is a known issue with Kyverno and we are working on finding a better solution to this problem.
Follow up on any known bugs from Kyverno related to the issues that we have faced and make sure that we are aware of any potential issues that might arise in the future.
2024-09-10