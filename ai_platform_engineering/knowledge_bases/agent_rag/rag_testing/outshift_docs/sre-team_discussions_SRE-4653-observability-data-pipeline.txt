kafka
msk
observability
slis
sre-4653
Observability Data Pipeline Discussion
¶
2023-01-23
Sally, Sri, RQ
Context
¶
We need to aggregate SLI metrics from multiple deployment clusters/regions for HA applications
SLI metrics require long-term storage (at least 13 months)
OTEL Collector was chosen as the telemetry router/shipper for the following reasons:
vendor-agnostic; supports wide range of logs/metrics/trace formats via receivers/processors/exporters (
opentelemetry-collector-contrib
)
enables immediate adoption for established storage backends (e.g. InfluxDB) that should be compatible with eventual integration with FSO
open source project that is becoming an industry standard
We already ship and aggregate logs from source k8s clusters to a SaaS logging platform (elastic.co)
Requirements
¶
Logs and metrics must not be lost in the event of temporary disruptions in data transport and ingestion
Logs/metrics volume is highly variable; processes transporting data must be horizontally scalable
Recovery of observability data must not depend on availability of the monitored applications/systems
Flexible routing/filtering - we must be able to drop/fan-out/redirect logs and metrics to drop noisy logs/metrics and switch backend storages (e.g., eventual adoption of FSO)
Minimal operational overhead of managing these pipelines, which will scale with cluster deployments
Proposed Solution
¶
Deploy per-region (i.e., reachable over LAN) Kafka at each source site to buffer logs/metrics when remote writes via OTEL collectors fail (e.g., WAN network disruptions, remote storage performance issues)
OTEL collector agents receive metrics/logs from applications and exports them immediately to a local Kafka
Can use Prometheus receiver to scrape Prometheus metrics - no need to modify applications
OTEL collector "shippers" consumes logs/metrics from local Kafka via
Kafka receiver
and writes to remote SaaS endpoints (e.g., InfluxDB, Elastic)
[Optional] Kafka at self-managed storage site provides resilience against data ingestion delays/failures
Discussion Points
¶
Limitations of OTEL Collector persistent queues
¶
OTEL Collector supports persistent queues which was requested
in this issue
However, it does
not support horizontal scaling via multiple replicas
, which violates requirement (2).
Moreover, when a persistent queue is shut down, it is not drained, which can lead to lost data. The
recommendation
is to leverage an external queueing system like Kafka.
Separate OTEL collector "receivers" from "shippers"?
¶
In this context, "receivers" collect/scrape logs/metrics emitted by apps and writes them to a local Kafka buffer; "shippers" consume buffered data in local Kafka and exports it to appropriate sink storage endpoint.
Should the same collectors be responsible for both "receiver" and "shipper" roles, or should we deploy separate fleets for each role?
Currently, OTEL collectors are deployed as a daemonset on the source k8s cluster. 
If we use the same collectors for both roles:
Pros:
Simplified deployment
Cons:
limited scalability: outage scenario may require scaling just the shippers to "catch up" buffered data
blast radius: pipeline changes to receivers and shippers can impact each other
Conclusion: Yes, "receivers" and "shippers" should be handled by separate OTEL deployments because the cons outweigh the pros; deploying another instance of OTEL collector should be trivial.
Kafka deployment methodology
¶
The primary role for local Kafka is to provide reliable buffering to prevent data loss in the event of WAN netowrk disruption or storage ingestion problems; therefore, it must be reachable by the logs/metrics emitting source over LAN.
There are several options for deploying local Kafka buffers while meeting the above requirement:
Self-managed Kafka cluster via VMs provisioned in the same/peered AWS networks as the source k8s
Pros: blast radius isolation to/from issues impacting source k8s cluster
Cons: high operational overhead
Kafka on the source k8s cluster (e.g., SDM)
Cons:
operational risks with persistent volumes (broker nodes may be rescheduled across different AZ's, but EBS volumes persist in a single AZ)
same blast radius to/from source k8s cluster
increased compute cost to source k8s clusters
Cloud Provider managed Kafka (e.g., MSK) in the same/peered AWS networks as the source k8s
Pros:
blast radius isolation to/from issues impacting source k8s cluster
low operational overhead
Cons:
Solution is specific to Cloud Provider of source clusters
AWS offers
MSK
GCP offers
managed Kafka via Confluent
Discuss: Enable network peering and share the same source Kafka cluster with multiple k8s clusters in the same region?
Based on the above assessment, the proposed path forward is to leverage
Cloud Provider managed Kafka option
.
References
¶
opentelemetry-collector issue: Persistent Queue issues/questions
opentelemetry-collector issue: Drain persistent queue on shutdown
2023-08-25