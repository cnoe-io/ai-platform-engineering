EKS Cluster Upgrade Notes
¶
Overview
¶
This document captures the common issues and solutions when upgrading EKS clusters.
Pre-Upgrade Preparation
¶
Remember to upgrade incrementally
- You cannot jump directly from v1.29 to v1.31. You must upgrade through each minor version: 1.29â1.30â1.31.
Plan for these components:
EKS control plane
Managed nodegroups (if used)
Karpenter nodepools (if used)
EKS add-ons (vpc-cni, kube-proxy, aws-ebs-csi-driver)
Custom AMIs (if used)
Common Issues & Solutions
¶
1. Terraform aws-auth ConfigMap Error
¶
Error:
Error: configmaps "aws-auth" is forbidden: User "system:anonymous" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
with module.eks_all_in_one.kubernetes_config_map_v1_data.aws_auth_sre_data[0],
on .terraform/modules/eks_all_in_one/eks-aws-auth-cm.tf line 29, in resource "kubernetes_config_map_v1_data" "aws_auth_sre_data":
29: resource "kubernetes_config_map_v1_data" "aws_auth_sre_data" {
Solution:
# Remove the state resource and plan again
atlantis
state
rm
module.eks_all_in_one.kubernetes_config_map_v1_data.aws_auth_sre_data
[
0
]
2. Invalid AMI Error During Nodegroup Updates
¶
Error:
Error: updating EKS Node Group: operation error EKS: UpdateNodegroupVersion, https response error StatusCode: 400, InvalidRequestException: The image id '[ami-xxxxxxxxxx]' does not exist
Solution:
- Verify AMI availability for the target Kubernetes version
- Update your AMI ID in Terraform with a compatible version
- Use AWS-managed AMI types if custom AMIs aren't available yet
3. PodDisruptionBudget (PDB) Blocking Node Draining
¶
Issue:
Restrictive PDBs with
minAvailable=1
or
maxUnavailable=0
can prevent nodes from being drained during the upgrade.
Solution:
- Temporarily modify PDBs to be less restrictive:
  - For PDBs with
minAvailable
set: Change to
minAvailable=0
- For PDBs with
maxUnavailable=0
: Change to
maxUnavailable=1
Sample script:
#!/bin/bash
# Create backup of current PDBs
BACKUP_FILE
=
"pdb_backup_
$(
date
+%Y%m%d_%H%M%S
)
.yaml"
kubectl
get
pdb
-A
-o
yaml
>
"
${
BACKUP_FILE
}
"
# Identify restrictive PDBs
kubectl
get
pdb
-A
-o
json
|
jq
-r
'.items[] |
select(.spec.minAvailable != null and .spec.minAvailable >= 1) |
"\(.metadata.namespace)|\(.metadata.name)"'
|
while
IFS
=
'|'
read
-r
namespace
name
;
do
echo
"Setting minAvailable=0 for
${
namespace
}
/
${
name
}
..."
kubectl
patch
pdb
"
$name
"
-n
"
$namespace
"
--type
=
'json'
-p
=
'[{"op": "replace", "path": "/spec/minAvailable", "value": 0}]'
done
# After the upgrade, restore original PDBs
kubectl
apply
-f
"
${
BACKUP_FILE
}
"
4. Add-ons Stuck in UPDATING State
¶
Issue:
Add-ons like vpc-cni, kube-proxy, and aws-ebs-csi-driver may get stuck in the UPDATING state.
Solution:
- Update add-ons one at a time
- Try with
--resolve-conflicts PRESERVE
first
- If still stuck, use
--resolve-conflicts OVERWRITE
(caution: can disrupt workloads)
- As a last resort, delete and recreate the add-on
# Check add-on status
aws
eks
describe-addon
--cluster-name
YOUR-CLUSTER-NAME
--addon-name
vpc-cni
# Update with preserve
aws
eks
update-addon
--cluster-name
YOUR-CLUSTER-NAME
--addon-name
vpc-cni
--addon-version
LATEST_COMPATIBLE_VERSION
--resolve-conflicts
PRESERVE
# If needed, update with overwrite
aws
eks
update-addon
--cluster-name
YOUR-CLUSTER-NAME
--addon-name
vpc-cni
--addon-version
LATEST_COMPATIBLE_VERSION
--resolve-conflicts
OVERWRITE
5. Incompatible Applications After Upgrade
¶
Issue:
Some applications may experience errors after the upgrade (e.g.,
CreateContainerConfigError
or
CrashLoopBackOff
).
Affected applications in our environment:
- komodor-agent: CreateContainerConfigError
- falcofsk: Init:CrashLoopBackOff
Solution:
- Update these applications to versions compatible with the new Kubernetes version
- Check application logs for specific compatibility issues:
kubectl
logs
-n
NAMESPACE
POD_NAME
kubectl
describe
pod
-n
NAMESPACE
POD_NAME
Upgrade Procedure
¶
Update Terraform configuration
with the new EKS version (1.30 first, then 1.31)
Run Terraform apply
to upgrade the control plane
Update EKS add-ons
to compatible versions
Modify restrictive PDBs
to allow node draining
Update managed nodegroups
if using them
Update Karpenter nodepools and EC2NodeClass
for the new version:
Update EC2NodeClass AMI selector to include new version compatible AMIs
Update NodePool kubeletVersion to the new version
Monitor node replacement
and workload migration
Troubleshoot
any application-specific compatibility issues
Restore PDB settings
after the upgrade is complete
Verification Commands
¶
# Check cluster version
aws
eks
describe-cluster
--name
YOUR-CLUSTER-NAME
--query
cluster.version
# Check nodes version and status
kubectl
get
nodes
-o
custom-columns
=
NAME:.metadata.name,VERSION:.status.nodeInfo.kubeletVersion,STATUS:.status.conditions
[
?
(
@.type
==
\"
Ready
\"
)]
.status
# Check add-on status
aws
eks
list-addons
--cluster-name
YOUR-CLUSTER-NAME
# Check for problematic pods
kubectl
get
pods
-A
|
grep
-v
"Running\|Completed"
2025-04-01