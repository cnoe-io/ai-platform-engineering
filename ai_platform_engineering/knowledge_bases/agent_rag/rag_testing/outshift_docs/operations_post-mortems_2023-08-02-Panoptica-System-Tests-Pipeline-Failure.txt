panoptica
post mortem
system tests
2023-08-02; Panoptica System Test Pipeline Failure (Post Mortem)
¶
Summary
¶
Intent: We were modifying the Panoptica pipelines including (but not limited to) the
system tests periodic pipeline
Why: To introduce build stage metrics which would allow us to track the duration + pass/fail, of each stage in the pipeline. An ask from the Panoptica team.
What happened: After applying the reviewed
PR
, the pipeline failed to run. Upon further investigation we found that the pipeline was failing because the
srePipeline
library was pointing to the wrong branch. This was not caught in the PR review process because the pipeline was not run before/after the PR was merged. In addition there was an issue with the
system_test_group
variable was not being set correctly in the pipeline. Lastly there was a syntax discrepancy in the post stage of the pipeline which was causing the pipeline to not auto trigger in accordance to this new
change
, there was an extra
o
in line
639
of the
jenkins-pipeline-utils/jenkins-files/periodic-system-tests.Jenkins
file.
When
¶
2023-08-01 11:37 IST - 2023-08-03 12:13 IST
Timeline
¶
2023-08-01
¶
11:37 IST: PR was reviewed and merged
2023-08-02
¶
10:14 IST: Panoptica reached out to us in the
SRE for Panoptica(CNAPP)
webex space, saying that the system tests pipeline was failing
10:21 IST: Dan applied the fix for the branch misconfiguration of the
srePipeline
library in the pipeline on the master branch in order to unblock the system test runs
10:32 IST: Dan replayed to the Panoptica team in the
SRE for Panoptica(CNAPP)
webex space, saying that the pipeline was fixed and relayed the reason for urgency, which was that the system tests were not running
15:55 IST: Panoptica reached out again, stating that the system tests pipeline was still failing and that they have no visibility in their metrics dashboard
15:59 IST: We relayed to the team that we would investigate the issue internally with the SRE team and revert the code if we were unable to fix it in a timely manner
15:59 IST: Dan started investigating the issue, found that the error was coming from the application code itself
16:03 IST: In addition we noticed that the scheduler for the pipeline was off/not working, so we manually triggered the pipeline again while investigating the issue
16:07 IST: Conversation was switched over to the
Venture Panoptica (internal)
webex space to discuss internally with the SRE team
16:10 IST: We noted the PR that was created by our team for the pipeline changes, and that the pipeline was tested in a separate branch before merging to master, but in a different pipeline to check functionality of new stage metrics changes
16:45 IST: After investigating together with the team we found that the
system_test_group
variable was not being set correctly in the pipeline, and that it was passing a
null
value to the system tests scripts
16:53 IST: We noticed that the
system_test_group
variable was being set multiple times in the pipeline, first as a Jenkins parameter
system_tests_group
and then as an env var
system_test_group
, we thought it might have to do with the syntax of the variables, but investigation proved that that was not the issue
17:25 IST: After further investigation we found that the
system_test_group
variable was being set correctly in the pipeline, but that the
system_test_group
variable was not being passed correctly to the system tests scripts since the
def params = [:]
was colliding with the
parameters
block when calling
params.system_tests_group
in the system tests stage
17:41 IST: After running a few tests to confirm that this was the issue, we found that a fix can be made to rename the top
def params = [:]
to
def paramsForLibrary = [:]
in order to avoid the collision with the Jenkins parameters set in the pipeline
17:55-59 IST: We applied the fix to the pipeline via
commit
after testing and confirmation using Jekins
replay
functionality. We also notified the Panoptica team via
SRE for Panoptica(CNAPP)
webex space that the fix was applied and that the pipeline was running successfully
18:25 IST: While monitoring the pipeline we noticed it was failing on some application code related failures and unreachable endpoints, we notified the Panoptica team via
SRE for Panoptica(CNAPP)
webex space that the pipeline was failing for that reason, and assured that a rerun fixed the issue
19:15 IST: Panoptica relayed concern still that things were not stable and that we should revert all changes, we assured them that the fix was working and that we are monitoring the pipeline for any discrepancies, so there is no need for revert at this time
2023-08-03
¶
10:19 IST: After taking another look at the
periodic system tests
, Dan noticed that the pipeline was not being triggered automatically, and investigated to find that the following
PR
was applied a week ago, which removed the cron scheduler and replaced it with a
post
stage trigger, which was not working as expected
10:19 IST: Dan reached out to the Panoptica team in the
SRE for Panoptica(CNAPP)
webex space, saying that the pipeline was not being triggered automatically and that he was investigating the issue
10:30 IST: After noticinf a minor syntax error in the pipeline code, Dan tested a replay run of the pipeline and confirmed that the pipeline was working as expected
12:13 IST: Upon confirmation and applying the fix to the
master
branch, Dan notified the Panoptica team that the issue was resolved and that the pipeline was working/triggering as expected
Impact
¶
System(s) impacted?
¶
system tests periodic pipeline
, loss of visibility into the quality of code being pushed to master (potential Production code)
Teams impacted?
¶
Panoptica
Analysis
¶
The issue was caused by a misconfiguration of the pipeline, which was not caught in the PR phase of the pipeline. The issue was not caught in the PR phase because the pipeline that was used to test the changes was not the same pipeline that was used for system tests, and the pipeline that was used for system tests cannot be run in a separate branch without conflicting with the
master
branch. In addition multiple, major changed were made to the pipeline simultaneously during the week, from multiple teams. This caused the issue to be difficult to track down, and the issue was not caught until the system tests started running in
master
Solution:
Take better care when making changes to the pipeline, and make sure that the changes are tested in the same pipeline that is used for system tests
Sometimes better to have multiple reviewers on the PR to catch issues like this, but that are aware of the context for changes being made
System test pipeline should be reconfigured to decouple so many parties from making changes to it, and should be more stable
Redesign of the pipeline should be made to make it more stable and easier to test changes on it, without having to run on
master
Takeaways
¶
Take extreme care when reviewing PRs for venture related pipelines, especially if production is dependant on it
Communicate any major changes to such pipelines in the venture's webex space, so that they are aware of the changes and can be on the lookout for any issues
Collaboration is key, this should never be a one person job, especially when dealing with production related pipelines, and it should not be an "US vs. THEM" type of approach, but rather a "WE" approach
These type of situations usually call for a synced timeline on when changes should be made, and when they should be tested, and when they should be applied. Until we have a decoupled architecture for the pipelines, this is the best approach to take
2023-08-03