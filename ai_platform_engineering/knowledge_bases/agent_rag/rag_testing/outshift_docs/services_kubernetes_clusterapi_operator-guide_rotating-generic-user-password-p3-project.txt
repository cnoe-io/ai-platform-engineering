rotate P3 password
rotate openstack kubernetes password
Rotate the generic password for P3
How-to: Rotate the generic password for P3 Openstack Kubernetes Clusters
¶
Overview
¶
Outshift Platform ClusterAPI Kubernetes clusters in Cisco Internal Openstack (P3) use Cisco Generic User Credentials to manage the Kubernetes Clusters.
Generic user credentials are used in the following places
ClusterAPI Control cluster
Target Cluster Machine Nodes to talk to Openstack.
Outshift Platform also uses a dedicated generic user per Kubernetes cluster to minimize the disruption when generic user passwords change.
Example,
p3-dev-1
use a generic user credentials using
p3-dev-1.gen
user.
Pre-requisites
¶
Step 1:
Rotate the generic password
in the ADAM console and verify.
Step 2. Update/Rotate generic user passwords in Keeper
Generic user password stored in eticloud
here
Generic user password stored in eticloud/teamsecrets
here
Generic user password stored in eticloud/eticcprod used in cloud_credentials and cloud_credential_secret
here
Make sure to update all three fields of the secret:
OS_PASSWORD
,
cloud_credentials
, and
cloud_credential_secret
.
Note
For the
cloud_credentials
and
cloud_credential_secret
password you will need to decode it first from binary64 and then encode it back to binary64 after updating the password.
Step 4. Setup SSH to ClusterAPI kubernetest Nodes
The ssh key used by the nodes is in keeper as user
ops1
You may also follow the following doc to use the
sre-utilities
script:
SSH via Bastion
Step 5. Setup Kubeconfig for the respective cluster
Each cluster's kube config file that is used for kubectl access is stored in
Keeper
Use
k8s-shell-helper in sre-utilities
to simplify your cluster access
Process
¶
Get Update Targets/Instances
¶
P3 clusterapi Kubernetes clusters are Openstack clusters within a Cisco P3 datacenter. The virtual machines that make up the controlplane and worker nodes can be found in the ETI SRE project. Use the
RunOn MultiCloud Manager
to access the project.
Warning
When accessing the P3 horizon console from RunOn, you will not always be redirected to the correct project of choice. Make sure to select the right project via the
Project
dropdown in the top right corner of the Horizon console.
Once logged into RunOn, locate the
OpenStack
service name matching the cluster where credentials need to be updated.
Click the
OpenStack
hyperlink to manage the cluster infrastructure.
Note the control-plane, bastion, and worker node instances. These will be the targets in the following steps.
### SSH into Cluster API Nodes
Update the ssh configuration using the
SSH Config Documentation
Verify you are able to SSH into one of the update targets.
Access is through a bastion, so there may be to prompts for adding a known host entry.
Updating Credentials
¶
The following is a list of locations where we found the password needs to be changed. Change the
CAPI control cluster
(
p3-capi-control
project) first and then change the worker clusters (in the target cluster you are rotating the gen user for).
Note
If the CAPI control cluster
p3-capi-control
in the first step below is not available you will need to SSH into each control node (in your target cluster) and manually edit the
/etc/kubernetes/cloud.conf
file manually. Following the update, restart the
controller-manager
pod assigned to the node.
You will need to modify the embedded password in three of the resources for the CAPI control cluster
p3-capi-control
; the secret,
-cloud-config, KubeadmConfigTemplate and kubeadmcontrolplane.
In the secret, you will need to decrypt the
clouds.yaml
and update the password and add it back.
In both of the other resources, there is a
- content:
for the
/etc/kubernetes/cloud.conf
that contains the base64 encoded version of the cloud_credentials secret above.
Below is an example of the secret (password redacted) and the kubectl commands.
Note
Once you update the kubeadmcontrolplane, ClusterAPI will replace each control plane node (one at a time) in the target remote cluster. It will create a new instance, once it is up and running it will then drain an old instance and delete it. It does not do that for worker nodes, you have to do that manually for them.
This is an example for the
p3-dev-1
target worker cluster. You will need to repeat this process for each worker cluster you are updating.
[Global]
auth-url=https://cloud-rcdn-1.cisco.com:5000/v3
username="p3-dev-1.gen"
password="?????????"
tenant-id="b60d3144136e4c6aa08789bb9cf8b221"
tenant-name="p3-dev-1"
domain-name="cisco"
region="cloud-rcdn-1"
kubectl edit KubeadmConfigTemplate p3-dev-1-md-0
kubectl edit kubeadmcontrolplane p3-dev-1-control-plane
Once that above process is complete, switch over to your target cluster (for example
p3-dev-1
in this case) using the
k8s-shell-helper
script.
p3 p3-dev-1
Change the Openstack cinder secret and bump the version of the secret in
sre-cluster-configs
.  Before updating, you should verify the token for external secrets has not expired on the target cluster. Set your kubectl context to the cluster you're updating and run:
NEW_ORPHAN_TOKEN=$(VAULT_TOKEN=$VAULT_TOKEN VAULT_NAMESPACE=eticloud vault token create -policy=external-secrets -ttl=768h -orphan -format=json |jq -r '.auth.client_token') && kubectl delete secret vault-token -n external-secrets --ignore-not-found && kubectl create secret generic vault-token --from-literal=vault-token=$NEW_ORPHAN_TOKEN -n external-secrets
Verify the new configuration is in place for Openstack Cinder:
kubectl get secret cinder-csi-cloud-config -n openstack-cinder-csi -o yaml
Tip
Decode the base64 encoded
data.cloud-config
value and verify the new password is in place:
kubectl get secret cinder-csi-cloud-config -n openstack-cinder-csi -o json | jq -r '.data."cloud-config"' | base64 -d
Tip
Verify that the
password
in the decoded output matches the newly rotated password value.
You will need to modify the worker nodes (not the nodes labeled
control-plane
) as this step requires restart of the
openstack-cinder-csi
pods. Edit each node and modify the label
topology.cinder.csi.openstack.org/zone
value to match the value of
topology.kubernetes.io/zone
.
If you are not replacing the worker nodes, use the bastion to tunnel to each instance. For example, to log into host
10.7.0.60
via the bastion
10.226.177.241
:
ssh -i ~/.ssh/ops1 ubuntu@10.7.0.60 -o "proxycommand ssh -W %h:%p -i ~/.ssh/ops1 ubuntu@10.226.177.241"
Once you're logged in, you will need to modify the
cloud.conf
and update the password:
sudo vi /etc/kubernetes/cloud.conf
In the Openstack runon dashboard, do a soft reboot of all the modified instances. Wait for the reboot to finish and verify the node is back ready. In affect you will be doing a rolling restart of the worker nodes.
Warning
Remember to double check that you are in the right project in the P3 console, as mentioned in the beginning of this document.
kubectl get nodes
Next, once all the nodes are up and functioning, get the pods in the
openstack-cinder-csi
namespace, delete all the pods, and verify they restart.
kubectl get pods -n openstack-cinder-csi
NAME                                      READY   STATUS    RESTARTS   AGE
openstack-cinder-csi-controllerplugin-0   6/6     Running   28         54d
openstack-cinder-csi-nodeplugin-757mn     3/3     Running   3          53d
openstack-cinder-csi-nodeplugin-7prvb     3/3     Running   0          13d
openstack-cinder-csi-nodeplugin-7xwnp     3/3     Running   3          53d
openstack-cinder-csi-nodeplugin-fk2mv     3/3     Running   0          13d
openstack-cinder-csi-nodeplugin-jb9fk     3/3     Running   3          53d
openstack-cinder-csi-nodeplugin-tlwzm     3/3     Running   0          13d
kubectl -n openstack-cinder-csi delete pod --all
kubectl get pods -n openstack-cinder-csi
NAME                                      READY   STATUS    RESTARTS   AGE
openstack-cinder-csi-controllerplugin-0   6/6     Running   0          3m6s
openstack-cinder-csi-nodeplugin-6wbtz     3/3     Running   0          3m14s
openstack-cinder-csi-nodeplugin-8g9qn     3/3     Running   0          3m8s
openstack-cinder-csi-nodeplugin-b9cpz     3/3     Running   0          3m16s
openstack-cinder-csi-nodeplugin-bn4k5     3/3     Running   0          3m5s
openstack-cinder-csi-nodeplugin-pvrsd     3/3     Running   0          3m12s
openstack-cinder-csi-nodeplugin-xdzfb     3/3     Running   0          3m6s
Edit the worker nodes and reset the
topology.cinder.csi.openstack.org/zone
to
nova
.
DO NOT RESTART THE PODS
.
Verify all the pods are in a running state with
kubectl get pods -n openstack-cinder-csi
2024-05-29