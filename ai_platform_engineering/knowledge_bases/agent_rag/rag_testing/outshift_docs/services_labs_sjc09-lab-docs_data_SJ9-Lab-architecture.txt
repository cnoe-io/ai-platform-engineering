SRE-Lab in Building 9 First Floor Room 187
¶
This page is the portal of information regarding the SRE Lab in Building 9.
Space and IP Allocations
¶
SJC09 lab is a shared lab space with a total of 14 racks, 9 racks for the SRE-team and 5 racks for the SE(System Engineering) team. SE manage their gear by them selves. However, we share the same infrastructure services such as LDAP, DHCP NameServer and so on.
SRE team IP block
SE team IP block
Shared Block(Infra)
10.200.97.0/24
10.200.98.0/24
10.200.96.0/24
10.200.100.0/24
10.200.99.0/24
10.200.101.0/24
10.200.102.0/24
10.200.103.0/24
IP Mappings
SJ9-vCenter
¶
As we will be using Terraform and other automation tools to manage SRE lab resources on premises we implemented vCenter server in the lab.\
eti-sj9-vcenter
is an operational vCenter server that we use to provision the most recent compute resources in the lab.\
More hosts will be added into this server as we reclaim unused hosts in the lab. This allows us to manage all the compute resources in a single centralized domain.
Our current tooling, such as LXC and KVM, tends to be tedious when new virtual resources/instances are requested by our customers and teams we support. VMware's vSphere product provides greater ease of use and the best flexibility when provisioning resources as it may be driven by automation tools such as Terraform.
We store the lab secrets in vault
eticloud/labs/sjc
namespace.
Some of the lab resources can be only accessed with the lab
LDAP
credentials.
Lab Structure
The layout of this lab is two rows of network and compute equipment.  It is a shared lab space with the Web Operations Architecture and Systems engineering teams.
The ET&I SRE portion of the lab covers nine racks that are broken down into compute services in racks A4 - A6, B1 and B6 and network infrastructure & services in racks B2 through B5
Front of raw-a, compute resource services.
There are three separate data planes in the lab providing connectivity.
Data plane 1
provides connectivity Cisco Corporate IT network.
Data plane 2
provides connectivity to the CIMC interfaces on the UCS devices and is reachable via the corporate network.
Data plane 3
is a high speed data plane based around a non-oversubscribed 100GE CLOS fabric.
The top of rack switches are configured in to two VRFs with an equal amount of servers on each VRF. Each VRF has 4 x 100GE ECMP links in to the leaf nodes in the CLOS fabric.
Servers are connected to the high speed data plane at 40GE. This architecture provides a 1.2:1 over-subscription level within each VRF, and a non over-subscription level leaving the top of rack switch in to the fabric.
The design and cabling concept provides a one time cabling of the topology providing the ability to change the topology by shutting down ports and/or changing the routing policies.
Front of row-b, network and infra services.
Lab Layout
Lab Hot Aisle
The primary fabric uses eBGP for the routing protocol and is configured in the same methods as many of the web/hyperscale networks where there is no underlying IGP (OSFP/ISIS) for topology discover as BGP operates as the IGP and peers layer 3 port to layer 3 port.
Each device in the high speed data plane is it's own Autonomous System within the BGP topology.
Both IPv4 and IPv6 are configured from top of rack to top of rack.
ETI-SRE SJC9 Lab Topology
Structured Cabling and Power
There is an additional network connected off of the high speed data plane utilizing ISIS as the routing protocol with MPLS tunnels. (shown below)
Overall connected topology of the high speed data plane for eBGP and ISIS/MPLS domains.
The lab provides infrastructure services to both network and compute.
- Redundant DNS with local lab domain
-
LDAP services
-
inventory management
-
Host monitoring with Ganglia
-
IP Address Management
-
Nagios for host and service monitoring and alerting
-
Observium SNMP based observability of network devices
-
IXChariot Host based traffic generation for flow and application simulation
-
Grafana dashboarding for streaming telemetry
-
Lab service landing page
infrastructure services
Infrastructure services, depending on scale and performance requirements are running on baremetal, in Linux Containers (LXC), Libvirt KVM or docker containers.
Container to host mapping
High Speed Data Plane and Lab Gateway device listing
You can see
racktables
inventory management for more details about inventory and the asset rack location mappings.
2023-08-29