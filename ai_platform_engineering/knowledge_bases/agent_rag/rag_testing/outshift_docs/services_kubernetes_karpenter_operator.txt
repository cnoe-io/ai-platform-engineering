karpenter
For SRE
Terraform
¶
data
"aws_caller_identity"
"current"
{}
data
"aws_region"
"current"
{}
data
"aws_eks_cluster"
"cluster"
{
name
=
var.eks_name
}
locals
{
account_id
=
data.aws_caller_identity.current.account_id
region_id
=
data.aws_region.current.name
oidc_id
=
trimprefix
(
data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer
,
"https://"
)
}
resource
"aws_iam_policy"
"aws_karpenter_controller_policy"
{
name
=
"KarpenterControllerPolicy-${var.eks_name}"
description
=
"${var.eks_name} AWS Karpenter Controller Role IAM Policy"
policy
=
jsonencode
({
"Version"
:
"2012-10-17"
,
"Statement"
:
[
{
"Action"
:
[
"ssm:GetParameter"
,
"ec2:DescribeImages"
,
"ec2:RunInstances"
,
"ec2:DescribeSubnets"
,
"ec2:DescribeSecurityGroups"
,
"ec2:DescribeLaunchTemplates"
,
"ec2:DescribeInstances"
,
"ec2:DescribeInstanceTypes"
,
"ec2:DescribeInstanceTypeOfferings"
,
"ec2:DescribeAvailabilityZones"
,
"ec2:DeleteLaunchTemplate"
,
"ec2:CreateTags"
,
"ec2:CreateLaunchTemplate"
,
"ec2:CreateFleet"
,
"ec2:DescribeSpotPriceHistory"
,
"pricing:GetProducts"
],
"Effect"
:
"Allow"
,
"Resource"
:
"*"
,
"Sid"
:
"Karpenter"
},
{
"Action"
:
"ec2:TerminateInstances"
,
"Condition"
:
{
"StringLike"
:
{
"ec2:ResourceTag/karpenter.sh/nodepool"
:
"*"
}
},
"Effect"
:
"Allow"
,
"Resource"
:
"*"
,
"Sid"
:
"ConditionalEC2Termination"
},
{
"Effect"
:
"Allow"
,
"Action"
:
"iam:PassRole"
,
"Resource"
:
"arn:aws:iam::${local.account_id}:role/KarpenterNodeRole-${var.eks_name}"
,
"Sid"
:
"PassNodeIAMRole"
},
{
"Effect"
:
"Allow"
,
"Action"
:
"eks:DescribeCluster"
,
"Resource"
:
"arn:aws:eks:${var.aws_default_region}:${local.account_id}:cluster/${var.eks_name}"
,
"Sid"
:
"EKSClusterEndpointLookup"
},
{
"Sid"
:
"AllowScopedInstanceProfileCreationActions"
,
"Effect"
:
"Allow"
,
"Resource"
:
"*"
,
"Action"
:
[
"iam:CreateInstanceProfile"
],
"Condition"
:
{
"StringEquals"
:
{
"aws:RequestTag/kubernetes.io/cluster/${var.eks_name}"
:
"owned"
,
"aws:RequestTag/topology.kubernetes.io/region"
:
"${var.aws_default_region}"
},
"StringLike"
:
{
"aws:RequestTag/karpenter.k8s.aws/ec2nodeclass"
:
"*"
}
}
},
{
"Sid"
:
"AllowScopedInstanceProfileTagActions"
,
"Effect"
:
"Allow"
,
"Resource"
:
"*"
,
"Action"
:
[
"iam:TagInstanceProfile"
],
"Condition"
:
{
"StringEquals"
:
{
"aws:ResourceTag/kubernetes.io/cluster/${var.eks_name}"
:
"owned"
,
"aws:ResourceTag/topology.kubernetes.io/region"
:
"${var.aws_default_region}"
,
"aws:RequestTag/kubernetes.io/cluster/${var.eks_name}"
:
"owned"
,
"aws:RequestTag/topology.kubernetes.io/region"
:
"${var.aws_default_region}"
},
"StringLike"
:
{
"aws:ResourceTag/karpenter.k8s.aws/ec2nodeclass"
:
"*"
,
"aws:RequestTag/karpenter.k8s.aws/ec2nodeclass"
:
"*"
}
}
},
{
"Sid"
:
"AllowScopedInstanceProfileActions"
,
"Effect"
:
"Allow"
,
"Resource"
:
"*"
,
"Action"
:
[
"iam:AddRoleToInstanceProfile"
,
"iam:RemoveRoleFromInstanceProfile"
,
"iam:DeleteInstanceProfile"
],
"Condition"
:
{
"StringEquals"
:
{
"aws:ResourceTag/kubernetes.io/cluster/${var.eks_name}"
:
"owned"
,
"aws:ResourceTag/topology.kubernetes.io/region"
:
"${var.aws_default_region}"
},
"StringLike"
:
{
"aws:ResourceTag/karpenter.k8s.aws/ec2nodeclass"
:
"*"
}
}
},
{
"Sid"
:
"AllowInstanceProfileReadActions"
,
"Effect"
:
"Allow"
,
"Resource"
:
"*"
,
"Action"
:
"iam:GetInstanceProfile"
}
]
})
}
resource
"aws_iam_role"
"aws_karpenter_controller_role"
{
name
=
"KarpenterControllerRole-${var.eks_name}"
assume_role_policy
=
jsonencode
({
"Version"
:
"2012-10-17"
"Statement"
:
[
{
"Effect"
:
"Allow"
,
"Principal"
:
{
"Federated"
:
"arn:aws:iam::${local.account_id}:oidc-provider/${local.oidc_id}"
},
"Action"
:
"sts:AssumeRoleWithWebIdentity"
,
"Condition"
:
{
"StringEquals"
:
{
"${local.oidc_id}:aud"
:
"sts.amazonaws.com"
}
}
}
]
})
force_detach_policies
=
true
}
resource
"aws_iam_role"
"aws_karpenter_node_role"
{
name
=
"KarpenterNodeRole-${var.eks_name}"
assume_role_policy
=
jsonencode
({
"Version"
:
"2012-10-17"
,
"Statement"
:
[
{
"Effect"
:
"Allow"
,
"Principal"
:
{
"Service"
:
"ec2.amazonaws.com"
},
"Action"
:
"sts:AssumeRole"
}
]
})
force_detach_policies
=
true
}
resource
"aws_iam_role_policy_attachment"
"aws_karpenter_controller_attachment"
{
role
=
aws_iam_role.aws_karpenter_controller_role.name
policy_arn
=
aws_iam_policy.aws_karpenter_controller_policy.arn
}
resource
"aws_iam_role_policy_attachment"
"aws_karpenter_node_attachment_1"
{
role
=
aws_iam_role.aws_karpenter_node_role.name
policy_arn
=
"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}
resource
"aws_iam_role_policy_attachment"
"aws_karpenter_node_attachment_2"
{
role
=
aws_iam_role.aws_karpenter_node_role.name
policy_arn
=
"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}
resource
"aws_iam_role_policy_attachment"
"aws_karpenter_node_attachment_3"
{
role
=
aws_iam_role.aws_karpenter_node_role.name
policy_arn
=
"arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}
resource
"aws_iam_role_policy_attachment"
"aws_karpenter_node_attachment_4"
{
role
=
aws_iam_role.aws_karpenter_node_role.name
policy_arn
=
"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}
resource
"aws_iam_role_policy_attachment"
"aws_karpenter_node_attachment_5"
{
role
=
aws_iam_role.aws_karpenter_node_role.name
policy_arn
=
"arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
}
On EKS additional groups configuration add this gtoup:
aws_auth_additional_roles
=
[
{
# Role for karpenter!
rolearn
=
"arn:aws:iam::${local.account_id}:role/KarpenterNodeRole-${var.eks_name}"
,
username
=
"system:node:{{EC2PrivateDNSName}}"
,
groups
=
[
"system:bootstrappers", "system:nodes"
]
}
]
If terraform didn't do it, add it manually:
kubectl edit configmap aws-auth -n kube-system
and add
-
"groups"
:
-
"system:bootstrappers"
-
"system:nodes"
"rolearn"
:
"arn:aws:iam::<aws_account_id>:role/KarpenterNodeRole-<cluster_name>"
"username"
:
"system:node:{{EC2PrivateDNSName}}"
Baseapp
¶
We have option now to create default nodeclass and nodepool out of the box, developers no need to worry about it.
Take a look carefuly for subnetSelectorTerms and securityGroupSelectorTerms tags, at this example I used default tags that we are getting from VPC and EKS module, but you can add your custom tags and use them.
Manifest:
¶
The below template has been added to the baseapps
generic examples
karpenter
:
replicas
:
1
serviceAccount
:
name
:
karpenter
annotations
:
eks.amazonaws.com/role-arn
:
'arn:aws:iam::<aws-account-id>:role/KarpenterControllerRole-<cluster-name>'
controller
:
resources
:
requests
:
cpu
:
1
memory
:
1Gi
limits
:
cpu
:
1
memory
:
1Gi
settings
:
clusterName
:
'<cluster-name>'
generic_nodepools
:
enabled
:
"true"
nodeClasses
:
-
name
:
"default"
amiFamily
:
"AL2"
amiSelectorTerms
:
-
name
:
CiscoHardened-EKS1.28AmazonLinux2-amd64-hvm-*
owner
:
"849570812361"
roleName
:
"KarpenterNodeRole-<cluster-name>"
subnetSelectorTerms
:
-
tags
:
ApplicationName
:
"<cluster-name>"
# replace with your cluster name
kubernetes.io/role/internal-elb
:
"1"
securityGroupSelectorTerms
:
-
tags
:
ApplicationName
:
"<cluster-name>-eks"
-
tags
:
ApplicationName
:
"<cluster-name>"
Name
:
"<cluster-name>-vpc-default"
nodePools
:
-
name
:
"default"
labels
:
applications
:
default
requirements
:
-
key
:
kubernetes.io/arch
operator
:
In
values
:
[
"amd64"
]
-
key
:
kubernetes.io/os
operator
:
In
values
:
[
"linux"
]
-
key
:
karpenter.sh/capacity-type
operator
:
In
values
:
[
"on-demand"
]
-
key
:
karpenter.k8s.aws/instance-category
operator
:
In
values
:
[
"c"
,
"m"
,
"r"
]
-
key
:
karpenter.k8s.aws/instance-generation
operator
:
Gt
values
:
[
"2"
]
nodeClassRefName
:
"default"
limits
:
cpu
:
"10000"
memory
:
1000Gi
disruption
:
consolidationPolicy
:
WhenUnderutilized
expireAfter
:
720h
taints
:
-
key
:
applications/default
effect
:
NoSchedule
You can use the AWS CLI to double check you targetted the correct resources (default and EKS cluster subnets/security groups):
# duo SSO auth, then
export
AWS_PROFILE
=
<aws-account-id>
export
AWS_REGION
=
<aws-region>
aws
ec2
describe-security-groups
--filters
"Name=tag:ApplicationName,Values=<cluster-name>"
aws
ec2
describe-security-groups
--filters
"Name=tag:Name,Values=<cluster-name>"
# wildcards are supported by the CLI
aws
ec2
describe-subnets
--filters
"Name=tag:ApplicationName,Values=*<cluster-name>*"
Troubleshooting
¶
SPOT instances issue
AuthFailure.ServiceLinkedRoleCreationNotPermitted: The provided credentials do not have permission to create the service-linked role for EC2 Spot Instances
Run via CLI:
aws iam create-service-linked-role --aws-service-name spot.amazonaws.com
(link)[
https://karpenter.sh/docs/troubleshooting/#missing-service-linked-role
]
2024-04-25