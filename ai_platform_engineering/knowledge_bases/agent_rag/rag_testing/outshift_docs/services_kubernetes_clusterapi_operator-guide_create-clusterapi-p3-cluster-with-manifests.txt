Creating a p3 cluster with ClusterAPI
¶
Prerequisites
¶
clusterctl
¶
clusterctl is used to do the creation of the cluster.  We are currently using the eti-cc as the management cluster for it.  You can download it using brew or asdf.
brew install
brew install clusterctl
asdf install
asdf plugin install clusterctl
asdf install clusterctl 0.4.5
asdf shell clusterctl 0.4.5
Glance
¶
Glance is used to upload base images.
pip3 install python-glanceclient
To upload an image,
glance image-list
glance image-create --name ubuntu-2004-kube-v1.20.10 --disk-format qcow2 --container-format bare --file ./ubuntu-2004-kube-v1.20.10 --progress
Openstack client
¶
Openstack ctl used to verify connections
pip3 install python-openstackclient
CAPI resource templating
¶
We use CAPI-provided templates and the clusterctl command as templating engine to render based on environment variables requires that we set the variables.  The easiest way is to download the env.rc and run that.  Download the env.rc
wget https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-openstack/master/templates/env.rc -O ./env.rc
Create Openstack Project
¶
Current implementation assumes a one-to-one relationship betwen a cluster and the runon openstack project.
Step 1: Create a new generic user
¶
Step 2: Create a new runon openstack project
¶
Step 3: Creating the cluster YAML file
¶
You will need to set your kubectl context to the eti-cc cluster.  That is the management cluster for the clusterAPI.  The kubeconfig can be downloaded from
Vault eti-cc kubeconfig
.
Once the context is set, you will need to modify the clouds.yaml to include the generic user/password.  They can be found in
Vault P3 credentials
Below is an example of a clouds.yaml.  You will replace the username with the generic from above and add a line after with
pasword: <blah>
with the password from the generic above.
# This is a clouds.yaml file, which can be used by OpenStack tools as a source
# of configuration on how to connect to a cloud. If this is your only cloud,
# just put this file in ~/.config/openstack/clouds.yaml and tools like
# python-openstackclient will just work with no further config. (You will need
# to add your password to the auth section)
# If you have more than one cloud account, add the cloud entry to the clouds
# section of your existing file and you can refer to them by name with
# OS_CLOUD=openstack or --os-cloud=openstack
clouds:
openstack:
auth:
auth_url: https://cloud-alln-1.cisco.com:5000/v3
username: "damarcil"
project_id: c0262d74437740bea117b2488b24855b
project_name: "eti-gitops-direct-connect"
user_domain_name: "cisco"
region_name: "cloud-alln-1"
interface: "public"
identity_api_version: 3
You can now run the env.rc to set the environment variables.
source ./env.rc clouds.yaml openstack
You will also need to set some other variables.  You need to downlaod the PEM file from the openstack dashboard.
https://cloud-alln-1.cisco.com/dashboard/project/
export OPENSTACK_CLOUD_CACERT_B64=''
export OPENSTACK_DNS_NAMESERVERS="10.240.240.1"
export OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR="8vCPUx16GB"
export OPENSTACK_NODE_MACHINE_FLAVOR="8vCPUx16GB"
export OPENSTACK_IMAGE_NAME="ubuntu-2004-kube-v1.20.10"
export OPENSTACK_SSH_KEY_NAME="ops1"
export OPENSTACK_FAILURE_DOMAIN=cloud-alln-1-a
ok, now you are ready to create the yaml for the cluster and then the cluster itself.
clusterctl generate cluster eti-gitops-1 --kubernetes-version v1.21.3 --control-plane-machine-count=1 --worker-machine-count=1 > eti-gitops-1.yaml
kubectx eti-cc-admin@eti-cc
kubectl apply -f eti-gitops-1.yaml
You can tail the logs for errors or progress for the cluster creation.  This will create the network, router and control-plane instance for the cluster.  It will take about 10 minutes.
kubectl -n capo-system logs -l control-plane=capo-controller-manager -c manager -f
The worker nodes are not created until you install a CNI for the cluster.  Now we need to get the kube config for the new cluster.
clusterctl describe cluster eti-gitops-1
clusterctl get kubeconfig eti-gitops-1 > eti-gitops-1.kubeconfig
Add that to your ~/.kube and
change the context
to the new cluster.
We will use calico for the new cluster's CNI.
curl https://docs.projectcalico.org/manifests/calico.yaml -O
kubectl apply -f calico.yaml
This will get you your worker node for the cluster.
At this point, you are ready to add the cluster to ArgoCD and give it clusterops such as cert-manager, external-secrets, imagepullsecrets and openstack-cinder-csi (for persistent volumes).
Gotchas
¶
Persistent Volumes
¶
You will need to edit the worker nodes in the cluster to modify the label needed for persistent volumes.
kubectl get nodes
kubectl label --overwrite <NODE> topology.cinder.csi.openstack.org/zone=nova
Find the label
topology.cinder.csi.openstack.org/zone
and change its value to be
nova
Bastion
¶
If you require a bastion host (and you do) you will need to modify the cluster yaml files before you apply it to the management cluster.  For example, in the eti-gitops-1.yaml above, edit it and add to the
OpenStackCluster
the bastion section.
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha4
kind: OpenStackCluster
metadata:
name: eti-gitops-1
namespace: default
spec:
cloudName: openstack
dnsNameservers:
- 10.240.240.1
- 10.240.241.1
identityRef:
kind: Secret
name: eti-gitops-1-cloud-config
managedAPIServerLoadBalancer: true
managedSecurityGroups: true
nodeCidr: 10.6.0.0/24
bastion:
enabled: true
instance:
flavor: 2vCPUx4GB
image: UBUNTU-20_04-SM-2021-08-V1
sshKeyName: ops1
2023-08-29