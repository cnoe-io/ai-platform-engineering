EKS
self-service
troubleshooting
Runbook for Troubleshooting Self-Service EKS Issues
¶
ClusterAPI for EKS self-service
¶
ClusterAPI EKS provider is running in
p3-prod-1
in the
capa-system
namespace.  All clusters for EKS are in the
capi-eks-clusters
namespace.  The ArgoCD for EKS clusters can be found
here
Cluster stuck in delete state
¶
Users delete clusters by removing them from the EKS folder and triggering ClusterAPI to perform a delete on the cluster.  Occassionaly, the delete hangs and is stuck in a delete state.  If this occurs, you should first determine where the cluster deleting is in the delete phase.
DO NOT
do anything if the delete is only a few minutes old.  Log into the AWS account and first check the EKS dashboard to see if the cluster is there or is still deleting.  If still deleting, wait.  If not there, check the VPN for the cluster in the VPN dashboard.  If VPN is gone, that means the cluster has been deleted but ClusterAPI is not sync'ing properly.  The cause most times will be resources not being cleaned up.  Tail the cluster api AWS provider logs and grep for the cluster in question. For example, the following is looking for log entries for the eks-damarcil-1 cluster.
k
logs
capa-controller-manager-6d785797dc-ppgs4
-n
capa-system
-f
|
grep
eks-damarcil-1
The following steps are how you further investigate the issue once you have done the above.
Verify that the cluster is in the deleting state.
k
get
clusters
-n
capi-eks-clusters
NAMESPACE
NAME
PHASE
AGE
VERSION
capi-eks-clusters
calisti-eks-test1
Provisioned
3d
capi-eks-clusters
calisti-sre-eks-test1
Provisioned
85m
capi-eks-clusters
eks-example-1
Provisioned
47d
capi-eks-clusters
eks-fst-calisti-1
Provisioned
12d
capi-eks-clusters
eks-hliu4-2
Deleting
50d
capi-eks-clusters
eks-hliu4-test2
Provisioned
3d
capi-eks-clusters
eks-nmallapr-1
Provisioned
13d
capi-eks-clusters
eks-rnasani-1
Provisioned
33d
capi-eks-clusters
eks-saolds-1
Provisioned
14d
capi-eks-clusters
eks-shridhsh-1
Provisioned
17d
capi-eks-clusters
eks-sicoli-1
Provisioned
2d20h
capi-eks-clusters
eks-sushroff-1
Provisioned
18d
Determine the lowest resource that is still not being cleaned out.  Start with the AWS managed resources.  In the following example, we first look at the
awsmanagedcontrolplanes
, there is no resource for that for the
eks-hliu4-2
cluster.
k
get
awsmanagedcontrolplanes.controlplane.cluster.x-k8s.io
-n
capi-eks-clusters
NAMESPACE
NAME
CLUSTER
READY
VPC
BASTION
IP
capi-eks-clusters
calisti-eks-test1-control-plane
calisti-eks-test1
true
vpc-0e2e596de8475f9e3
capi-eks-clusters
calisti-sre-eks-test1-control-plane
calisti-sre-eks-test1
true
vpc-0cb4158638eb75a4d
capi-eks-clusters
eks-example-1-control-plane
eks-example-1
true
vpc-03438f4729d5d4d83
capi-eks-clusters
eks-fst-calisti-1-control-plane
eks-fst-calisti-1
true
vpc-092571a45afaee57a
capi-eks-clusters
eks-hliu4-test2-control-plane
eks-hliu4-test2
true
vpc-0daeec51833e8378d
capi-eks-clusters
eks-nmallapr-1-control-plane
eks-nmallapr-1
true
vpc-0156a3980c88ab33d
capi-eks-clusters
eks-rnasani-1-control-plane
eks-rnasani-1
true
vpc-082b1b6702d5abd17
capi-eks-clusters
eks-saolds-1-control-plane
eks-saolds-1
true
vpc-0b13744c240e51b9e
capi-eks-clusters
eks-shridhsh-1-control-plane
eks-shridhsh-1
true
vpc-03c1a7bbd72182de1
capi-eks-clusters
eks-sicoli-1-control-plane
eks-sicoli-1
true
vpc-038105006a1e2dc40
capi-eks-clusters
eks-sushroff-1-control-plane
eks-sushroff-1
true
vpc-00f24f7a2215929d3
Next we look at the
awsmanagedmachinepool
and find that it no longer exists.
k
get
awsmanagedmachinepool
-n
capi-eks-clusters
NAMESPACE
NAME
READY
REPLICAS
capi-eks-clusters
calisti-eks-test1-pool-0
true
6
capi-eks-clusters
calisti-sre-eks-test1-pool-0
true
8
capi-eks-clusters
eks-example-1-pool-0
true
3
capi-eks-clusters
eks-fst-calisti-1-pool-0
true
3
capi-eks-clusters
eks-hliu4-test2-pool-0
true
2
capi-eks-clusters
eks-nmallapr-1-pool-0
true
6
capi-eks-clusters
eks-rnasani-1-pool-0
true
6
capi-eks-clusters
eks-saolds-1-pool-0
true
6
capi-eks-clusters
eks-shridhsh-1-pool-0
true
6
capi-eks-clusters
eks-sicoli-1-pool-0
true
6
capi-eks-clusters
eks-sushroff-1-pool-0
true
6
Finally we look at the base resources of
controlplane
and
machinepool
and here is where we notice resources that are still in the
deleting
state.
k
get
machinepool
-n
capi-eks-clusters
NAMESPACE
NAME
CLUSTER
REPLICAS
PHASE
AGE
VERSION
capi-eks-clusters
calisti-eks-test1-pool-0
calisti-eks-test1
6
Running
3d
capi-eks-clusters
calisti-sre-eks-test1-pool-0
calisti-sre-eks-test1
8
Running
86m
capi-eks-clusters
eks-example-1-pool-0
eks-example-1
3
Running
47d
capi-eks-clusters
eks-fst-calisti-1-pool-0
eks-fst-calisti-1
3
Running
12d
capi-eks-clusters
eks-hliu4-2-pool-0
eks-hliu4-2
3
Deleting
50d
capi-eks-clusters
eks-hliu4-test2-pool-0
eks-hliu4-test2
2
Running
3d
capi-eks-clusters
eks-nmallapr-1-pool-0
eks-nmallapr-1
6
Running
13d
capi-eks-clusters
eks-rnasani-1-pool-0
eks-rnasani-1
6
Running
33d
capi-eks-clusters
eks-saolds-1-pool-0
eks-saolds-1
6
Running
14d
capi-eks-clusters
eks-shridhsh-1-pool-0
eks-shridhsh-1
6
Running
17d
capi-eks-clusters
eks-sicoli-1-pool-0
eks-sicoli-1
6
Running
2d20h
capi-eks-clusters
eks-sushroff-1-pool-0
eks-sushroff-1
6
Running
18d
Since we know the EKS cluster is gone and there is no node group, we need to remove the resourse.  To do that edit the
machinepool
and remove the
finalizers
.  Once that is done, the cluster will disappear.
k
edit
machinepool
eks-hliu4-2-pool-0
-n
capi-eks-clusters
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion
:
cluster.x-k8s.io/v1beta1
kind
:
MachinePool
metadata
:
annotations
:
kubectl.kubernetes.io/last-applied-configuration
:
|
{
"apiVersion"
:
"cluster.x-k8s.io/v1beta1"
,
"kind"
:
"MachinePool"
,
"metadata"
:{
"annotations"
:{},
"labels"
:{
"argocd-clusters.prod.eticloud.io"
:
"eks-hliu4-2"
},
"name"
:
"eks-hliu4-2-pool-0"
,
"namespace"
:
"capi-eks-clusters"
},
"spec"
:{
"clusterName"
:
"eks-hliu4-2"
,
"replicas"
:
3
,
"template"
:{
"spec"
:{
"bootstrap"
:{
"dataSecretName"
:
""
},
"clusterName"
:
"eks-hliu4-2"
,
"infrastructureRef"
:{
"apiVersion"
:
"infrastructure.cluster.x-k8s.io/v1beta1"
,
"kind"
:
"AWSManagedMachinePool"
,
"name"
:
"eks-hliu4-2-pool-0"
}}}}}
creationTimestamp
:
"2022-12-04T15:48:50Z"
deletionGracePeriodSeconds
:
0
deletionTimestamp
:
"2023-01-20T20:48:21Z"
finalizers
:
-
machinepool.cluster.x-k8s.io
generation
:
5
labels
:
2023-01-24