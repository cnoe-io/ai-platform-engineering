2022-11-29: elastic-dev-1 is running out of space
¶
Summary
¶
elastic-dev-1 ran out of space and alerted SRE team.
Table of Contents
¶
Summary
When
Timeline
Impact
System(s) impacted?
Teams impacted?
Analysis
Takeaways
When
¶
On 2022-11-22, SRE team was alerted from Elasticsearch Service that it is nearing limit for storage.
Timeline
¶
Timeline of resolution
2022-11-24
- The API key linked to the S3 bucket used by the cloud snapshot policy was disable.
Incident timeline:
2022-11-22 07:36 PM CEST
- Elasticsearch Service send elastic-dev-1 near limit storage alert.
2022-11-28 07:36 PM CEST
- Sri and David created the JIRA tasks following multiple alerts received for elastic-dev-1.
2022-11-28 12:50 PM CEST
- Sri assigned the task to Jeremie.
2022-11-28 12:50 PM CEST
- Jeremie investigated and resolved the issue.
Impact
¶
System(s) impacted?
¶
No production impact
SRE clean-up policy was broken
Teams impacted?
¶
SRE
Analysis
¶
The original event came from Elastic service mentioning the elastic-dev-1 deployment is going to reach disk space issue.
- Sri provider the pointer to
elastic-dev-1
. This cluster information was not available using Cisco SSO. Cisco generic user with basic auth credentials are stored in
keeper
.
Dan suggested that we re-index the logstash indices using
https://www.thirdrocktechkno.com/blog/6-steps-to-reindex-elasticsearch-data/
.
Investigation:
Started the investigation on the
indicies in elastic-dev-1
. They were showing lot of big and old files from logstash to self-svc. Most of the indicies were in yellow or red state.
A closer look at logstash index details showed an error on backup failure since 24
th
Nov 2022
Also verified
cloud-snapshot-policy
and it showed the following error:
{
"type": "snapshot_exception",
"reason": "[found-snapshots:cloud-snapshot-2022.11.29-v6hw8k2vqsujegdnw8pjna] failed to create snapshot successfully, 24 out of 129 total shards failed",
"stack_trace": "SnapshotException[[found-snapshots:cloud-snapshot-2022.11.29-v6hw8k2vqsujegdnw8pjna] failed to create snapshot successfully, 24 out of 129 total shards failed]
Verified the
repositories
details and
found that there a connection issue to S3
: amazon_s3_exception.
Continued to look at
IAM configuration in eticloud
linked to the
S3 bucket
and confirmed that the AWS IAM API key was disabled.
Re-enabled the AWS IAM Key. That did resolve the connection issue to S3 bucket
Re-ran the index clean-up policy to clear the alert received as a part of this Page.
Previous re-run operation succeeded and removed old indices which were very large. This freed up lot of extra space.
Cluster was back to healthy state and we were able to udpate its configuration.
Takeaways
¶
The documentation must include access credentials to services/environments.
AWS IAM API Key deprecation need to be widely communicated and documented.
Although the cluster is back to normal, we will need a follow-up task to upgrade it to a newer version as the current version will be EOL in 65 days.
2023-08-29