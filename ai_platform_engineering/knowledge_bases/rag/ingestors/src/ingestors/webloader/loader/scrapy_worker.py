#!/usr/bin/env python3
"""
Scrapy worker subprocess.

This module runs in a separate process with Twisted's reactor.run() as the main loop.
It receives crawl requests via multiprocessing Queue and sends results back.

Usage:
    This module is spawned by ScrapyWorkerPool using multiprocessing.Process.
    Do not run directly.
"""

# Install Twisted reactor FIRST before any other imports
from scrapy.utils.reactor import install_reactor

install_reactor("twisted.internet.asyncioreactor.AsyncioSelectorReactor")

import hashlib
import re
import sys
import time
import traceback
from multiprocessing import Queue
from typing import List
from urllib.parse import urlparse, urljoin

from twisted.internet import reactor
from scrapy import Spider, Request
from scrapy.crawler import CrawlerRunner
from scrapy.http import Response
from scrapy.utils.log import configure_logging

from common import utils as common_utils
from common.models.server import ScrapySettings, CrawlMode

from .worker_types import (
  WorkerMessage,
  MessageType,
  CrawlRequest,
  CrawlProgress,
  CrawlDocuments,
  CrawlResult,
  CrawlStatus,
)
from .settings import build_scrapy_settings
from .parsers import ParserRegistry

# Import all parsers to register them
from .parsers import docusaurus, mkdocs, sphinx, readthedocs, vitepress, generic  # noqa: F401


class WorkerSpider(Spider):
  """
  Generic spider that handles all crawl modes.

  This spider is configured at runtime based on the CrawlRequest.
  """

  name = "worker_spider"

  def __init__(
    self,
    request: CrawlRequest,
    result_queue: Queue,
    *args,
    **kwargs,
  ):
    super().__init__(*args, **kwargs)
    self.crawl_request = request
    self.result_queue = result_queue

    self.start_url = request.url
    self.max_pages = request.max_pages
    self.crawl_mode = request.crawl_mode
    self.follow_external = request.follow_external_links
    self.allowed_patterns = request.allowed_url_patterns or []
    self.denied_patterns = request.denied_url_patterns or []

    # Track the effective domain (may change after redirect for sitemap mode)
    self.effective_domain: str | None = None

    # Tracking
    self.pages_crawled = 0
    self.pages_failed = 0
    self.documents: List[dict] = []
    self.visited_urls: set = set()
    self.start_time = time.time()

    # Track filtering stats for better error messages
    self.urls_found_in_sitemap = 0
    self.urls_filtered_external = 0
    self.urls_filtered_pattern = 0
    self.urls_filtered_max_pages = 0

    # Collect error messages for reporting
    self.errors: list[str] = []
    self.max_errors = 50  # Limit to prevent memory issues

    # Track sitemap discovery attempts for error reporting
    self.sitemap_urls_checked: list[str] = []
    self.robots_urls_checked: list[str] = []
    self.sitemap_url_used: str | None = None  # The sitemap that was successfully loaded

    # Progress tracking
    self.total_pages_to_crawl: int | None = None  # Known total (from sitemap)
    self.pending_urls: set = set()  # URLs queued but not yet crawled

    # Progress reporting
    self.last_progress_time = 0
    self.progress_interval = 2  # Report progress every 2 seconds (was 5)

    # Cancellation support
    self._cancelled = False

    # Batch streaming settings
    self.batch_size = 50  # Send documents to main process every N documents
    self.batch_number = 0
    self.documents_in_current_batch: List[dict] = []

  def _build_request_meta(self, **extra_meta) -> dict:
    """
    Build request meta dict with Playwright settings if JS rendering is enabled.

    Args:
        **extra_meta: Additional meta fields to include

    Returns:
        Meta dict for Request objects
    """
    meta = dict(extra_meta)

    if self.crawl_request.render_javascript:
      from scrapy_playwright.page import PageMethod

      # Enable Playwright for this request
      meta["playwright"] = True
      meta["playwright_include_page"] = False  # Don't need page object in callback

      # Build page methods for waiting
      page_methods = []
      if self.crawl_request.wait_for_selector:
        page_methods.append(
          PageMethod(
            "wait_for_selector",
            self.crawl_request.wait_for_selector,
            timeout=self.crawl_request.page_load_timeout * 1000,
          )
        )
      # Wait for network idle to ensure dynamic content is loaded
      page_methods.append(
        PageMethod(
          "wait_for_load_state",
          "networkidle",
          timeout=self.crawl_request.page_load_timeout * 1000,
        )
      )

      if page_methods:
        meta["playwright_page_methods"] = page_methods

    return meta

  def start_requests(self):
    """Generate initial request(s) based on crawl mode."""
    # Send initial progress message for JS rendering
    if self.crawl_request.render_javascript:
      self.logger.info("JavaScript rendering enabled - starting Chromium browser")
      progress = CrawlProgress(
        job_id=self.crawl_request.job_id,
        pages_crawled=0,
        pages_failed=0,
        message="Starting Chromium browser for JavaScript rendering...",
      )
      self.result_queue.put(WorkerMessage.crawl_progress(progress).to_dict())

    if self.crawl_mode == "sitemap":
      # For sitemap mode, try to discover sitemap.xml
      # First try subdirectory path, then fall back to root domain
      # For example: https://example.com/docs/ -> try /docs/sitemap.xml, then /sitemap.xml
      parsed = urlparse(self.start_url)
      subdirectory_base = self.start_url.rstrip("/")
      root_base = f"{parsed.scheme}://{parsed.netloc}"

      # Determine if we have a subdirectory path
      has_subdirectory = parsed.path and parsed.path.rstrip("/") != ""

      # Try subdirectory sitemap.xml first (if there's a path)
      sitemap_url = f"{subdirectory_base}/sitemap.xml"
      self.sitemap_urls_checked.append(sitemap_url)
      yield Request(
        sitemap_url,
        callback=self.parse_sitemap,
        errback=self.handle_sitemap_error,
        meta={
          "subdirectory_base": subdirectory_base,
          "root_base": root_base,
          "has_subdirectory": has_subdirectory,
          "is_root_fallback": False,
        },
      )
    else:
      # Single URL or recursive mode - start with the URL
      yield Request(self.start_url, callback=self.parse_page, errback=self.handle_error, meta=self._build_request_meta())

  def parse_sitemap(self, response: Response):
    """Parse sitemap.xml and yield requests for each URL."""
    # Update effective domain based on where we actually landed (handles redirects)
    self.effective_domain = urlparse(response.url).netloc
    self.sitemap_url_used = response.url
    self.logger.info(f"Sitemap loaded from {response.url}, effective domain: {self.effective_domain}")

    # Extract URLs from sitemap
    urls = re.findall(r"<loc>(.*?)</loc>", response.text)
    self.urls_found_in_sitemap = len(urls)

    self.logger.info(f"Found {len(urls)} URLs in sitemap")

    # Track how many URLs we'll actually crawl
    urls_to_crawl = []
    for url in urls[: self.max_pages]:
      if self._should_follow(url):
        urls_to_crawl.append(url)
        self.pending_urls.add(url)

    # Set total for progress tracking
    self.total_pages_to_crawl = len(urls_to_crawl)

    self.logger.info(f"Queued {len(urls_to_crawl)} URLs for crawling. Filtered: {self.urls_filtered_external} external, {self.urls_filtered_pattern} by pattern, {self.urls_filtered_max_pages} over max pages limit")

    # Yield requests
    for url in urls_to_crawl:
      yield Request(url, callback=self.parse_page, errback=self.handle_error, meta=self._build_request_meta())

  def handle_sitemap_error(self, failure):
    """
    Handle sitemap fetch failure with two-step fallback:
    1. If subdirectory sitemap failed, try root sitemap
    2. If root sitemap failed (or no subdirectory), try robots.txt (subdirectory then root)
    3. If all fail, raise error (don't fall back to single page crawl)
    """
    meta = failure.request.meta
    subdirectory_base = meta.get("subdirectory_base", self.start_url.rstrip("/"))
    root_base = meta.get("root_base", subdirectory_base)
    has_subdirectory = meta.get("has_subdirectory", False)
    is_root_fallback = meta.get("is_root_fallback", False)

    error_detail = self._get_failure_reason(failure)
    error_msg = f"Sitemap fetch failed ({error_detail}): {failure.request.url}"
    self.logger.warning(error_msg)
    if len(self.errors) < self.max_errors:
      self.errors.append(error_msg)

    # If we haven't tried root sitemap yet and there's a subdirectory, try root
    if has_subdirectory and not is_root_fallback:
      root_sitemap_url = f"{root_base}/sitemap.xml"
      self.logger.info(f"Trying root sitemap: {root_sitemap_url}")
      self.sitemap_urls_checked.append(root_sitemap_url)
      yield Request(
        root_sitemap_url,
        callback=self.parse_sitemap,
        errback=self.handle_sitemap_error,
        meta={
          "subdirectory_base": subdirectory_base,
          "root_base": root_base,
          "has_subdirectory": has_subdirectory,
          "is_root_fallback": True,
        },
      )
    else:
      # Both sitemaps failed (or no subdirectory), try robots.txt
      # Start with subdirectory robots.txt if applicable
      if has_subdirectory:
        robots_url = f"{subdirectory_base}/robots.txt"
        self.logger.info(f"Trying subdirectory robots.txt: {robots_url}")
        self.robots_urls_checked.append(robots_url)
        yield Request(
          robots_url,
          callback=self.parse_robots,
          errback=self.handle_robots_error,
          meta={
            "subdirectory_base": subdirectory_base,
            "root_base": root_base,
            "is_root_fallback": False,
          },
        )
      else:
        # No subdirectory, try root robots.txt directly
        robots_url = f"{root_base}/robots.txt"
        self.logger.info(f"Trying robots.txt: {robots_url}")
        self.robots_urls_checked.append(robots_url)
        yield Request(
          robots_url,
          callback=self.parse_robots,
          errback=self.handle_robots_error,
          meta={
            "subdirectory_base": subdirectory_base,
            "root_base": root_base,
            "is_root_fallback": True,  # Mark as final attempt
          },
        )

  def parse_robots(self, response: Response):
    """Parse robots.txt for sitemap URLs."""
    sitemaps = re.findall(r"Sitemap:\s*(\S+)", response.text, re.IGNORECASE)

    if sitemaps:
      self.logger.info(f"Found {len(sitemaps)} sitemap(s) in robots.txt: {sitemaps}")
      for sitemap_url in sitemaps:
        self.sitemap_urls_checked.append(sitemap_url)
        yield Request(
          sitemap_url,
          callback=self.parse_sitemap,
          errback=self.handle_sitemap_from_robots_error,
          meta={"from_robots": True},
        )
    else:
      # No sitemap in robots.txt - try root robots.txt if we checked subdirectory
      meta = response.request.meta
      is_root_fallback = meta.get("is_root_fallback", False)
      root_base = meta.get("root_base", "")
      subdirectory_base = meta.get("subdirectory_base", "")

      if not is_root_fallback and root_base != subdirectory_base:
        # Try root robots.txt
        robots_url = f"{root_base}/robots.txt"
        self.logger.info(f"No sitemap in subdirectory robots.txt, trying root: {robots_url}")
        self.robots_urls_checked.append(robots_url)
        yield Request(
          robots_url,
          callback=self.parse_robots,
          errback=self.handle_robots_error,
          meta={
            "subdirectory_base": subdirectory_base,
            "root_base": root_base,
            "is_root_fallback": True,
          },
        )
      else:
        # All options exhausted - fail with detailed error
        self._fail_sitemap_discovery("No Sitemap directive found in robots.txt")

  def handle_robots_error(self, failure):
    """
    Handle robots.txt fetch failure.
    Try root robots.txt if subdirectory failed, otherwise fail with error.
    """
    meta = failure.request.meta
    is_root_fallback = meta.get("is_root_fallback", False)
    root_base = meta.get("root_base", "")
    subdirectory_base = meta.get("subdirectory_base", "")

    error_detail = self._get_failure_reason(failure)
    error_msg = f"robots.txt fetch failed ({error_detail}): {failure.request.url}"
    self.logger.warning(error_msg)
    if len(self.errors) < self.max_errors:
      self.errors.append(error_msg)

    if not is_root_fallback and root_base != subdirectory_base:
      # Try root robots.txt
      robots_url = f"{root_base}/robots.txt"
      self.logger.info(f"Trying root robots.txt: {robots_url}")
      self.robots_urls_checked.append(robots_url)
      yield Request(
        robots_url,
        callback=self.parse_robots,
        errback=self.handle_robots_error,
        meta={
          "subdirectory_base": subdirectory_base,
          "root_base": root_base,
          "is_root_fallback": True,
        },
      )
    else:
      # All options exhausted - fail with detailed error
      self._fail_sitemap_discovery("robots.txt not found or inaccessible")

  def handle_sitemap_from_robots_error(self, failure):
    """Handle failure when fetching a sitemap URL found in robots.txt."""
    error_detail = self._get_failure_reason(failure)
    error_msg = f"Sitemap from robots.txt failed ({error_detail}): {failure.request.url}"
    self.logger.error(error_msg)
    if len(self.errors) < self.max_errors:
      self.errors.append(error_msg)
    # This sitemap URL was explicitly listed in robots.txt but failed
    # Don't try other fallbacks - this is a configuration error on the site
    self._fail_sitemap_discovery(f"Sitemap URL from robots.txt is not accessible: {failure.request.url}")

  def _fail_sitemap_discovery(self, reason: str):
    """
    Record a fatal sitemap discovery failure.
    Called when all sitemap/robots.txt fallbacks have been exhausted.
    """
    # Build detailed error message with all URLs checked
    checked_urls = []
    if self.sitemap_urls_checked:
      checked_urls.append(f"Sitemaps checked: {', '.join(self.sitemap_urls_checked)}")
    if self.robots_urls_checked:
      checked_urls.append(f"robots.txt checked: {', '.join(self.robots_urls_checked)}")

    full_error = f"Sitemap discovery failed: {reason}. {' | '.join(checked_urls)}"
    self.logger.error(full_error)
    if len(self.errors) < self.max_errors:
      self.errors.append(full_error)

    # Mark the crawl as having a fatal sitemap error
    # The spider will close with 0 pages crawled, and the error will be reported

  def parse_page(self, response: Response):
    """Parse a page and extract content."""
    # Remove from pending set
    self.pending_urls.discard(response.url)

    # Check if crawl was cancelled
    if self._cancelled:
      self.logger.debug(f"Skipping {response.url} - crawl cancelled")
      return

    # Check limits
    if self.pages_crawled >= self.max_pages:
      return

    # Skip if already visited
    if response.url in self.visited_urls:
      return
    self.visited_urls.add(response.url)

    # Update effective domain after following redirects (e.g., caipe.io -> cnoe-io.github.io)
    # This ensures that in recursive mode, we follow links on the actual domain we landed on
    if self.effective_domain is None:
      response_domain = urlparse(response.url).netloc
      start_domain = urlparse(self.start_url).netloc
      if response_domain != start_domain:
        self.effective_domain = response_domain
        self.logger.info(f"Detected redirect: {start_domain} -> {response_domain}, updating effective domain")

    # Handle non-200 responses
    if response.status != 200:
      error_msg = f"Ignoring non-200 response ({response.status}): {response.url}"
      self.logger.warning(error_msg)
      self.pages_failed += 1
      if len(self.errors) < self.max_errors:
        self.errors.append(error_msg)
      return

    try:
      # Extract content using parser registry
      result = ParserRegistry.parse(response)

      if result.content and len(result.content.strip()) >= 10:
        # Create document
        now = int(time.time())
        # Use the system default freshness (configurable via DEFAULT_FRESH_UNTIL_SECONDS env var)
        fresh_until = common_utils.get_default_fresh_until()

        doc = {
          "id": self._generate_doc_id(response.url),
          "page_content": result.content,
          "metadata": {
            "datasource_id": self.crawl_request.datasource_id,
            "document_id": self._generate_doc_id(response.url),
            "title": result.title or "",
            "description": result.description or "",
            "document_type": "webpage",
            "document_ingested_at": now,
            "fresh_until": fresh_until,
            "ingestor_id": self.crawl_request.ingestor_id,
            "is_graph_entity": False,
            "metadata": {
              "source": response.url,
              "language": result.language or "",
              "generator": result.generator or "",
            },
          },
        }
        # Add to current batch instead of documents list (for streaming)
        self.documents.append(doc)
        self.documents_in_current_batch.append(doc)
        self.pages_crawled += 1

        # Flush batch if it's full
        self._maybe_flush_document_batch()

        self.logger.debug(f"Parsed page {self.pages_crawled}: {response.url}")
      else:
        # Skip pages with no meaningful content (redirects, images, etc.)
        # This is not an error, just nothing to extract
        self.logger.debug(f"Skipped page with no content: {response.url}")

    except Exception as e:
      error_msg = f"Error parsing {response.url}: {e}"
      self.logger.error(error_msg)
      self.pages_failed += 1
      # Collect error messages for reporting
      if len(self.errors) < self.max_errors:
        self.errors.append(error_msg)

    # Report progress periodically
    self._maybe_report_progress(response.url)

    # Follow links if in recursive mode (but not if cancelled)
    if self.crawl_mode == "recursive" and self.pages_crawled < self.max_pages and not self._cancelled:
      for link in self._extract_links(response):
        if self._should_follow(link) and link not in self.pending_urls:
          self.pending_urls.add(link)
          yield Request(link, callback=self.parse_page, errback=self.handle_error, meta=self._build_request_meta())

  def handle_error(self, failure):
    """Handle request errors."""
    url = failure.request.url

    # Extract meaningful error details from the failure
    error_detail = self._get_failure_reason(failure)
    error_msg = f"{error_detail}: {url}"

    self.logger.error(error_msg)
    self.pages_failed += 1
    # Collect error messages for reporting
    if len(self.errors) < self.max_errors:
      self.errors.append(error_msg)

  def _get_failure_reason(self, failure) -> str:
    """Extract a human-readable reason from a Twisted Failure."""
    from twisted.internet.error import DNSLookupError, TimeoutError, ConnectionRefusedError, TCPTimedOutError
    from scrapy.spidermiddlewares.httperror import HttpError

    exc = failure.value

    # Check for specific exception types
    if failure.check(HttpError):
      response = exc.response
      return f"HTTP {response.status}"
    elif failure.check(DNSLookupError):
      return "DNS lookup failed"
    elif failure.check(TimeoutError, TCPTimedOutError):
      return "Connection timed out"
    elif failure.check(ConnectionRefusedError):
      return "Connection refused"
    else:
      # For other errors, use the exception class name and message
      exc_name = type(exc).__name__
      exc_msg = str(exc)
      if exc_msg and exc_msg != exc_name:
        return f"{exc_name}: {exc_msg}"
      return exc_name

  def _should_follow(self, url: str, track_filtering: bool = True) -> bool:
    """
    Check if a URL should be followed.

    Args:
        url: The URL to check
        track_filtering: If True, increment filtering counters when rejecting URLs
    """
    if url in self.visited_urls:
      return False

    if self.pages_crawled >= self.max_pages:
      if track_filtering:
        self.urls_filtered_max_pages += 1
      return False

    # Check external links
    if not self.follow_external:
      # Use effective_domain if set (e.g., after following sitemap redirect)
      # Otherwise use the original start_url domain
      if self.effective_domain:
        allowed_domain = self.effective_domain
      else:
        allowed_domain = urlparse(self.start_url).netloc

      url_domain = urlparse(url).netloc
      if url_domain != allowed_domain:
        if track_filtering:
          self.urls_filtered_external += 1
          # Log the first few for debugging
          if self.urls_filtered_external <= 3:
            self.logger.debug(f"Filtered external URL: {url} (domain {url_domain} != {allowed_domain})")
        return False

    # Check allowed patterns
    if self.allowed_patterns:
      if not any(re.search(p, url) for p in self.allowed_patterns):
        if track_filtering:
          self.urls_filtered_pattern += 1
        return False

    # Check denied patterns
    if self.denied_patterns:
      if any(re.search(p, url) for p in self.denied_patterns):
        if track_filtering:
          self.urls_filtered_pattern += 1
        return False

    return True

  def _extract_links(self, response: Response) -> List[str]:
    """Extract links from a response."""
    links = []
    for href in response.css("a::attr(href)").getall():
      # Skip anchors, javascript, mailto, etc.
      if href.startswith(("#", "javascript:", "mailto:", "tel:")):
        continue

      # Convert to absolute URL
      absolute_url = urljoin(response.url, href)

      # Only follow http/https
      if absolute_url.startswith(("http://", "https://")):
        links.append(absolute_url)

    return links

  def _generate_doc_id(self, url: str) -> str:
    """Generate a document ID from URL."""
    url_hash = hashlib.sha256(url.encode()).hexdigest()[:12]
    return f"doc_{self.crawl_request.datasource_id}_{url_hash}"

  def _maybe_report_progress(self, current_url: str):
    """Report progress if enough time has passed."""
    now = time.time()
    if now - self.last_progress_time >= self.progress_interval:
      self.last_progress_time = now

      # Build progress message based on crawl mode
      queue_size = len(self.pending_urls)
      if self.total_pages_to_crawl:
        # Sitemap mode - we know the total
        message = f"Crawling {self.pages_crawled}/{self.total_pages_to_crawl} pages"
      elif queue_size > 0:
        # Recursive mode - show queue size
        message = f"Crawling... {self.pages_crawled} pages ({queue_size} queued)"
      else:
        message = f"Crawling... {self.pages_crawled} pages"

      progress = CrawlProgress(
        job_id=self.crawl_request.job_id,
        pages_crawled=self.pages_crawled,
        pages_failed=self.pages_failed,
        current_url=current_url,
        message=message,
        total_pages=self.total_pages_to_crawl,
        queue_size=queue_size,
      )
      self.result_queue.put(WorkerMessage.crawl_progress(progress).to_dict())

  def cancel(self):
    """
    Cancel the crawl.

    Called by worker_main when a CANCEL_CRAWL message is received.
    The spider will stop processing new pages and close gracefully.
    """
    self.logger.info(f"Crawl cancelled for job {self.crawl_request.job_id}")
    self._cancelled = True
    # Flush any pending documents
    self._flush_document_batch(is_final=True)

  def _maybe_flush_document_batch(self):
    """Flush document batch if we've reached batch_size."""
    if len(self.documents_in_current_batch) >= self.batch_size:
      self._flush_document_batch(is_final=False)

  def _flush_document_batch(self, is_final: bool = False):
    """
    Send accumulated documents to main process.

    Args:
        is_final: True if this is the last batch (spider closing)
    """
    if not self.documents_in_current_batch and not is_final:
      return

    self.batch_number += 1
    docs = CrawlDocuments(
      job_id=self.crawl_request.job_id,
      documents=self.documents_in_current_batch,
      batch_number=self.batch_number,
      is_final_batch=is_final,
    )
    self.result_queue.put(WorkerMessage.crawl_documents(docs).to_dict())
    self.logger.info(f"Sent document batch {self.batch_number} with {len(self.documents_in_current_batch)} documents (final={is_final})")

    # Clear the batch
    self.documents_in_current_batch = []

  def closed(self, reason):
    """Called when spider closes."""
    elapsed = time.time() - self.start_time

    # Flush any remaining documents in the batch
    if self.documents_in_current_batch:
      self._flush_document_batch(is_final=True)

    # Determine status and build fatal error message
    fatal_error = None
    if self._cancelled:
      status = CrawlStatus.PARTIAL
      fatal_error = "Crawl was cancelled"
    elif self.pages_crawled == 0:
      status = CrawlStatus.FAILED
      # Build detailed error message explaining why no pages were crawled
      fatal_error = self._build_failure_message()
    elif self.pages_failed > 0:
      status = CrawlStatus.PARTIAL
    else:
      status = CrawlStatus.SUCCESS

    result = CrawlResult(
      job_id=self.crawl_request.job_id,
      status=status,
      pages_crawled=self.pages_crawled,
      pages_failed=self.pages_failed,
      documents=[],  # Documents are now streamed via CRAWL_DOCUMENTS messages
      elapsed_seconds=elapsed,
      fatal_error=fatal_error,
      errors=self.errors,
      # Include filtering stats for debugging
      urls_found_in_sitemap=self.urls_found_in_sitemap,
      urls_filtered_external=self.urls_filtered_external,
      urls_filtered_pattern=self.urls_filtered_pattern,
      urls_filtered_max_pages=self.urls_filtered_max_pages,
      # Include sitemap discovery info
      sitemap_url_used=self.sitemap_url_used,
    )

    self.result_queue.put(WorkerMessage.crawl_result(result).to_dict())
    self.logger.info(f"Spider closed: {reason}, crawled {self.pages_crawled} pages in {elapsed:.1f}s")

  def _build_failure_message(self) -> str:
    """Build a detailed message explaining why the crawl failed."""
    parts = []

    # Check if this was a sitemap crawl that found URLs but none were followed
    if self.urls_found_in_sitemap > 0:
      parts.append(f"Found {self.urls_found_in_sitemap} URLs in sitemap but 0 were scraped.")

      filter_details = []
      if self.urls_filtered_external > 0:
        original_domain = urlparse(self.start_url).netloc
        effective = self.effective_domain or original_domain
        if original_domain != effective:
          filter_details.append(f"{self.urls_filtered_external} filtered as external (sitemap domain '{effective}' differs from start URL domain '{original_domain}')")
        else:
          filter_details.append(f"{self.urls_filtered_external} filtered as external links")

      if self.urls_filtered_pattern > 0:
        filter_details.append(f"{self.urls_filtered_pattern} filtered by URL patterns")

      if self.urls_filtered_max_pages > 0:
        filter_details.append(f"{self.urls_filtered_max_pages} filtered by max pages limit ({self.max_pages})")

      if filter_details:
        parts.append("Filtering breakdown: " + "; ".join(filter_details) + ".")

      # Suggest fix for domain mismatch
      if self.urls_filtered_external > 0 and self.effective_domain:
        original_domain = urlparse(self.start_url).netloc
        if original_domain != self.effective_domain:
          parts.append(f"Tip: The site redirects from '{original_domain}' to '{self.effective_domain}'. Try using 'https://{self.effective_domain}' as the start URL, or enable 'Follow external links' to allow cross-domain crawling.")
    else:
      # Generic failure message - could be sitemap discovery failure or other issue
      if self.crawl_mode == "sitemap":
        parts.append("Sitemap discovery failed.")
        if self.sitemap_urls_checked:
          parts.append(f"Sitemaps checked: {', '.join(self.sitemap_urls_checked)}.")
        if self.robots_urls_checked:
          parts.append(f"robots.txt checked: {', '.join(self.robots_urls_checked)}.")
      else:
        parts.append("No pages were crawled.")

      if self.pages_failed > 0:
        parts.append(f"{self.pages_failed} requests failed.")

    # Include collected error messages for more detail
    if self.errors:
      parts.append("Errors: " + "; ".join(self.errors[:5]))  # Show first 5 errors
      if len(self.errors) > 5:
        parts.append(f"(and {len(self.errors) - 5} more)")

    return " ".join(parts)


def build_spider_settings(request: CrawlRequest) -> dict:
  """Build Scrapy settings from a CrawlRequest."""
  # Convert to ScrapySettings
  settings = ScrapySettings(
    crawl_mode=CrawlMode(request.crawl_mode),
    max_depth=request.max_depth,
    max_pages=request.max_pages,
    render_javascript=request.render_javascript,
    wait_for_selector=request.wait_for_selector,
    page_load_timeout=request.page_load_timeout,
    follow_external_links=request.follow_external_links,
    allowed_url_patterns=request.allowed_url_patterns,
    denied_url_patterns=request.denied_url_patterns,
    download_delay=request.download_delay,
    concurrent_requests=request.concurrent_requests,
    respect_robots_txt=request.respect_robots_txt,
    user_agent=request.user_agent,
  )

  return build_scrapy_settings(settings)


def run_crawl(request: CrawlRequest, result_queue: Queue, spider_holder: dict):
  """
  Run a single crawl using Scrapy.

  This function sets up the CrawlerRunner and runs the spider.

  Args:
      request: The crawl request configuration
      result_queue: Queue to send results back to main process
      spider_holder: Dict to store reference to active spider for cancellation
  """
  from scrapy import signals

  # Build settings
  scrapy_settings = build_spider_settings(request)

  # Configure logging
  configure_logging({"LOG_LEVEL": "INFO"})

  # Create runner
  runner = CrawlerRunner(settings=scrapy_settings)

  # Store request info for spider lookup
  spider_holder["job_id"] = request.job_id

  # Use signal to capture spider reference when it opens
  def on_spider_opened(spider):
    spider_holder["spider"] = spider

  def on_spider_closed(spider, reason):
    spider_holder["spider"] = None
    spider_holder["job_id"] = None

  # Create crawler and connect signals
  crawler = runner.create_crawler(WorkerSpider)
  crawler.signals.connect(on_spider_opened, signal=signals.spider_opened)
  crawler.signals.connect(on_spider_closed, signal=signals.spider_closed)

  # Run spider
  deferred = runner.crawl(crawler, request=request, result_queue=result_queue)

  return deferred


def worker_main(worker_id: int, request_queue: Queue, result_queue: Queue):
  """
  Main entry point for worker subprocess.

  This function runs the Twisted reactor and processes crawl requests.

  Args:
      worker_id: Unique ID for this worker
      request_queue: Queue to receive crawl requests from
      result_queue: Queue to send results back to main process
  """
  print(f"[Worker {worker_id}] Starting...")

  # Signal that we're ready
  result_queue.put(WorkerMessage.worker_ready(worker_id).to_dict())

  # Track active spider for cancellation support
  spider_holder = {"spider": None, "job_id": None, "crawling": False}

  def check_for_cancellation():
    """Check for cancellation messages while a crawl is in progress."""
    if not spider_holder["crawling"]:
      return  # No crawl in progress, main check_queue handles everything

    try:
      if not request_queue.empty():
        msg_dict = request_queue.get_nowait()
        msg = WorkerMessage.from_dict(msg_dict)

        if msg.type == MessageType.SHUTDOWN:
          print(f"[Worker {worker_id}] Received shutdown signal during crawl")
          if spider_holder["spider"]:
            spider_holder["spider"].cancel()
          reactor.stop()
          return

        if msg.type == MessageType.CANCEL_CRAWL:
          job_id = msg.payload.get("job_id")
          print(f"[Worker {worker_id}] Received cancel request for job: {job_id}")
          if spider_holder["spider"] and spider_holder["job_id"] == job_id:
            spider_holder["spider"].cancel()
          # Don't return - let crawl finish naturally after cancel flag is set

        # If we get a new CRAWL_REQUEST while one is running, put it back (shouldn't happen)
        if msg.type == MessageType.CRAWL_REQUEST:
          print(f"[Worker {worker_id}] WARNING: Received crawl request while already crawling, ignoring")

    except Exception as e:
      print(f"[Worker {worker_id}] Error checking for cancellation: {e}")

    # Keep checking while crawling
    if spider_holder["crawling"]:
      reactor.callLater(0.5, check_for_cancellation)

  def check_queue():
    """Check for new requests in the queue."""
    try:
      # Non-blocking check
      if not request_queue.empty():
        msg_dict = request_queue.get_nowait()
        msg = WorkerMessage.from_dict(msg_dict)

        if msg.type == MessageType.SHUTDOWN:
          print(f"[Worker {worker_id}] Received shutdown signal")
          # Cancel active spider if any
          if spider_holder["spider"]:
            spider_holder["spider"].cancel()
          reactor.stop()
          return

        if msg.type == MessageType.CANCEL_CRAWL:
          job_id = msg.payload.get("job_id")
          print(f"[Worker {worker_id}] Received cancel request for job: {job_id}")
          # Cancel if this is the active job
          if spider_holder["spider"] and spider_holder["job_id"] == job_id:
            spider_holder["spider"].cancel()
          # Continue checking queue (don't return, crawl will finish on its own)

        if msg.type == MessageType.CRAWL_REQUEST:
          # Parse request
          request = CrawlRequest(**msg.payload)
          print(f"[Worker {worker_id}] Starting crawl: {request.url}")

          # Signal crawl started
          result_queue.put(WorkerMessage.crawl_started(request.job_id).to_dict())

          # Mark as crawling and start cancellation checker
          spider_holder["crawling"] = True
          reactor.callLater(0.5, check_for_cancellation)

          # Run the crawl
          d = run_crawl(request, result_queue, spider_holder)

          # When done, reset crawling flag and check for more work
          def on_crawl_done(_):
            spider_holder["crawling"] = False
            reactor.callLater(0.1, check_queue)
            return _

          d.addCallback(on_crawl_done)
          d.addErrback(lambda f: handle_crawl_error(f, request.job_id, result_queue))
          return

    except Exception as e:
      print(f"[Worker {worker_id}] Error checking queue: {e}")
      traceback.print_exc()

    # Schedule next check
    reactor.callLater(0.5, check_queue)

  def handle_crawl_error(failure, job_id: str, result_queue: Queue):
    """Handle crawl errors."""
    error_msg = str(failure.value)
    print(f"[Worker {worker_id}] Crawl error: {error_msg}")

    result = CrawlResult(
      job_id=job_id,
      status=CrawlStatus.FAILED,
      pages_crawled=0,
      pages_failed=0,
      fatal_error=error_msg,
    )
    result_queue.put(WorkerMessage.crawl_result(result).to_dict())

    # Continue checking for more work
    reactor.callLater(0.1, check_queue)

  # Start checking queue after reactor starts
  reactor.callWhenRunning(check_queue)

  # Run the reactor - this blocks until reactor.stop() is called
  try:
    reactor.run(installSignalHandlers=False)
  except Exception as e:
    print(f"[Worker {worker_id}] Reactor error: {e}")
    result_queue.put(WorkerMessage.worker_error(str(e)).to_dict())

  print(f"[Worker {worker_id}] Exiting")


if __name__ == "__main__":
  # This should not be run directly - it's spawned by ScrapyWorkerPool
  print("This module should be spawned by ScrapyWorkerPool, not run directly")
  sys.exit(1)
