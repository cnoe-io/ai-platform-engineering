# Copyright 2025 CNOE Contributors
# SPDX-License-Identifier: Apache-2.0

import logging
import uuid
import os
from langchain_core.messages import AIMessage
from langgraph.graph.state import CompiledStateGraph
from langgraph_supervisor import create_supervisor
from langgraph_supervisor.handoff import create_forward_message_tool
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.store.memory import InMemoryStore
from cnoe_agent_utils import LLMFactory

from ai_platform_engineering.multi_agents.platform_engineer.prompts import (
  system_prompt,
  response_format_instruction
)
from ai_platform_engineering.agents.argocd.a2a_agent_client.agent import argocd_a2a_remote_agent
from ai_platform_engineering.agents.backstage.a2a_agent_client.agent import backstage_a2a_remote_agent
from ai_platform_engineering.agents.confluence.a2a_agent_client.agent import confluence_a2a_remote_agent
from ai_platform_engineering.agents.github.a2a_agent_client.agent import github_a2a_remote_agent
from ai_platform_engineering.agents.jira.a2a_agent_client.agent import jira_a2a_remote_agent
from ai_platform_engineering.agents.pagerduty.a2a_agent_client.agent import pagerduty_a2a_remote_agent
from ai_platform_engineering.agents.slack.a2a_agent_client.agent import slack_a2a_remote_agent

from ai_platform_engineering.utils.models.generic_agent import (
  ResponseFormat
)

# Only import komodor_agent if KOMODOR_AGENT_HOST is set in the environment
KOMODOR_ENABLED = os.getenv("ENABLE_KOMODOR", "false").lower() == "true"
if KOMODOR_ENABLED:
    from ai_platform_engineering.agents.komodor.a2a_agent_client.agent import komodor_a2a_remote_agent

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AIPlatformEngineerMAS:
  def __init__(self):
    self.graph = self.build_graph()

  def get_graph(self) -> CompiledStateGraph:
    """
    Returns the compiled LangGraph instance for the AI Platform Engineer MAS.

    This method initializes the graph if it has not been created yet and returns
    the compiled graph instance.

    Returns:
        CompiledStateGraph: The compiled LangGraph instance.
    """
    if not hasattr(self, 'graph'):
      self.graph = self.build_graph()
    return self.graph

  def build_graph(self) -> CompiledStateGraph:
    """
    Constructs and compiles a LangGraph instance.

    This function initializes a `SupervisorAgent` to create the base graph structure
    and uses an `InMemorySaver` as the checkpointer for the compilation process.

    The resulting compiled graph can be used to execute Supervisor workflow in LangGraph Studio.

    Returns:
    CompiledGraph: A fully compiled LangGraph instance ready for execution.
    """
    model = LLMFactory().get_llm()

    # Check if LANGGRAPH_DEV is defined in the environment
    # if os.getenv("LANGGRAPH_DEV"):
    #   checkpointer = None
    #   store = None
    # else:
    checkpointer = InMemorySaver()
    store = InMemoryStore()

    agent_tools = [
      argocd_a2a_remote_agent,
      backstage_a2a_remote_agent,
      confluence_a2a_remote_agent,
      github_a2a_remote_agent,
      jira_a2a_remote_agent,
      pagerduty_a2a_remote_agent,
      slack_a2a_remote_agent,
    ]
    if KOMODOR_ENABLED:
      agent_tools.append(komodor_a2a_remote_agent)

     # The argument is the name to assign to the resulting forwarded message
    forwarding_tool = create_forward_message_tool("platform_engineer_supervisor")

    graph = create_supervisor(
      model=model,
      agents=[],
      prompt=system_prompt,
      add_handoff_back_messages=False,
      tools=[forwarding_tool] + agent_tools,
      output_mode="last_message",
      supervisor_name="platform_engineer_supervisor",
      response_format=(response_format_instruction, ResponseFormat),
    ).compile(
      checkpointer=checkpointer,
      store=store,
    )
    logger.debug("LangGraph supervisor created and compiled successfully.")
    return graph

  async def serve(self, prompt: str):
    """
    Processes the input prompt and returns a response from the graph.
    Args:
        prompt (str): The input prompt to be processed by the graph.
    Returns:
        str: The response generated by the graph based on the input prompt.
    """
    try:
      logger.debug(f"Received prompt: {prompt}")
      if not isinstance(prompt, str) or not prompt.strip():
        raise ValueError("Prompt must be a non-empty string.")
      result = await self.graph.ainvoke({
          "messages": [
              {
                  "role": "user",
                  "content": prompt
              }
          ],
      }, {"configurable": {"thread_id": uuid.uuid4()}})

      messages = result.get("messages", [])
      if not messages:
        raise RuntimeError("No messages found in the graph response.")

      # Find the last AIMessage with non-empty content
      for message in reversed(messages):
        if isinstance(message, AIMessage) and message.content.strip():
          logger.debug(f"Valid AIMessage found: {message.content.strip()}")
          return message.content.strip()

      raise RuntimeError("No valid AIMessage found in the graph response.")
    except ValueError as ve:
      logger.error(f"ValueError in serve method: {ve}")
      raise ValueError(str(ve))
    except Exception as e:
      logger.error(f"Error in serve method: {e}")
      raise Exception(str(e))
